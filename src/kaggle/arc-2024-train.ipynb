{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cax\n",
    "!pip install mediapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import jax\n",
    "import jaxlib\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import flax\n",
    "from flax import nnx\n",
    "import optax\n",
    "import cax\n",
    "from cax.core.ca import CA\n",
    "from cax.core.perceive.depthwise_conv_perceive import DepthwiseConvPerceive\n",
    "from cax.core.perceive.kernels import grad_kernel, identity_kernel\n",
    "from cax.core.update.residual_update import ResidualUpdate\n",
    "\n",
    "print(f\"jax {jax.__version__}\")\n",
    "print(f\"jaxlib {jaxlib.__version__}\")\n",
    "print(f\"cax {cax.__version__}\")\n",
    "print(f\"flax {flax.__version__}\")\n",
    "print(f\"optax {optax.__version__}\")\n",
    "\n",
    "# List all files under the input directory\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Paths to the dataset files\n",
    "training_challenges_path = '/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "training_solutions_path = '/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json'\n",
    "evaluation_challenges_path = '/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json'\n",
    "evaluation_solutions_path = '/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json'\n",
    "\n",
    "# Load the training data\n",
    "with open(training_challenges_path, 'r') as f:\n",
    "    training_challenges = json.load(f)\n",
    "with open(training_solutions_path, 'r') as f:\n",
    "    training_solutions = json.load(f)\n",
    "\n",
    "# Load the evaluation data\n",
    "with open(evaluation_challenges_path, 'r') as f:\n",
    "    evaluation_challenges = json.load(f)\n",
    "with open(evaluation_solutions_path, 'r') as f:\n",
    "    evaluation_solutions = json.load(f)\n",
    "\n",
    "def process_tasks(challenges, solutions):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    task_indices = []\n",
    "    task_id_to_index = {}\n",
    "    index = 0\n",
    "    for task_id in challenges.keys():\n",
    "        if task_id not in task_id_to_index:\n",
    "            task_id_to_index[task_id] = index\n",
    "            index += 1\n",
    "        task_index = task_id_to_index[task_id]\n",
    "        task = challenges[task_id]\n",
    "        solution = solutions[task_id]\n",
    "        # For each task, get train pairs\n",
    "        train_pairs = task['train']\n",
    "        test_pairs = task['test']\n",
    "        # Combine train pairs for training\n",
    "        for pair in train_pairs:\n",
    "            input_grid = np.array(pair['input'], dtype=np.int32)\n",
    "            output_grid = np.array(pair['output'], dtype=np.int32)\n",
    "            inputs.append(input_grid)\n",
    "            outputs.append(output_grid)\n",
    "            task_indices.append(task_index)\n",
    "        # Use test pairs and solutions\n",
    "        for i, test_input in enumerate(test_pairs):\n",
    "            input_grid = np.array(test_input['input'], dtype=np.int32)\n",
    "            output_grid = np.array(solution[i], dtype=np.int32)\n",
    "            inputs.append(input_grid)\n",
    "            outputs.append(output_grid)\n",
    "            task_indices.append(task_index)\n",
    "    return inputs, outputs, task_indices, task_id_to_index\n",
    "\n",
    "# Process training data\n",
    "training_inputs, training_outputs, training_task_indices, task_id_to_index = process_tasks(training_challenges, training_solutions)\n",
    "\n",
    "# Process evaluation data\n",
    "evaluation_inputs, evaluation_outputs, evaluation_task_indices, _ = process_tasks(evaluation_challenges, evaluation_solutions)\n",
    "\n",
    "def pad_grid(grid, max_rows=30, max_cols=30, pad_value=0):\n",
    "    padded_grid = np.full((max_rows, max_cols), pad_value, dtype=np.int32)\n",
    "    rows = grid.shape[0]\n",
    "    cols = grid.shape[1]\n",
    "    padded_grid[:rows, :cols] = grid\n",
    "    return padded_grid\n",
    "\n",
    "# Pad all grids in the training data\n",
    "padded_training_inputs = [pad_grid(grid) for grid in training_inputs]\n",
    "padded_training_outputs = [pad_grid(grid) for grid in training_outputs]\n",
    "\n",
    "# Pad all grids in the evaluation data\n",
    "padded_evaluation_inputs = [pad_grid(grid) for grid in evaluation_inputs]\n",
    "padded_evaluation_outputs = [pad_grid(grid) for grid in evaluation_outputs]\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "training_inputs_array = np.stack(padded_training_inputs)\n",
    "training_outputs_array = np.stack(padded_training_outputs)\n",
    "evaluation_inputs_array = np.stack(padded_evaluation_inputs)\n",
    "evaluation_outputs_array = np.stack(padded_evaluation_outputs)\n",
    "\n",
    "training_task_indices_array = np.array(training_task_indices, dtype=np.int32)\n",
    "evaluation_task_indices_array = np.array(evaluation_task_indices, dtype=np.int32)\n",
    "\n",
    "print(f\"Training inputs shape: {training_inputs_array.shape}\")\n",
    "print(f\"Training outputs shape: {training_outputs_array.shape}\")\n",
    "print(f\"Training task indices shape: {training_task_indices_array.shape}\")\n",
    "print(f\"Evaluation inputs shape: {evaluation_inputs_array.shape}\")\n",
    "print(f\"Evaluation outputs shape: {evaluation_outputs_array.shape}\")\n",
    "print(f\"Evaluation task indices shape: {evaluation_task_indices_array.shape}\")\n",
    "\n",
    "# Convert numpy arrays to JAX arrays\n",
    "training_inputs_array = jnp.array(training_inputs_array)\n",
    "training_outputs_array = jnp.array(training_outputs_array)\n",
    "training_task_indices_array = jnp.array(training_task_indices_array)\n",
    "evaluation_inputs_array = jnp.array(evaluation_inputs_array)\n",
    "evaluation_outputs_array = jnp.array(evaluation_outputs_array)\n",
    "evaluation_task_indices_array = jnp.array(evaluation_task_indices_array)\n",
    "\n",
    "# Set up training parameters\n",
    "seed = 0\n",
    "key = jax.random.PRNGKey(seed)\n",
    "rngs = nnx.Rngs(seed)\n",
    "\n",
    "channel_size = 32\n",
    "num_spatial_dims = 2  # Set to 2 for 2D grids\n",
    "num_kernels = 2\n",
    "hidden_size = 256\n",
    "cell_dropout_rate = 0.5\n",
    "batch_size = 2\n",
    "num_steps = 2\n",
    "learning_rate = 1e-3\n",
    "ds_size = 30  # Grid size\n",
    "num_train_steps = 4\n",
    "print_interval = 2\n",
    "\n",
    "task_list = list(task_id_to_index.keys())\n",
    "num_tasks = len(task_list)\n",
    "\n",
    "print(f\"Number of tasks: {num_tasks}\")\n",
    "\n",
    "# Define functions to initialize state\n",
    "def init_state(key):\n",
    "    idx = jax.random.randint(key, (), 0, training_inputs_array.shape[0])\n",
    "    input_grid = training_inputs_array[idx]\n",
    "    target_grid = training_outputs_array[idx]\n",
    "    task_index = training_task_indices_array[idx]\n",
    "    state = jnp.zeros((ds_size, ds_size, channel_size))\n",
    "    # Initialize the first channel with the input grid\n",
    "    state = state.at[..., 0].set(input_grid)\n",
    "    return state, target_grid, task_index\n",
    "\n",
    "def init_state_test(key):\n",
    "    idx = jax.random.randint(key, (), 0, evaluation_inputs_array.shape[0])\n",
    "    input_grid = evaluation_inputs_array[idx]\n",
    "    target_grid = evaluation_outputs_array[idx]\n",
    "    task_index = evaluation_task_indices_array[idx]\n",
    "    state = jnp.zeros((ds_size, ds_size, channel_size))\n",
    "    # Initialize the first channel with the input grid\n",
    "    state = state.at[..., 0].set(input_grid)\n",
    "    return state, target_grid, task_index\n",
    "\n",
    "# Define the NCA model\n",
    "class EmbedCA(CA):\n",
    "    embed_input: nnx.Embed\n",
    "    embed_task: nnx.Embed\n",
    "\n",
    "    def __init__(self, perceive, update, embed_input, embed_task):\n",
    "        super().__init__(perceive, update)\n",
    "        self.embed_input = embed_input\n",
    "        self.embed_task = embed_task\n",
    "\n",
    "    def __call__(self, state, task_embed, num_steps=1, all_steps=False):\n",
    "        steps = []\n",
    "        for _ in range(num_steps):\n",
    "            state = self.step(state, task_embed)\n",
    "            if all_steps:\n",
    "                steps.append(state)\n",
    "        if all_steps:\n",
    "            return jnp.stack(steps)\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "# Custom DepthwiseConvPerceive with print statements\n",
    "class MyDepthwiseConvPerceive(DepthwiseConvPerceive):\n",
    "    def __call__(self, state: jnp.ndarray) -> jnp.ndarray:\n",
    "        print(f\"Perceive input state shape: {state.shape}\")\n",
    "        print(f\"Depthwise conv kernel shape: {self.depthwise_conv.kernel.value.shape}\")\n",
    "        print(f\"Depthwise conv features: {self.depthwise_conv.features}\")\n",
    "        print(f\"Depthwise conv feature group count: {self.depthwise_conv.feature_group_count}\")\n",
    "        perception = self.depthwise_conv(state)\n",
    "        return perception\n",
    "\n",
    "# Initialize the NCA model components\n",
    "perceive = MyDepthwiseConvPerceive(channel_size, rngs, num_kernels=num_kernels, kernel_size=(3, 3))\n",
    "update = ResidualUpdate(\n",
    "    num_spatial_dims,\n",
    "    channel_size,\n",
    "    num_kernels * channel_size + 8,\n",
    "    (hidden_size,),\n",
    "    rngs,\n",
    "    cell_dropout_rate=cell_dropout_rate,\n",
    ")\n",
    "embed_input = nnx.Embed(num_embeddings=10, features=3, rngs=rngs)\n",
    "embed_task = nnx.Embed(num_embeddings=num_tasks, features=8, rngs=rngs)\n",
    "\n",
    "# Initialize the NCA model\n",
    "ca = EmbedCA(perceive, update, embed_input, embed_task)\n",
    "\n",
    "# Initialize the kernels\n",
    "identity = identity_kernel(ndim=2)  # Shape: (3, 3, 1)\n",
    "gradient = grad_kernel(ndim=2)      # Shape: (3, 3, 2)\n",
    "print(f\"identity kernel shape: {identity.shape}\")\n",
    "print(f\"gradient kernel shape: {gradient.shape}\")\n",
    "\n",
    "# Stack the kernels along the last axis to get shape (3, 3, 3)\n",
    "base_kernel = jnp.concatenate([identity, gradient], axis=-1)  # Shape: (3, 3, 3)\n",
    "print(f\"base_kernel shape after concatenation: {base_kernel.shape}\")\n",
    "\n",
    "# Expand dimensions to match required shape\n",
    "base_kernel = base_kernel[:, :, None, :]  # Shape: (3, 3, 1, 3)\n",
    "print(f\"base_kernel shape after expand_dims: {base_kernel.shape}\")\n",
    "\n",
    "# Total number of features (output channels)\n",
    "features = channel_size * num_kernels\n",
    "print(f\"channel_size: {channel_size}\")\n",
    "print(f\"num_kernels: {num_kernels}\")\n",
    "print(f\"Total features (output channels): {features}\")\n",
    "\n",
    "# Calculate the number of tiles needed\n",
    "tiles = int(np.ceil(features / base_kernel.shape[-1]))\n",
    "print(f\"Number of tiles: {tiles}\")\n",
    "\n",
    "# Tile the base_kernel to match the number of features\n",
    "kernel = jnp.tile(base_kernel, (1, 1, 1, tiles))\n",
    "print(f\"kernel shape after tiling: {kernel.shape}\")\n",
    "\n",
    "# Slice the kernel to get the exact number of features needed\n",
    "kernel = kernel[:, :, :, :features]\n",
    "print(f\"kernel shape after slicing: {kernel.shape}\")\n",
    "\n",
    "# Verify the kernel shape matches the expected shape\n",
    "expected_kernel_shape = (3, 3, 1, features)\n",
    "print(f\"Expected kernel shape: {expected_kernel_shape}\")\n",
    "assert kernel.shape == expected_kernel_shape, \"Kernel shape does not match expected shape.\"\n",
    "\n",
    "# Assign the kernel to the depthwise convolution layer\n",
    "perceive.depthwise_conv.kernel = nnx.Param(kernel)\n",
    "print(f\"Assigned kernel shape: {perceive.depthwise_conv.kernel.value.shape}\")\n",
    "print(f\"Depthwise conv features: {perceive.depthwise_conv.features}\")\n",
    "print(f\"Feature group count: {perceive.depthwise_conv.feature_group_count}\")\n",
    "\n",
    "# Extract parameters\n",
    "params = nnx.state(ca, nnx.Param)\n",
    "print(\"Number of params:\", jax.tree_util.tree_reduce(lambda x, y: x + y.size, params, 0))\n",
    "\n",
    "# Set up the optimizer\n",
    "lr_sched = optax.linear_schedule(init_value=learning_rate, end_value=0.1 * learning_rate, transition_steps=2000)\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),\n",
    "    optax.adam(learning_rate=lr_sched),\n",
    ")\n",
    "update_params = nnx.All(nnx.Param, nnx.PathContains(\"update\"))\n",
    "optimizer = nnx.Optimizer(ca, optimizer, wrt=update_params)\n",
    "\n",
    "# Define the loss and accuracy functions\n",
    "def mse(state, target):\n",
    "    return jnp.mean(jnp.square(state[..., :3] - target))\n",
    "\n",
    "@nnx.jit\n",
    "def accuracy_fn(state, target):\n",
    "    predictions = jnp.argmax(state[..., :3], axis=-1)\n",
    "    correct = jnp.sum(predictions == target)\n",
    "    total = target.size\n",
    "    return correct / total\n",
    "\n",
    "@nnx.jit\n",
    "def loss_fn(ca, state, target, task_index):\n",
    "    input_grid = state[..., 0]\n",
    "    input_embed = ca.embed_input(jnp.asarray(input_grid, dtype=jnp.int32))\n",
    "    task_embed = ca.embed_task(jnp.asarray(task_index, dtype=jnp.int32))\n",
    "    state = state.at[..., :3].set(input_embed)\n",
    "    target_embed = ca.embed_input(jnp.asarray(target, dtype=jnp.int32))\n",
    "    state_axes = nnx.StateAxes({nnx.RngState: 0, ...: None})\n",
    "    # Print shapes before calling ca\n",
    "    print(f\"State shape before CA: {state.shape}\")\n",
    "    print(f\"Task embed shape: {task_embed.shape}\")\n",
    "    state = nnx.split_rngs(splits=batch_size)(\n",
    "        nnx.vmap(\n",
    "            lambda ca, state, task_embed: ca(state, task_embed, num_steps=num_steps),\n",
    "            in_axes=(state_axes, 0, 0),\n",
    "        )\n",
    "    )(ca, state, task_embed)\n",
    "    loss = mse(state, target_embed)\n",
    "    return loss\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(ca, optimizer, key):\n",
    "    keys = jax.random.split(key, batch_size)\n",
    "    state, target, task_index = jax.vmap(init_state)(keys)\n",
    "    loss, grad = nnx.value_and_grad(loss_fn, argnums=nnx.DiffState(0, update_params))(ca, state, target, task_index)\n",
    "    optimizer.update(grad)\n",
    "    return loss\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(ca, key):\n",
    "    keys = jax.random.split(key, batch_size)\n",
    "    state, target, task_index = jax.vmap(init_state_test)(keys)\n",
    "    accuracy = accuracy_fn(state, target)\n",
    "    return accuracy\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(num_train_steps), desc=\"Training\", unit=\"train_step\")\n",
    "losses = []\n",
    "eval_accuracies = []\n",
    "\n",
    "for i in pbar:\n",
    "    key, subkey = jax.random.split(key)\n",
    "    loss = train_step(ca, optimizer, subkey)\n",
    "    losses.append(loss)\n",
    "\n",
    "    if i % print_interval == 0 or i == num_train_steps - 1:\n",
    "        avg_loss = sum(losses[-print_interval:]) / len(losses[-print_interval:])\n",
    "        pbar.set_postfix({\"Average Loss\": f\"{avg_loss:.6f}\"})\n",
    "        accuracy = eval_step(ca, subkey)\n",
    "        eval_accuracies.append(accuracy)\n",
    "        avg_accuracy = sum(eval_accuracies[-print_interval:]) / len(eval_accuracies[-print_interval:])\n",
    "        print(f\"Step {i}, Average Loss: {avg_loss:.6f}, Eval Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "# Prepare submission\n",
    "test_challenges_path = '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json'\n",
    "\n",
    "with open(test_challenges_path, 'r') as f:\n",
    "    test_challenges = json.load(f)\n",
    "\n",
    "submission = dict()\n",
    "for task_id, task in test_challenges.items():\n",
    "    test_pairs = task['test']\n",
    "    outputs = []\n",
    "    for test_input in test_pairs:\n",
    "        input_grid = np.array(test_input['input'], dtype=np.int32)\n",
    "        padded_input = pad_grid(input_grid)\n",
    "        # Initialize state\n",
    "        state = np.zeros((ds_size, ds_size, channel_size), dtype=np.float32)\n",
    "        state[..., 0] = padded_input\n",
    "        # Embed input and task\n",
    "        input_embed = ca.embed_input(jnp.asarray(state[..., 0], dtype=jnp.int32))\n",
    "        # Assuming task_index is known, else set to 0\n",
    "        task_index = task_id_to_index.get(task_id, 0)\n",
    "        task_embed = ca.embed_task(jnp.asarray(task_index, dtype=jnp.int32))\n",
    "        state = jnp.array(state)\n",
    "        state = state.at[..., :3].set(input_embed)\n",
    "        # Run the model\n",
    "        state_axes = nnx.StateAxes({nnx.RngState: None, ...: None})\n",
    "        state = ca(state, task_embed, num_steps=num_steps)\n",
    "        # Get the output\n",
    "        output_grid = jnp.argmax(state[..., :3], axis=-1)\n",
    "        output_grid = output_grid.astype(int)\n",
    "        # Remove padding if necessary (depends on original input size)\n",
    "        original_shape = input_grid.shape\n",
    "        output_grid = output_grid[:original_shape[0], :original_shape[1]]\n",
    "        outputs.append({\"output\": output_grid.tolist()})\n",
    "    submission[task_id] = outputs\n",
    "\n",
    "# Save the submission\n",
    "with open('submission.json', 'w') as f:\n",
    "    json.dump(submission, f)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
