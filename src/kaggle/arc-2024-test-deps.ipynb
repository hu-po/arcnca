{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cax\n",
    "!pip install mediapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import jax\n",
    "import jaxlib\n",
    "import jax.numpy as jnp\n",
    "import mediapy\n",
    "import optax\n",
    "import cax\n",
    "from cax.core.ca import CA\n",
    "from cax.core.perceive.depthwise_conv_perceive import DepthwiseConvPerceive\n",
    "from cax.core.perceive.kernels import grad_kernel, identity_kernel\n",
    "from cax.core.update.residual_update import ResidualUpdate\n",
    "import flax\n",
    "from flax import nnx\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f\"jax {jax.__version__}\")\n",
    "print(f\"jaxlib {jaxlib.__version__}\")\n",
    "print(f\"cax {cax.__version__}\")\n",
    "print(f\"flax {flax.__version__}\")\n",
    "print(f\"optax {optax.__version__}\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# List all files under the input directory\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Paths to the dataset files\n",
    "training_challenges_path = '/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "training_solutions_path = '/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json'\n",
    "evaluation_challenges_path = '/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json'\n",
    "evaluation_solutions_path = '/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json'\n",
    "\n",
    "# Load the training data\n",
    "with open(training_challenges_path, 'r') as f:\n",
    "    training_challenges = json.load(f)\n",
    "with open(training_solutions_path, 'r') as f:\n",
    "    training_solutions = json.load(f)\n",
    "\n",
    "# Load the evaluation data\n",
    "with open(evaluation_challenges_path, 'r') as f:\n",
    "    evaluation_challenges = json.load(f)\n",
    "with open(evaluation_solutions_path, 'r') as f:\n",
    "    evaluation_solutions = json.load(f)\n",
    "\n",
    "def process_tasks(challenges, solutions):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for task_id in challenges.keys():\n",
    "        task = challenges[task_id]\n",
    "        solution = solutions[task_id]\n",
    "        # For each task, get train pairs\n",
    "        train_pairs = task['train']\n",
    "        for pair in train_pairs:\n",
    "            input_grid = np.array(pair['input'], dtype=np.int32)\n",
    "            output_grid = np.array(pair['output'], dtype=np.int32)\n",
    "            inputs.append(input_grid)\n",
    "            outputs.append(output_grid)\n",
    "    return inputs, outputs\n",
    "\n",
    "# Process training data\n",
    "training_inputs, training_outputs = process_tasks(training_challenges, training_solutions)\n",
    "\n",
    "# Process evaluation data\n",
    "evaluation_inputs, evaluation_outputs = process_tasks(evaluation_challenges, evaluation_solutions)\n",
    "\n",
    "def get_max_grid_size(grids):\n",
    "    max_rows = max(len(grid) for grid in grids)\n",
    "    max_cols = max(len(grid[0]) if len(grid) > 0 else 0 for grid in grids)\n",
    "    return max_rows, max_cols\n",
    "\n",
    "# Determine the maximum grid size for padding\n",
    "max_input_rows, max_input_cols = get_max_grid_size(training_inputs + evaluation_inputs)\n",
    "max_output_rows, max_output_cols = get_max_grid_size(training_outputs + evaluation_outputs)\n",
    "\n",
    "# Set a fixed grid size (e.g., 30x30 as per dataset description)\n",
    "fixed_rows, fixed_cols = 30, 30\n",
    "\n",
    "def pad_grid(grid, max_rows=fixed_rows, max_cols=fixed_cols, pad_value=0):\n",
    "    padded_grid = np.full((max_rows, max_cols), pad_value, dtype=np.int32)\n",
    "    rows = len(grid)\n",
    "    cols = len(grid[0]) if rows > 0 else 0\n",
    "    padded_grid[:rows, :cols] = grid\n",
    "    return padded_grid\n",
    "\n",
    "# Pad all grids in the training data\n",
    "padded_training_inputs = [pad_grid(grid) for grid in training_inputs]\n",
    "padded_training_outputs = [pad_grid(grid) for grid in training_outputs]\n",
    "\n",
    "# Pad all grids in the evaluation data\n",
    "padded_evaluation_inputs = [pad_grid(grid) for grid in evaluation_inputs]\n",
    "padded_evaluation_outputs = [pad_grid(grid) for grid in evaluation_outputs]\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "training_inputs_array = np.stack(padded_training_inputs)\n",
    "training_outputs_array = np.stack(padded_training_outputs)\n",
    "evaluation_inputs_array = np.stack(padded_evaluation_inputs)\n",
    "evaluation_outputs_array = np.stack(padded_evaluation_outputs)\n",
    "\n",
    "print(f\"Training inputs shape: {training_inputs_array.shape}\")\n",
    "print(f\"Training outputs shape: {training_outputs_array.shape}\")\n",
    "print(f\"Evaluation inputs shape: {evaluation_inputs_array.shape}\")\n",
    "print(f\"Evaluation outputs shape: {evaluation_outputs_array.shape}\")\n",
    "\n",
    "# Now the data is ready for JAX/Flax training\n",
    "# You can proceed to define your model and training loop\n",
    "\n",
    "# Example: Create a simple JAX dataset iterator\n",
    "def data_generator(inputs, outputs, batch_size):\n",
    "    num_samples = inputs.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, num_samples - batch_size + 1, batch_size):\n",
    "        batch_indices = indices[start_idx:start_idx + batch_size]\n",
    "        yield inputs[batch_indices], outputs[batch_indices]\n",
    "\n",
    "# Example usage of data generator\n",
    "batch_size = 32\n",
    "train_data_gen = data_generator(training_inputs_array, training_outputs_array, batch_size)\n",
    "\n",
    "# Fetch one batch of data\n",
    "batch_inputs, batch_outputs = next(train_data_gen)\n",
    "print(f\"Batch inputs shape: {batch_inputs.shape}\")\n",
    "print(f\"Batch outputs shape: {batch_outputs.shape}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
