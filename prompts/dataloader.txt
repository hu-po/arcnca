Here is a notebook that loads a dataset for the kaggle arc challenge. It currently pads the data and also keeps track of the indices of the tasks. Note the separate evaluation and training tasks. I wanted to add some additional data augmentation usign vertical and horizontal flipping, noise, rotation. What else can be improved? To help with your analysis I have included some information on the dataset from the kaggle challenge:

<description>

source: https://www.kaggle.com/competitions/arc-prize-2024/overview
Evaluation
This competition evaluates submissions on the percentage of correct predictions. For each task, you should predict exactly 2 outputs for every test input grid contained in the task. (Tasks can have more than one test input that needs a predicted output.) Each task test output has one ground truth. For a given task output, any of the 2 predicted outputs matches the ground truth exactly, you score 1 for that task test output, otherwise 0. The final score is the sum averaged of the highest score per task output divided by the total number of task test outputs.

Submission File
The submission file for this competition must be a json named submission.json.

For each task output in the evaluation set, you should make exactly 2 predictions (attempt_1, attempt_2). The structure of predictions is shown below. Most tasks only have a single output (a single dictionary enclosed in a list), although some tasks have multiple outputs that must be predicted. These should contain two dictionaries of predictions enclosed in a list, as is shown by the example below. When a task has multiple test outputs that need to be predicted (e.g., task 12997ef3 below), they must be in the same order as the corresponding test inputs.

IMPORTANT: All the task_ids in the input challenges json file must also be present in the submission.json file. Both "attempt_1" and "attempt_2" must be present, even if your submission doesn't have 2 predictions.

{"00576224": [{"attempt_1": [[0, 0], [0, 0]], "attempt_2": [[0, 0], [0, 0]]}],
 "009d5c81": [{"attempt_1": [[0, 0], [0, 0]], "attempt_2": [[0, 0], [0, 0]]}],
 "12997ef3": [{"attempt_1": [[0, 0], [0, 0]], "attempt_2": [[0, 0], [0, 0]]},
              {"attempt_1": [[0, 0], [0, 0]], "attempt_2": [[0, 0], [0, 0]]}],
 ...
}

source: https://www.kaggle.com/competitions/arc-prize-2024/data
The objective of this competition is to create an algorithm that is capable of solving abstract reasoning tasks. Critically, these are novel tasks: tasks that the algorithm has never seen before. Hence, simply memorizing a set of reasoning templates will not suffice.

The format is different from the previous competition, so please read this information carefully, and refer to supplementary documentation as needed.

When looking at a task, a "test-taker" has access to inputs and outputs of the demonstration pairs (train pairs), plus the input(s) of the test pair(s). The goal is to construct the output grid(s) corresponding to the test input grid(s), using 2 trials for each test input. "Constructing the output grid" involves picking the height and width of the output grid, then filling each cell in the grid with a symbol (integer between 0 and 9, which are visualized as colors). Only exact solutions (all cells match the expected answer) can be said to be correct.

Any additional information, as well as an interactive app to explore the objective of this competition is found at the ARCPrize.org. It is highly recommended that you explore the interactive app, as the best way to understand the objective of the competition.

Task files
The information is stored in two files:

arc-agi_training-challenges.json: contains input/output pairs that demonstrate reasoning pattern to be applied to the "test" input for each task. This file and the corresponding solutions file can be used as training for your models.
arc-agi_training-solutions.json: contains the corresponding task "test" outputs (ground truth).
arc-agi_evaluation-challenges.json: contains input/output pairs that demonstrate reasoning pattern to be applied to the "test" input for each task. This file and the corresponding solutions file can be used as validation data for your models.
arc-agi_evaluation-solutions.json: contains the corresponding task "test" outputs (ground truth).
arc-agi_test-challenges.json: this file contains the tasks that will be used for the leaderboard evaluation, and contains "train" input/output pairs as well as the "test" input for each task. Your task is to predict the "test" output. Note: The file shown on this page is a placeholder using tasks from arc-agi_evaluation-challenges.json. When you submit your notebook to be rerun, this file is swapped with the actual test challenges.
sample_submission.json: a submission file in the correct format
Each task contains a dictionary with two fields:

"train": demonstration input/output pairs. It is a list of "pairs" (typically 3 pairs).
"test": test input - your model should predict the output.
A "pair" is a dictionary with two fields:

"input": the input "grid" for the pair.
"output": the output "grid" for the pair.
A "grid" is a rectangular matrix (list of lists) of integers between 0 and 9 (inclusive). The smallest possible grid size is 1x1 and the largest is 30x30.

The data on this page should be used to develop and evaluate your models. When notebooks are submitted for rerun, they are scored using 100 unseen tasks found in the rerun file named arc-agi_test_challenges.json. The rerun tasks will contain train pairs of inputs and outputs as well as the tasks test input. Your algorithm must predict the test output. The majority of the 100 tasks used for leaderboard score only have one test input that will require a corresponding output prediction, although for a small number of tasks, you will be asked to make predictions for two test inputs.

source: https://github.com/maxencefaldor/cax
CAX introduces a unifying framework for all cellular automata types, encompassing discrete, continuous, and neural models across any number of dimensions (Table 1). This flexible architecture is built upon two key components: the perceive module and the update module. Together, these modules define the local rule of the CA. At each time step, this local rule is applied uniformly to all cells in the grid, generating the next global state of the system, as explained in Section 2.1. This modular approach not only provides a clear separation of concerns but also facilitates easy experimentation and extension of existing CA models.
A cellular automaton is a simple model of computation consisting of a regular grid of cells, each in a particular state. The grid can be in any finite number of dimensions. For each cell, a set of cells called its neighborhood is defined relative to the specified cell. The grid is updated at discrete time steps according to a fixed rule that determines the new state of each cell based on its current state and the states of the cells in its neighborhood.
A controllable cellular automaton (CCA) is a generalization of CA that incorporates the ability to accept external inputs at each time step. CCAs formalize the concept of Goal-Guided NCA that has been introduced in the literature by Sudhakaran et al. (2022). The external inputs can modify the behavior of CCAs, offering the possibility to respond dynamically to changing conditions or control signals while maintaining the fundamental principles of cellular automata.

</description>

<notebook>
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'pytest>=8.3.2' 'numpy>=1.26.4' 'pillow>=10.4.0' 'msgpack>=1.1.0' 'requests>=2.32.3' 'mediapy>=1.2.2' tqdm\n",
    "!pip install --no-deps 'optax==0.2.3' 'chex==0.1.86' 'flax>=0.9.0' orbax-checkpoint tensorstore 'typing-extensions>=4.2' 'absl-py>=2.1.0' 'toolz>=1.0.0' 'etils[epy]>=1.9.4'\n",
    "!pip install wandb\n",
    "!wandb login\n",
    "!git clone https://github.com/hu-po/cax.git /cax\n",
    "!pip install --upgrade /cax --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import jax\n",
    "import jaxlib\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "import flax\n",
    "from flax import nnx\n",
    "import optax\n",
    "import cax\n",
    "\n",
    "for pkg in [jax, jaxlib, cax, flax, optax]:\n",
    "    print(pkg.__name__, pkg.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph =  os.environ[\"MORPH\"]\n",
    "morph_nb_filepath =  os.environ[\"MORPH_NB_FILEPATH\"]\n",
    "morph_output_dir =  os.environ[\"MORPH_OUTPUT_DIR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {len(data)} tasks from {path}\")\n",
    "    return data\n",
    "\n",
    "train_challenges = load_data('/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json')\n",
    "train_solutions = load_data('/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json')\n",
    "eval_challenges = load_data('/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json')\n",
    "eval_solutions = load_data('/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json')\n",
    "\n",
    "def process_tasks(challenges, solutions):\n",
    "    inputs, outputs, task_indices = [], [], []\n",
    "    task_id_to_index = {}\n",
    "    for index, task_id in enumerate(challenges.keys()):\n",
    "        task_id_to_index[task_id] = index\n",
    "        task = challenges[task_id]\n",
    "        solution = solutions[task_id]\n",
    "        for pair in task['train']:\n",
    "            inputs.append(np.array(pair['input'], dtype=np.int32))\n",
    "            outputs.append(np.array(pair['output'], dtype=np.int32))\n",
    "            task_indices.append(index)\n",
    "        for i, test_input in enumerate(task['test']):\n",
    "            inputs.append(np.array(test_input['input'], dtype=np.int32))\n",
    "            outputs.append(np.array(solution[i], dtype=np.int32))\n",
    "            task_indices.append(index)\n",
    "    return inputs, outputs, task_indices, task_id_to_index\n",
    "\n",
    "def pad_grids(grids, max_size=30, pad_value=0):\n",
    "    padded_grids = []\n",
    "    for grid in grids:\n",
    "        padded = np.full((max_size, max_size), pad_value, dtype=np.int32)\n",
    "        rows, cols = grid.shape\n",
    "        padded[:rows, :cols] = grid\n",
    "        padded_grids.append(padded)\n",
    "    return np.stack(padded_grids)\n",
    "\n",
    "def prepare_data(challenges, solutions):\n",
    "    inputs, outputs, task_indices, task_id_to_index = process_tasks(challenges, solutions)\n",
    "    print(f\"\\t number of samples: {len(inputs)}\")\n",
    "    inputs_array = pad_grids(inputs)\n",
    "    outputs_array = pad_grids(outputs)\n",
    "    task_indices_array = np.array(task_indices, dtype=np.int32)\n",
    "    inputs_array = jnp.array(inputs_array)\n",
    "    outputs_array = jnp.array(outputs_array)\n",
    "    task_indices_array = jnp.array(task_indices_array)\n",
    "    return inputs_array, outputs_array, task_indices_array, task_id_to_index\n",
    "\n",
    "print(\"Processing train data...\")\n",
    "train_inputs, train_outputs, train_task_indices, task_id_to_index = prepare_data(train_challenges, train_solutions)\n",
    "print(f\"\\t inputs shape: {train_inputs.shape}\")\n",
    "print(f\"\\t outputs shape: {train_outputs.shape}\")\n",
    "print(f\"\\t task indices shape: {train_task_indices.shape}\")\n",
    "\n",
    "print(\"Processing eval data...\")\n",
    "eval_inputs, eval_outputs, eval_task_indices, _ = prepare_data(eval_challenges, eval_solutions)\n",
    "print(f\"\\t inputs shape: {eval_inputs.shape}\")\n",
    "print(f\"\\t outputs shape: {eval_outputs.shape}\")\n",
    "print(f\"\\t task indices shape: {eval_task_indices.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_challenges_path = '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json'\n",
    "prepare_submission(ca, test_challenges_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
</notebook>