{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<config>\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    seed: int = int(os.environ.get(\"SEED\", 42))\n",
    "    # --- data\n",
    "    data_seed: int = 0\n",
    "    train_challenges: str = '/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "    train_solutions: str = '/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json'\n",
    "    valid_challenges: str = '/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json'\n",
    "    valid_solutions: str = '/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json'\n",
    "    submission_challenges: str = '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json'\n",
    "    num_order_augs: int = 8\n",
    "    num_color_augs: int = 32\n",
    "    # --- logging \n",
    "    morph: str = os.environ.get(\"MORPH\", \"test\")\n",
    "    compute_backend: str = os.environ.get(\"COMPUTE_BACKEND\", \"oop\")\n",
    "    wandb_entity: str = os.environ.get(\"WANDB_ENTITY\", \"hug\")\n",
    "    wandb_project: str = os.environ.get(\"WANDB_PROJECT\", \"arc-test\")\n",
    "    created_on: str = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    # --- training\n",
    "    learning_rate: float = 1e-3 # initial learning rate\n",
    "    min_lr: float = 1e-6 # minimum learning rate\n",
    "    num_epochs: int = 100\n",
    "    batch_size: int = 32\n",
    "    print_every: int = 10\n",
    "    early_stopping_patience: int = 10 # stop training if no improvement for this many epochs\n",
    "    lr_decay_factor: float = 0.5 # reduce lr by half when plateaued\n",
    "    lr_patience: int = 5 # adjust learning rate if no improvement for this many epochs\n",
    "    # --- model\n",
    "\n",
    "#<\\config>\n",
    "\n",
    "#<dataprep>\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def load_tasks(challenges_path: str, solutions_path: str, cfg: Config):\n",
    "    with open(challenges_path, 'r') as f:\n",
    "        challenges_dict = json.load(f)\n",
    "    print(f\"loading challenges from {challenges_path}, found {len(challenges_dict)} challenges\")\n",
    "    if solutions_path is not None:\n",
    "        with open(solutions_path, 'r') as f:\n",
    "            solutions_dict = json.load(f)\n",
    "        print(f\"loading solutions from {solutions_path}, found {len(solutions_dict)} solutions\")\n",
    "    \"\"\"\n",
    "    tasks are stored in JSON format. Each JSON file consists of two key-value pairs.\n",
    "    train: a list of two to ten input/output pairs (typically three.) These are used for your algorithm to infer a rule.\n",
    "    test: a list of one to three input/output pairs (typically one.) Your model should apply the inferred rule from the train set and construct an output solution.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    for task_id in challenges_dict.keys():\n",
    "        task_train_in = []\n",
    "        task_train_out = []\n",
    "        task_test_in = []\n",
    "        task_test_out = []\n",
    "        \"\"\"\n",
    "        a \"grid\" is a rectangular matrix (list of lists) of integers between 0 and 9 (inclusive).\n",
    "        the smallest possible grid size is 1x1 and the largest is 30x30.\n",
    "        0 represents the background color, 1-9 represent the pattern colors.\n",
    "        \"\"\"\n",
    "        for pair in challenges_dict[task_id]['train']:\n",
    "            _task_train_in = np.array(pair['input'], dtype=np.uint8) # store as uint8 to save system memory\n",
    "            _task_train_out = np.array(pair['output'], dtype=np.uint8)\n",
    "            task_train_in.append(_task_train_in)\n",
    "            task_train_out.append(_task_train_out)\n",
    "        for grid in challenges_dict[task_id]['test']:\n",
    "            _task_test_in = np.array(grid['input'], dtype=np.uint8)\n",
    "            task_test_in.append(_task_test_in)\n",
    "        if solutions_path is not None:\n",
    "            for grid in solutions_dict[task_id]:\n",
    "                _grid = np.array(grid, dtype=np.uint8)\n",
    "                task_test_out.append(_grid)\n",
    "        tasks.append((task_id, task_train_in, task_train_out, task_test_in, task_test_out))\n",
    "    return tasks\n",
    "\n",
    "def augmentation(tasks, cfg: Config):\n",
    "    \"\"\"\n",
    "    basic \"spatial\" augmentation of grids: flipping (lr and ud), rotating (90 and 270)\n",
    "    basic \"time\" augmentation of tasks: changing the order of the training pairs\n",
    "    basic \"channel\" augmentation of grids: change the colors used in the grid (except for 0 the background color)\n",
    "    by pre-augmenting the dataset, we increase the size of the dataset on system memory,\n",
    "    since the dataset is small, the tradeoff of less gpu compute at train time is worth it.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(cfg.data_seed)\n",
    "    augmented_tasks = []\n",
    "    for task in tasks:\n",
    "        train_in, train_out, eval_in, eval_out = task\n",
    "        augs = [task]  # Start with the original task\n",
    "        spatial_augs = [np.fliplr, np.flipud, lambda x: np.rot90(x, 1), lambda x: np.rot90(x, 3)]\n",
    "        augs.extend([\n",
    "            ([aug(grid) for grid in train_in],\n",
    "             [aug(grid) for grid in train_out],\n",
    "             [aug(grid) for grid in eval_in],\n",
    "             [aug(grid) for grid in eval_out])\n",
    "            for aug in spatial_augs\n",
    "        ])\n",
    "        if len(train_in) > 1:\n",
    "            # new order for training pairs\n",
    "            for _ in range(cfg.num_order_augs):\n",
    "                train_order = rng.permutation(len(train_in))\n",
    "                rng.shuffle(train_order)\n",
    "                augs.append((\n",
    "                    [train_in[i] for i in train_order],\n",
    "                    [train_out[i] for i in train_order],\n",
    "                    eval_in,\n",
    "                    eval_out\n",
    "                ))\n",
    "        for _ in range(cfg.num_color_augs):\n",
    "            color_map = np.arange(10)\n",
    "            rng.shuffle(color_map[1:])  # keep 0 as background color\n",
    "            augs.append((\n",
    "                [color_map[grid] for grid in train_in],\n",
    "                [color_map[grid] for grid in train_out],\n",
    "                [color_map[grid] for grid in eval_in],\n",
    "                [color_map[grid] for grid in eval_out]\n",
    "            ))\n",
    "        augmented_tasks.extend(augs)\n",
    "    return augmented_tasks\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "if cfg.compute_backend == \"kaggle\":\n",
    "    # when submitting to kaggle, save the output to the current directory\n",
    "    output_dir = os.getcwd()\n",
    "else:\n",
    "    output_dir = f\"/arcnca/output/{cfg.morph}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "hparams_filepath = os.path.join(output_dir, \"config.json\")\n",
    "with open(hparams_filepath, 'w') as f:\n",
    "    json.dump(cfg.__dict__, f, indent=4)\n",
    "\n",
    "if not cfg.compute_backend == \"kaggle\":\n",
    "    import uuid\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "    wandb.init(entity=cfg.wandb_entity, project=cfg.wandb_project, name=f\"{cfg.compute_backend}.{cfg.morph}.{str(uuid.uuid4())[:6]}\", config=cfg.__dict__)\n",
    "    wandb.save(hparams_filepath)\n",
    "\n",
    "train_tasks = load_tasks(cfg.train_challenges, cfg.train_solutions, cfg)\n",
    "train_tasks = augmentation(train_tasks, cfg)\n",
    "valid_tasks = load_tasks(cfg.valid_challenges, cfg.valid_solutions, cfg)\n",
    "#<\\dataprep>\n",
    "\n",
    "#<model>\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "def init_params(key, cfg: Config):\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "def model(params, task_train_in, task_train_out, task_test_in, cfg: Config):\n",
    "    \"\"\"\n",
    "    constructing the output grid involves picking the height and width of the output grid, then filling each cell in the grid with a symbol (integer between 0 and 9)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "def loss_fn(task_test_out_predicted, task_test_out_target, cfg: Config):\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "def accuracy_fn(task_test_out_predicted, task_test_out_target, cfg: Config):\n",
    "    \"\"\"\n",
    "    only exact solutions (all cells match the expected answer) can be said to be correct.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "def make_dataloader(tasks, cfg: Config, train_mode=True):\n",
    "    \"\"\"\n",
    "    must be able to handle the variable-sized grids in the ARC tasks.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "#<\\model>\n",
    "\n",
    "#<training>\n",
    "import pickle\n",
    "\n",
    "def save_checkpoint(params, filename):\n",
    "    with open(os.path.join(output_dir, filename), 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    with open(os.path.join(output_dir, filename), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def train_step(params, opt_state, task_train_in, task_train_out, task_test_in, task_test_out, cfg: Config):\n",
    "    def loss_and_grad(params):\n",
    "        task_test_out_predicted = model(params, task_train_in, task_train_out, task_test_in, cfg)\n",
    "        loss = loss_fn(task_test_out_predicted, task_test_out, cfg)\n",
    "        return loss\n",
    "    loss, grads = jax.value_and_grad(loss_and_grad)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "def valid_step(params, valid_gen, cfg: Config):\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    num_batches = 0\n",
    "    for task_train_in, task_train_out, task_test_in, task_test_out in valid_gen:\n",
    "        task_test_out_pred = model(params, task_train_in, task_train_out, task_test_in, cfg)\n",
    "        loss = loss_fn(task_test_out_pred, task_test_out, cfg)\n",
    "        total_loss += loss\n",
    "        total_acc += accuracy_fn(task_test_out_pred, task_test_out, cfg)\n",
    "        num_batches += 1\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_acc = total_acc / num_batches\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "key = jax.random.PRNGKey(cfg.seed)\n",
    "params = init_params(key, cfg)\n",
    "optimizer = optax.adam(cfg.learning_rate)\n",
    "opt_state = optimizer.init(params)\n",
    "best_valid_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "lr_patience_counter = 0\n",
    "current_learning_rate = cfg.learning_rate\n",
    "for epoch in range(cfg.num_epochs):\n",
    "    print(f\"epoch {epoch + 1}/{cfg.num_epochs}\")\n",
    "    steps_per_epoch = int(jnp.ceil(len(train_tasks) / cfg.batch_size))\n",
    "    # training\n",
    "    train_gen = make_dataloader(train_tasks, cfg, train_mode=True)\n",
    "    for step in range(steps_per_epoch):\n",
    "        try:\n",
    "            task_train_in, task_train_out, task_test_in, task_test_out = next(train_gen)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        params, opt_state, train_loss = train_step(params, opt_state, task_train_in, task_train_out, task_test_in, task_test_out, cfg)\n",
    "        if step % cfg.print_every == 0:\n",
    "            print(f\"step {step + 1}/{steps_per_epoch}: loss = {train_loss.item():.4f}\")\n",
    "            if not cfg.compute_backend == \"kaggle\":\n",
    "                wandb.log({\"train_loss\": train_loss.item()}, step=step + 1)\n",
    "    # validation\n",
    "    valid_gen = make_dataloader(valid_tasks, cfg, train_mode=False)\n",
    "    valid_loss, valid_acc = valid_step(params, valid_gen, cfg)\n",
    "    print(f'valid_loss: {valid_loss.item():.4f}, valid_acc: {valid_acc.item():.4f}')\n",
    "    if not cfg.compute_backend == \"kaggle\":\n",
    "        wandb.log({\"valid_loss\": valid_loss.item(), \"valid_acc\": valid_acc.item()}, step=(epoch + 1) * steps_per_epoch)\n",
    "    # early stopping and learning rate scheduler\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        epochs_without_improvement = 0\n",
    "        lr_patience_counter = 0\n",
    "        save_checkpoint(params, \"best.pkl\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        lr_patience_counter += 1\n",
    "        print(f\"no improvement for {epochs_without_improvement} epochs\")\n",
    "        if epochs_without_improvement >= cfg.early_stopping_patience:\n",
    "            print(f\"early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    if lr_patience_counter >= cfg.lr_patience:\n",
    "        current_learning_rate = max(current_learning_rate * cfg.lr_decay_factor, cfg.min_lr)\n",
    "        print(f\"reducing learning rate to {current_learning_rate}\")\n",
    "        optimizer = optax.adam(current_learning_rate)\n",
    "        opt_state = optimizer.update_hyper_params(opt_state)\n",
    "        lr_patience_counter = 0\n",
    "save_checkpoint(params, \"final.pkl\")\n",
    "#<\\training>\n",
    "\n",
    "#<submission>\n",
    "submission_tasks = load_tasks(cfg.submission_challenges, None, cfg)\n",
    "predictions = {}\n",
    "for i, ckpt in enumerate([\"best.pkl\", \"final.pkl\"]):\n",
    "    params = load_checkpoint(ckpt)\n",
    "    submission_gen = make_dataloader(submission_tasks, cfg, train_mode=False)\n",
    "    for task_id, task_train_in, task_train_out, task_test_in, _ in submission_gen:\n",
    "        task_test_out_pred = model(params, task_train_in, task_train_out, task_test_in, cfg)\n",
    "        if task_id not in predictions:\n",
    "            predictions[task_id] = {}\n",
    "        predictions[task_id][f\"attempt_{i+1}\"] = task_test_out_pred.tolist()\n",
    "submission_filepath = os.path.join(output_dir, \"submission.json\")\n",
    "with open(submission_filepath, 'w') as f:\n",
    "    json.dump(predictions, f)\n",
    "results = {\"accuracy\": valid_acc.item()}\n",
    "results_filepath = os.path.join(output_dir, \"results.json\")\n",
    "with open(results_filepath, 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "if not cfg.compute_backend == \"kaggle\":\n",
    "    wandb.save(submission_filepath)\n",
    "    wandb.save(results_filepath)\n",
    "    wandb.finish()\n",
    "#<\\submission>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
