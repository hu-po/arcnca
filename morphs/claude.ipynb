{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import wandb\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Hyperparams:\n",
    "    seed: int = int(os.environ.get(\"SEED\", 42))\n",
    "    morph: str = os.environ.get(\"MORPH\", \"test\")\n",
    "    compute_backend: str = os.environ.get(\"COMPUTE_BACKEND\", \"oop\")\n",
    "    wandb_entity: str = os.environ.get(\"WANDB_ENTITY\", \"hug\")\n",
    "    wandb_project: str = os.environ.get(\"WANDB_PROJECT\", \"arc-test\")\n",
    "    created_on: str = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    # ---\n",
    "    learning_rate: float = 1e-3\n",
    "    batch_size: int = 8\n",
    "    num_epochs: int = 1\n",
    "    print_every: int = 1\n",
    "    # A \"grid\" is a rectangular matrix (list of lists) of integers between 0 and 9 (inclusive).\n",
    "    # The smallest possible grid size is 1x1 and the largest is 30x30.\n",
    "    pad_dim_space: int = 30\n",
    "    pad_dim_time_train: int = 10  # max number of training pairs\n",
    "    pad_dim_time_eval: int = 3    # max number of eval examples\n",
    "    pad_value: int = 0\n",
    "    num_channels: int = 10 # number of channels for the one-hot encoding (0 to 9 inclusive)\n",
    "    augment_prob: float = 0.5\n",
    "\n",
    "hp = Hyperparams()\n",
    "\n",
    "def load_tasks(challenges_path, solutions_path, hp: Hyperparams):\n",
    "    tasks = []\n",
    "    with open(challenges_path, 'r') as f:\n",
    "        challenges_dict = json.load(f)\n",
    "    print(f\"loading challenges from {challenges_path}, found {len(challenges_dict)} challenges\")\n",
    "    with open(solutions_path, 'r') as f:\n",
    "        solutions_dict = json.load(f)\n",
    "    print(f\"loading solutions from {solutions_path}, found {len(solutions_dict)} solutions\")\n",
    "    for task_id in challenges_dict.keys():\n",
    "        task = challenges_dict[task_id]\n",
    "        if task_id in solutions_dict:\n",
    "            task['test'] = [{'input': inp['input'], 'output': out} \n",
    "                            for inp, out in zip(task['test'], solutions_dict[task_id])]\n",
    "        tasks.append(task)\n",
    "    return tasks\n",
    "\n",
    "@jax.jit\n",
    "def one_hot_encode(grid, num_channels):\n",
    "    return jax.nn.one_hot(grid, num_channels)\n",
    "\n",
    "@partial(jax.jit, static_argnums=(3,))\n",
    "def pad_space(key, grid_in, grid_out, hp):\n",
    "    max_h = jnp.minimum(jnp.maximum(grid_in.shape[0], 0 if grid_out is None else grid_out.shape[0]), hp.pad_dim_space)\n",
    "    max_w = jnp.minimum(jnp.maximum(grid_in.shape[1], 0 if grid_out is None else grid_out.shape[1]), hp.pad_dim_space)\n",
    "    \n",
    "    start_h = jax.random.randint(key, shape=(), minval=0, maxval=hp.pad_dim_space - max_h + 1)\n",
    "    start_w = jax.random.randint(key, shape=(), minval=0, maxval=hp.pad_dim_space - max_w + 1)\n",
    "    \n",
    "    pad_height_in = (start_h, hp.pad_dim_space - grid_in.shape[0] - start_h)\n",
    "    pad_width_in = (start_w, hp.pad_dim_space - grid_in.shape[1] - start_w)\n",
    "    pad_width_in = ((pad_height_in[0], pad_height_in[1]), (pad_width_in[0], pad_width_in[1]), (0, 0))\n",
    "    \n",
    "    grid_in_padded = jnp.pad(grid_in, pad_width_in, mode='constant', constant_values=hp.pad_value)\n",
    "    \n",
    "    if grid_out is not None:\n",
    "        pad_height_out = (start_h, hp.pad_dim_space - grid_out.shape[0] - start_h)\n",
    "        pad_width_out = (start_w, hp.pad_dim_space - grid_out.shape[1] - start_w)\n",
    "        pad_width_out = ((pad_height_out[0], pad_height_out[1]), (pad_width_out[0], pad_width_out[1]), (0, 0))\n",
    "        grid_out_padded = jnp.pad(grid_out, pad_width_out, mode='constant', constant_values=hp.pad_value)\n",
    "    else:\n",
    "        grid_out_padded = None\n",
    "    \n",
    "    return grid_in_padded, grid_out_padded, (start_h, start_w)\n",
    "\n",
    "@partial(jax.jit, static_argnums=(3, 4))\n",
    "def pad_time(key, grid_in, grid_out, hp, is_eval=False):\n",
    "    pad_dim_time = hp.pad_dim_time_eval if is_eval else hp.pad_dim_time_train\n",
    "    \n",
    "    def pad_and_repeat(grid, pad_dim):\n",
    "        num_repeats = pad_dim - grid.shape[0]\n",
    "        indices = jax.random.randint(key, shape=(num_repeats,), minval=0, maxval=grid.shape[0])\n",
    "        repeated = grid[indices]\n",
    "        return jnp.concatenate([grid, repeated], axis=0)\n",
    "    \n",
    "    grid_in_padded = pad_and_repeat(grid_in, pad_dim_time)\n",
    "    \n",
    "    if grid_out is not None:\n",
    "        grid_out_padded = pad_and_repeat(grid_out, pad_dim_time)\n",
    "    else:\n",
    "        grid_out_padded = None\n",
    "    \n",
    "    # Shuffle the time dimension\n",
    "    perm = jax.random.permutation(key, pad_dim_time)\n",
    "    grid_in_padded = grid_in_padded[perm]\n",
    "    if grid_out_padded is not None:\n",
    "        grid_out_padded = grid_out_padded[perm]\n",
    "    \n",
    "    return grid_in_padded, grid_out_padded\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def random_augment(key, grid_in, hp):\n",
    "    def flip_lr(x):\n",
    "        return jnp.fliplr(x)\n",
    "    \n",
    "    def flip_ud(x):\n",
    "        return jnp.flipud(x)\n",
    "    \n",
    "    augmentations = [flip_lr, flip_ud]\n",
    "    \n",
    "    keys = jax.random.split(key, len(augmentations) + 1)\n",
    "    apply = jax.random.bernoulli(keys[0], hp.augment_prob, shape=(len(augmentations),))\n",
    "    \n",
    "    for i, augmentation in enumerate(augmentations):\n",
    "        grid_in = jax.lax.cond(apply[i], augmentation, lambda x: x, grid_in)\n",
    "    \n",
    "    # Shuffle the channel dimension\n",
    "    perm = jax.random.permutation(keys[-1], hp.num_channels)\n",
    "    grid_in = grid_in[..., perm]\n",
    "    \n",
    "    return grid_in\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def process_single_task(key, task, hp):\n",
    "    keys = jax.random.split(key, 5)\n",
    "    \n",
    "    # Unpack the task\n",
    "    train_pairs, test_inputs = task['train'], task['test']\n",
    "    \n",
    "    # Process training data\n",
    "    train_in = [jnp.array(pair['input'], dtype=jnp.int32) for pair in train_pairs]\n",
    "    train_out = [jnp.array(pair['output'], dtype=jnp.int32) for pair in train_pairs]\n",
    "    \n",
    "    # One-hot encode and pad training data\n",
    "    train_in_padded = []\n",
    "    train_out_padded = []\n",
    "    for t_in, t_out, k in zip(train_in, train_out, jax.random.split(keys[0], len(train_in))):\n",
    "        t_in_one_hot = jax.nn.one_hot(t_in, hp.num_channels)\n",
    "        t_out_one_hot = jax.nn.one_hot(t_out, hp.num_channels)\n",
    "        t_in_pad, t_out_pad, _ = pad_space(k, t_in_one_hot, t_out_one_hot, hp)\n",
    "        train_in_padded.append(t_in_pad)\n",
    "        train_out_padded.append(t_out_pad)\n",
    "    \n",
    "    # Stack and pad time for training data\n",
    "    train_in_padded = jnp.stack(train_in_padded)\n",
    "    train_out_padded = jnp.stack(train_out_padded)\n",
    "    train_in_padded, train_out_padded = pad_time(keys[1], train_in_padded, train_out_padded, hp)\n",
    "    \n",
    "    # Process evaluation data\n",
    "    eval_in = [jnp.array(test_input['input'], dtype=jnp.int32) for test_input in test_inputs]\n",
    "    \n",
    "    # One-hot encode and pad evaluation data\n",
    "    eval_in_padded = []\n",
    "    for t_in, k in zip(eval_in, jax.random.split(keys[2], len(eval_in))):\n",
    "        t_in_one_hot = jax.nn.one_hot(t_in, hp.num_channels)\n",
    "        t_in_pad, _, _ = pad_space(k, t_in_one_hot, None, hp)\n",
    "        eval_in_padded.append(t_in_pad)\n",
    "    \n",
    "    # Stack and pad time for evaluation data\n",
    "    eval_in_padded = jnp.stack(eval_in_padded)\n",
    "    eval_in_padded, _ = pad_time(keys[3], eval_in_padded, None, hp, is_eval=True)\n",
    "    \n",
    "    # Apply random augmentations\n",
    "    train_in_padded = jax.vmap(random_augment, in_axes=(0, 0, None))(\n",
    "        jax.random.split(keys[4], train_in_padded.shape[0]), train_in_padded, hp\n",
    "    )\n",
    "    train_out_padded = jax.vmap(random_augment, in_axes=(0, 0, None))(\n",
    "        jax.random.split(keys[4], train_out_padded.shape[0]), train_out_padded, hp\n",
    "    )\n",
    "    \n",
    "    return train_in_padded, train_out_padded, eval_in_padded\n",
    "\n",
    "def create_data_loader(tasks, hp, batch_size, is_train=True):\n",
    "    def prepare_batch(key, batch):\n",
    "        keys = jax.random.split(key, len(batch))\n",
    "        return jax.vmap(process_single_task, in_axes=(0, 0, None))(keys, batch, hp)\n",
    "    \n",
    "    def data_loader(key):\n",
    "        num_tasks = len(tasks)\n",
    "        num_batches = int(jnp.ceil(num_tasks / batch_size))\n",
    "        \n",
    "        if is_train:\n",
    "            key, shuffle_key = jax.random.split(key)\n",
    "            indices = jax.random.permutation(shuffle_key, num_tasks)\n",
    "            tasks_shuffled = [tasks[i] for i in indices]\n",
    "        else:\n",
    "            tasks_shuffled = tasks\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            batch_key, key = jax.random.split(key)\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, num_tasks)\n",
    "            batch = tasks_shuffled[start_idx:end_idx]\n",
    "            yield prepare_batch(batch_key, batch)\n",
    "    \n",
    "    return data_loader\n",
    "\n",
    "# <model>\n",
    "\n",
    "def init_params(key, hp: Hyperparams):\n",
    "    \"\"\"Initialize model parameters.\"\"\"\n",
    "    params = {}\n",
    "    keys = jax.random.split(key, 5)\n",
    "\n",
    "    # Convolutional layers for processing training examples\n",
    "    params['conv_train'] = {\n",
    "        'w': jax.random.normal(keys[0], (3, 3, 2 * hp.num_channels, 16)) * 0.01,\n",
    "        'b': jnp.zeros((16,))\n",
    "    }\n",
    "    print(f\"Initialized conv_train weights with shape: {params['conv_train']['w'].shape}\")\n",
    "\n",
    "    # Convolutional layers for processing evaluation inputs\n",
    "    params['conv_eval'] = {\n",
    "        'w': jax.random.normal(keys[1], (3, 3, hp.num_channels, 16)) * 0.01,\n",
    "        'b': jnp.zeros((16,))\n",
    "    }\n",
    "    print(f\"Initialized conv_eval weights with shape: {params['conv_eval']['w'].shape}\")\n",
    "\n",
    "    # Fully connected layer to combine features\n",
    "    combined_feature_size = 16  # From conv_train + conv_eval outputs\n",
    "    params['fc'] = {\n",
    "        'w': jax.random.normal(keys[2], (combined_feature_size * 2, hp.num_channels)) * 0.01,\n",
    "        'b': jnp.zeros((hp.num_channels,))\n",
    "    }\n",
    "    print(f\"Initialized fc weights with shape: {params['fc']['w'].shape}\")\n",
    "\n",
    "    return params\n",
    "\n",
    "def model(params, task_train_in, task_train_out, task_eval_in, hp):\n",
    "    \"\"\"Model function.\"\"\"\n",
    "\n",
    "    # Debug prints to check shapes\n",
    "    print(f\"task_train_in shape: {task_train_in.shape}\")\n",
    "    print(f\"task_train_out shape: {task_train_out.shape}\")\n",
    "    print(f\"task_eval_in shape: {task_eval_in.shape}\")\n",
    "\n",
    "    # Combine training inputs and outputs\n",
    "    batch_size = task_train_in.shape[0]\n",
    "    pad_dim_time_train = task_train_in.shape[1]\n",
    "    H, W = task_train_in.shape[2], task_train_in.shape[3]\n",
    "\n",
    "    # Concatenate inputs and outputs along the channel dimension\n",
    "    train_combined = jnp.concatenate([task_train_in, task_train_out], axis=-1)  # Now has 2 * hp.num_channels channels\n",
    "\n",
    "    # Process training examples\n",
    "    def process_train_example(train_example):\n",
    "        x = train_example  # Shape: (T, H, W, 2 * num_channels)\n",
    "        x = x.reshape(-1, H, W, 2 * hp.num_channels)  # Combine time dimension\n",
    "        # Debug print\n",
    "        print(f\"Processing train example with shape: {x.shape}\")\n",
    "        x = jax.nn.relu(jax.lax.conv_general_dilated(\n",
    "            x,\n",
    "            params['conv_train']['w'],\n",
    "            window_strides=(1, 1),\n",
    "            padding='SAME',\n",
    "            dimension_numbers=('NHWC', 'HWIO', 'NHWC')\n",
    "        ) + params['conv_train']['b'])\n",
    "        x = jnp.mean(x, axis=(0, 1, 2))  # Global average pooling\n",
    "        return x  # Shape: (features,)\n",
    "\n",
    "    train_features = jax.vmap(process_train_example)(train_combined)\n",
    "\n",
    "    # Process evaluation inputs\n",
    "    def process_eval_input(eval_input):\n",
    "        x = eval_input  # Shape: (T_eval, H, W, num_channels)\n",
    "        x = x.reshape(-1, H, W, hp.num_channels)\n",
    "        # Debug print\n",
    "        print(f\"Processing eval input with shape: {x.shape}\")\n",
    "        x = jax.nn.relu(jax.lax.conv_general_dilated(\n",
    "            x,\n",
    "            params['conv_eval']['w'],\n",
    "            window_strides=(1, 1),\n",
    "            padding='SAME',\n",
    "            dimension_numbers=('NHWC', 'HWIO', 'NHWC')\n",
    "        ) + params['conv_eval']['b'])\n",
    "        x = jnp.mean(x, axis=(0, 1, 2))  # Global average pooling\n",
    "        return x  # Shape: (features,)\n",
    "\n",
    "    eval_features = jax.vmap(process_eval_input)(task_eval_in)\n",
    "\n",
    "    # Combine features\n",
    "    combined_features = jnp.concatenate([train_features, eval_features], axis=-1)  # Shape: (batch_size, combined_features)\n",
    "\n",
    "    # Fully connected layer to predict output logits\n",
    "    logits = combined_features @ params['fc']['w'] + params['fc']['b']  # Shape: (batch_size, num_channels)\n",
    "\n",
    "    # Expand logits to grid shape\n",
    "    pad_dim_time_eval = task_eval_in.shape[1]\n",
    "    H_eval, W_eval = task_eval_in.shape[2], task_eval_in.shape[3]\n",
    "    logits = logits[:, None, None, None, :]  # Shape: (batch_size, 1, 1, 1, num_channels)\n",
    "    logits = jnp.tile(logits, (1, pad_dim_time_eval, H_eval, W_eval, 1))  # Broadcast to grid shape\n",
    "\n",
    "    # Debug print\n",
    "    print(f\"Final logits shape: {logits.shape}\")\n",
    "\n",
    "    return logits  # Shape: (batch_size, pad_dim_time_eval, H, W, num_channels)\n",
    "\n",
    "\n",
    "# </model>\n",
    "\n",
    "def loss_fn(task_eval_out_pred, task_eval_out_targ, hp: Hyperparams):\n",
    "    \"\"\"Compute the loss between predicted and target outputs.\"\"\"\n",
    "    # task_eval_out_pred: (batch_size, pad_dim_time_eval, H, W, num_channels)\n",
    "    # task_eval_out_targ: (batch_size, pad_dim_time_eval, H, W, num_channels)\n",
    "    logits = task_eval_out_pred.reshape(-1, hp.num_channels)\n",
    "    labels = task_eval_out_targ.reshape(-1, hp.num_channels)\n",
    "    labels = jnp.argmax(labels, axis=-1)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    loss = jnp.mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_step(params, opt_state, task_train_in, task_train_out, task_eval_in, task_eval_out, hp: Hyperparams):\n",
    "    def loss_and_grad(params):\n",
    "        task_eval_out_pred = model(params, task_train_in, task_train_out, task_eval_in, hp)\n",
    "        loss = loss_fn(task_eval_out_pred, task_eval_out, hp)\n",
    "        return loss\n",
    "    loss, grads = jax.value_and_grad(loss_and_grad)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "def valid_step(params, valid_gen, hp: Hyperparams):\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    num_batches = 0\n",
    "    for task_train_in, task_train_out, task_eval_in, task_eval_out in valid_gen:\n",
    "        task_eval_out_pred = model(params, task_train_in, task_train_out, task_eval_in, hp)\n",
    "        loss = loss_fn(task_eval_out_pred, task_eval_out, hp)\n",
    "        total_loss += loss\n",
    "        pred_classes = jnp.argmax(task_eval_out_pred, axis=-1)\n",
    "        true_classes = jnp.argmax(task_eval_out, axis=-1)\n",
    "        acc = jnp.mean(pred_classes == true_classes)\n",
    "        total_acc += acc\n",
    "        num_batches += 1\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_acc = total_acc / num_batches\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "print(f\"hyperparameters: {hp}\")\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    entity=hp.wandb_entity,\n",
    "    project=hp.wandb_project,\n",
    "    name=f\"{hp.compute_backend}.{hp.morph}.{str(uuid.uuid4())[:6]}\",\n",
    "    config=hp.__dict__,\n",
    ")\n",
    "key = jax.random.PRNGKey(hp.seed)\n",
    "train_tasks = load_tasks(\n",
    "    '/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json',\n",
    "    '/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json', hp)\n",
    "valid_tasks = load_tasks(\n",
    "    '/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json',\n",
    "    '/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json', hp)\n",
    "train_loader = create_data_loader(train_tasks, hp, batch_size=hp.batch_size, is_train=True)\n",
    "valid_loader = create_data_loader(valid_tasks, hp, batch_size=hp.batch_size, is_train=False)\n",
    "params = init_params(key, hp)\n",
    "optimizer = optax.adam(hp.learning_rate)\n",
    "opt_state = optimizer.init(params)\n",
    "for epoch in range(hp.num_epochs):\n",
    "    print(f\"epoch {epoch + 1}/{hp.num_epochs}\")\n",
    "    steps_per_epoch = int(jnp.ceil(len(train_tasks) / hp.batch_size))\n",
    "    train_key, key = jax.random.split(key)\n",
    "    \n",
    "    # Create an iterator for this epoch\n",
    "    train_iter = train_loader(train_key)\n",
    "    \n",
    "    for step in range(steps_per_epoch):\n",
    "        try:\n",
    "            task_train_in, task_train_out, task_eval_in = next(train_iter)\n",
    "            params, opt_state, train_loss = train_step(params, opt_state, task_train_in, task_train_out, task_eval_in, hp)\n",
    "            if step % hp.print_every == 0:\n",
    "                print(f\"step {step + 1}/{steps_per_epoch}: loss = {train_loss.item():.4f}\")\n",
    "                wandb.log({\"train_loss\": train_loss.item()}, step=step + 1)\n",
    "        except StopIteration:\n",
    "            break\n",
    "    \n",
    "    valid_key, key = jax.random.split(key)\n",
    "    valid_iter = valid_loader(valid_key)\n",
    "    valid_loss, valid_acc = valid_step(params, valid_iter, hp)\n",
    "    print(f'valid_loss: {valid_loss.item():.4f}, valid_acc: {valid_acc.item():.4f}')\n",
    "    wandb.log({\"valid_loss\": valid_loss.item(), \"valid_acc\": valid_acc.item()}, step=(epoch + 1) * steps_per_epoch)\n",
    "\n",
    "# Submission code with corrections\n",
    "submission_challenges_path = '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json'\n",
    "predictions = {}\n",
    "with open(submission_challenges_path, 'r') as f:\n",
    "    challenges_dict = json.load(f)\n",
    "print(f\"loading challenges from {submission_challenges_path}, found {len(challenges_dict)} challenges\")\n",
    "for task_id in challenges_dict.keys():\n",
    "    task = challenges_dict[task_id]\n",
    "    task_train_in = []\n",
    "    task_train_out = []\n",
    "    task_eval_in = []\n",
    "    for pair in task['train']:\n",
    "        task_train_in.append(pair['input'])\n",
    "        task_train_out.append(pair['output'])\n",
    "    for grid in task['test']:\n",
    "        task_eval_in.append(grid['input'])\n",
    "    # Process training data\n",
    "    task_train_in_processed = []\n",
    "    task_train_out_processed = []\n",
    "    for t_in, t_out in zip(task_train_in, task_train_out):\n",
    "        t_in = jnp.array(t_in, dtype=jnp.int32)\n",
    "        t_out = jnp.array(t_out, dtype=jnp.int32)\n",
    "        t_in = jax.nn.one_hot(t_in, hp.num_channels)\n",
    "        t_out = jax.nn.one_hot(t_out, hp.num_channels)\n",
    "        task_train_in_processed.append(t_in)\n",
    "        task_train_out_processed.append(t_out)\n",
    "    num_test_inputs = len(task_eval_in)\n",
    "    outputs_list = [{} for _ in range(num_test_inputs)]\n",
    "    for attempt_id in range(2):\n",
    "        key = jax.random.PRNGKey(attempt_id)\n",
    "        key, subkey1, subkey2 = jax.random.split(key, num=3)\n",
    "        # Pad training data\n",
    "        task_train_in_padded = []\n",
    "        task_train_out_padded = []\n",
    "        k1 = subkey1\n",
    "        for t_in, t_out in zip(task_train_in_processed, task_train_out_processed):\n",
    "            k1, subk = jax.random.split(k1)\n",
    "            t_in_padded, t_out_padded, _ = pad_space(subk, t_in, t_out, hp)\n",
    "            task_train_in_padded.append(t_in_padded)\n",
    "            task_train_out_padded.append(t_out_padded)\n",
    "        task_train_in_padded = jnp.stack(task_train_in_padded)\n",
    "        task_train_out_padded = jnp.stack(task_train_out_padded)\n",
    "        task_train_in_padded, task_train_out_padded = pad_time(subkey2, task_train_in_padded, task_train_out_padded, hp)\n",
    "        # Process each test input\n",
    "        for eval_example_id in range(num_test_inputs):\n",
    "            e_in = task_eval_in[eval_example_id]\n",
    "            e_in = jnp.array(e_in, dtype=jnp.int32)\n",
    "            e_in_one_hot = jax.nn.one_hot(e_in, hp.num_channels)\n",
    "            # Pad eval data\n",
    "            key, subkey3, subkey4 = jax.random.split(key, num=3)\n",
    "            e_in_padded, _, start_pos = pad_space(subkey3, e_in_one_hot, None, hp)\n",
    "            e_in_padded = e_in_padded[None]  # Add time dimension\n",
    "            e_in_padded, _ = pad_time(subkey4, e_in_padded, None, hp, is_eval=True)\n",
    "            # Predict\n",
    "            task_eval_out_pred = model(params, task_train_in_padded[None], task_train_out_padded[None], e_in_padded[None], hp)\n",
    "            task_eval_out_pred = task_eval_out_pred[0, 0]  # Remove batch and time dimension\n",
    "            # Un-padding and converting outputs\n",
    "            grid_out_pred = jnp.argmax(task_eval_out_pred, axis=-1)\n",
    "            sh, sw = start_pos\n",
    "            h = e_in.shape[0]\n",
    "            w = e_in.shape[1]\n",
    "            grid_out_pred = grid_out_pred[sh:sh + h, sw:sw + w]\n",
    "            grid_out_pred = grid_out_pred.tolist()\n",
    "            outputs_list[eval_example_id][f\"attempt_{attempt_id+1}\"] = grid_out_pred\n",
    "    predictions[task_id] = outputs_list\n",
    "\n",
    "if hp.compute_backend in ['oop', 'ojo', 'big']:\n",
    "    output_dir = f\"/arcnca/output{hp.morph}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "else:\n",
    "    # when submitting to kaggle, save the output to the current directory\n",
    "    output_dir = os.getcwd()\n",
    "\n",
    "submission_filepath = os.path.join(output_dir, \"submission.json\")\n",
    "with open('submission.json', 'w') as f:\n",
    "    json.dump(predictions, f)\n",
    "wandb.save(submission_filepath)\n",
    "\n",
    "hparams_filepath = os.path.join(output_dir, \"hparams.json\")\n",
    "with open(hparams_filepath, 'w') as f:\n",
    "    json.dump(hp.__dict__, f)\n",
    "wandb.save(hparams_filepath)\n",
    "\n",
    "results_filepath = os.path.join(output_dir, \"results.json\")\n",
    "with open(results_filepath, 'w') as f:\n",
    "    json.dump({\"accuracy\": valid_acc.item()}, f)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
