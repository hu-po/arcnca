{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import wandb\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Hyperparams:\n",
    "    seed: int = int(os.environ.get(\"SEED\", 42))\n",
    "    morph: str = os.environ.get(\"MORPH\", \"test\")\n",
    "    compute_backend: str = os.environ.get(\"COMPUTE_BACKEND\", \"oop\")\n",
    "    wandb_entity: str = os.environ.get(\"WANDB_ENTITY\", \"hug\")\n",
    "    wandb_project: str = os.environ.get(\"WANDB_PROJECT\", \"arc-test\")\n",
    "    created_on: str = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    # ---\n",
    "    learning_rate: float = 1e-3\n",
    "    batch_size: int = 8\n",
    "    num_epochs: int = 1\n",
    "    print_every: int = 1\n",
    "    pad_dim_space: int = 30\n",
    "    pad_dim_time_train: int = 10  # max number of training pairs\n",
    "    pad_dim_time_eval: int = 3    # max number of eval examples\n",
    "    pad_value: int = 0\n",
    "    num_channels: int = 10\n",
    "    augment_prob: float = 0.5\n",
    "\n",
    "hp = Hyperparams()\n",
    "\n",
    "def load_tasks(challenges_path, solutions_path, hp: Hyperparams):\n",
    "    \"\"\"Loads raw dataset from json files, stored in RAM.\"\"\"\n",
    "    tasks = []\n",
    "    with open(challenges_path, 'r') as f:\n",
    "        challenges_dict = json.load(f)\n",
    "    print(f\"loading challenges from {challenges_path}, found {len(challenges_dict)} challenges\")\n",
    "    with open(solutions_path, 'r') as f:\n",
    "        solutions_dict = json.load(f)\n",
    "    print(f\"loading solutions from {solutions_path}, found {len(solutions_dict)} solutions\")\n",
    "    for task_id in challenges_dict.keys():\n",
    "        print(f\"\\t task {task_id}\")\n",
    "        task_train_in = []\n",
    "        task_train_out = []\n",
    "        task_eval_in = []\n",
    "        task_eval_out = []\n",
    "        # there may be multiple training pairs for each task\n",
    "        for pair in challenges_dict[task_id]['train']:\n",
    "            task_train_in.append(pair['input'])\n",
    "            task_train_out.append(pair['output'])\n",
    "            print(f\"shape of task_train_in {jnp.array(pair['input']).shape}\")\n",
    "            print(f\"shape of task_train_out {jnp.array(pair['output']).shape}\")\n",
    "        for grid in challenges_dict[task_id]['test']:\n",
    "            task_eval_in.append(grid['input'])\n",
    "            print(f\"shape of task_eval_in {jnp.array(grid['input']).shape}\")\n",
    "        if task_id in solutions_dict:\n",
    "            for grid in solutions_dict[task_id]:\n",
    "                task_eval_out.append(grid)\n",
    "                print(f\"shape of task_eval_out {jnp.array(grid).shape}\")\n",
    "            assert len(task_eval_in) == len(task_eval_out)\n",
    "        else:\n",
    "            task_eval_out = [None] * len(task_eval_in)\n",
    "        assert len(task_train_in) == len(task_train_out)\n",
    "        assert len(task_eval_in) <= hp.pad_dim_time_eval\n",
    "        assert len(task_train_in) <= hp.pad_dim_time_train\n",
    "        tasks.append((task_train_in, task_train_out, task_eval_in, task_eval_out))\n",
    "    return tasks\n",
    "\n",
    "def pad_space(key, grid_in, grid_out, hp: Hyperparams):\n",
    "    # grids are [H, W, C], pick random start position in space based on max grid shape\n",
    "    grid_in_h, grid_in_w = grid_in.shape[:2]\n",
    "    if grid_out is not None:\n",
    "        grid_out_h, grid_out_w = grid_out.shape[:2]\n",
    "        max_h = max(grid_in_h, grid_out_h)\n",
    "        max_w = max(grid_in_w, grid_out_w)\n",
    "    else:\n",
    "        max_h, max_w = grid_in_h, grid_in_w\n",
    "    max_h = min(max_h, hp.pad_dim_space)\n",
    "    max_w = min(max_w, hp.pad_dim_space)\n",
    "    assert max_h <= hp.pad_dim_space\n",
    "    assert max_w <= hp.pad_dim_space\n",
    "    start_h = jax.random.randint(key, shape=(), minval=0, maxval=hp.pad_dim_space - max_h + 1)\n",
    "    start_w = jax.random.randint(key, shape=(), minval=0, maxval=hp.pad_dim_space - max_w + 1)\n",
    "    pad_height_in = (start_h, hp.pad_dim_space - grid_in_h - start_h)\n",
    "    pad_width_in = (start_w, hp.pad_dim_space - grid_in_w - start_w)\n",
    "    pad_width_in = (pad_height_in, pad_width_in, (0, 0))\n",
    "    grid_in = jnp.pad(grid_in, pad_width_in, mode='constant', constant_values=hp.pad_value)\n",
    "    if grid_out is not None:\n",
    "        pad_height_out = (start_h, hp.pad_dim_space - grid_out_h - start_h)\n",
    "        pad_width_out = (start_w, hp.pad_dim_space - grid_out_w - start_w)\n",
    "        pad_width_out = (pad_height_out, pad_width_out, (0, 0))\n",
    "        grid_out = jnp.pad(grid_out, pad_width_out, mode='constant', constant_values=hp.pad_value)\n",
    "    else:\n",
    "        grid_out = None\n",
    "    return grid_in, grid_out, (start_h, start_w)\n",
    "\n",
    "def pad_time(key, grid_in, grid_out, hp: Hyperparams, is_eval=False):\n",
    "    # different padding in time for eval and train examples\n",
    "    if is_eval:\n",
    "        pad_dim_time = hp.pad_dim_time_eval\n",
    "    else:\n",
    "        pad_dim_time = hp.pad_dim_time_train\n",
    "    # grids are [T, H, W, C], repeat random example in time until T == pad_dim_time\n",
    "    assert grid_in.shape[0] <= pad_dim_time\n",
    "    assert grid_in.shape[1] <= hp.pad_dim_space\n",
    "    assert grid_in.shape[2] <= hp.pad_dim_space\n",
    "    grid_in_list = [grid_in[i] for i in range(grid_in.shape[0])]\n",
    "    grid_out_list = [grid_out[i] for i in range(grid_out.shape[0])] if grid_out is not None else None\n",
    "    for _ in range(pad_dim_time - grid_in.shape[0]):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        repeat_example_idx = jax.random.randint(subkey, shape=(), minval=0, maxval=grid_in.shape[0])\n",
    "        grid_in_list.append(grid_in[repeat_example_idx])\n",
    "        if grid_out is not None:\n",
    "            grid_out_list.append(grid_out[repeat_example_idx])\n",
    "    grid_in = jnp.stack(grid_in_list)\n",
    "    if grid_out is not None:\n",
    "        grid_out = jnp.stack(grid_out_list)\n",
    "    # shuffle the T dimension\n",
    "    key, subkey = jax.random.split(key)\n",
    "    perm = jax.random.permutation(subkey, pad_dim_time)\n",
    "    grid_in = grid_in[perm]\n",
    "    if grid_out is not None:\n",
    "        grid_out = grid_out[perm]\n",
    "    return grid_in, grid_out\n",
    "\n",
    "# Remove rotation for rectangular grids\n",
    "augmentations = [jnp.fliplr, jnp.flipud]\n",
    "\n",
    "def random_augment(key, grid_in, grid_out, hp: Hyperparams):\n",
    "    apply = jax.random.bernoulli(key, hp.augment_prob, shape=(len(augmentations),))\n",
    "    for i, augmentation in enumerate(augmentations):\n",
    "        grid_in = jax.lax.cond(apply[i], augmentation, lambda x: x, grid_in)\n",
    "        if grid_out is not None:\n",
    "            grid_out = jax.lax.cond(apply[i], augmentation, lambda x: x, grid_out)\n",
    "    # shuffle the C dimension\n",
    "    key, subkey = jax.random.split(key)\n",
    "    perm = jax.random.permutation(subkey, hp.num_channels)\n",
    "    grid_in = grid_in[..., perm]\n",
    "    if grid_out is not None:\n",
    "        grid_out = grid_out[..., perm]\n",
    "    return grid_in, grid_out\n",
    "\n",
    "def data_loader(key, tasks, hp: Hyperparams, train_mode=True):\n",
    "    num_tasks = len(tasks)\n",
    "    if train_mode:\n",
    "        key, subkey = jax.random.split(key)\n",
    "        indices = jax.random.permutation(subkey, num_tasks)\n",
    "        tasks = [tasks[idx] for idx in indices]\n",
    "    num_batches = int(jnp.ceil(num_tasks / hp.batch_size))\n",
    "    for i in range(num_batches):\n",
    "        batch_key, key = jax.random.split(key)\n",
    "        start_idx = i * hp.batch_size\n",
    "        end_idx = min((i + 1) * hp.batch_size, num_tasks)\n",
    "        current_batch_size = end_idx - start_idx\n",
    "        subkeys = jax.random.split(batch_key, current_batch_size * 5)\n",
    "        subkeys = subkeys.reshape(current_batch_size, 5, 2)\n",
    "        batch_train_in = []\n",
    "        batch_train_out = []\n",
    "        batch_eval_in = []\n",
    "        batch_eval_out = []\n",
    "        for idx_in_batch, (j, subkey) in enumerate(zip(range(start_idx, end_idx), subkeys)):\n",
    "            k1, k2, k3, k4, k5 = subkey\n",
    "            k1 = jax.random.PRNGKey(k1[0])\n",
    "            k2 = jax.random.PRNGKey(k2[0])\n",
    "            k3 = jax.random.PRNGKey(k3[0])\n",
    "            k4 = jax.random.PRNGKey(k4[0])\n",
    "            k5 = jax.random.PRNGKey(k5[0])\n",
    "\n",
    "            # Convert each grid to a JAX array individually\n",
    "            task_train_in = [jnp.array(grid, dtype=jnp.int32) for grid in tasks[j][0]]\n",
    "            task_train_out = [jnp.array(grid, dtype=jnp.int32) for grid in tasks[j][1]]\n",
    "            task_eval_in = [jnp.array(grid, dtype=jnp.int32) for grid in tasks[j][2]]\n",
    "            if tasks[j][3][0] is not None:\n",
    "                task_eval_out = [jnp.array(grid, dtype=jnp.int32) for grid in tasks[j][3]]\n",
    "            else:\n",
    "                task_eval_out = None\n",
    "\n",
    "            # Convert grids to one-hot encoding individually\n",
    "            task_train_in = [jax.nn.one_hot(grid, hp.num_channels) for grid in task_train_in]\n",
    "            task_train_out = [jax.nn.one_hot(grid, hp.num_channels) for grid in task_train_out]\n",
    "            task_eval_in = [jax.nn.one_hot(grid, hp.num_channels) for grid in task_eval_in]\n",
    "            if task_eval_out is not None:\n",
    "                task_eval_out = [jax.nn.one_hot(grid, hp.num_channels) for grid in task_eval_out]\n",
    "\n",
    "            # Pad in space\n",
    "            task_train_in_padded = []\n",
    "            task_train_out_padded = []\n",
    "            k1_grid = k1\n",
    "            for t_in, t_out in zip(task_train_in, task_train_out):\n",
    "                k1_grid, subk = jax.random.split(k1_grid)\n",
    "                t_in_padded, t_out_padded, _ = pad_space(subk, t_in, t_out, hp)\n",
    "                task_train_in_padded.append(t_in_padded)\n",
    "                task_train_out_padded.append(t_out_padded)\n",
    "            task_train_in_padded = jnp.stack(task_train_in_padded)\n",
    "            task_train_out_padded = jnp.stack(task_train_out_padded)\n",
    "\n",
    "            # Pad in time\n",
    "            task_train_in_padded, task_train_out_padded = pad_time(k2, task_train_in_padded, task_train_out_padded, hp)\n",
    "\n",
    "            # Same for eval data\n",
    "            task_eval_in_padded = []\n",
    "            task_eval_out_padded = []\n",
    "            k3_grid = k3\n",
    "            for idx, e_in in enumerate(task_eval_in):\n",
    "                k3_grid, subk = jax.random.split(k3_grid)\n",
    "                e_out = task_eval_out[idx] if task_eval_out is not None else None\n",
    "                e_in_padded, e_out_padded, _ = pad_space(subk, e_in, e_out, hp)\n",
    "                task_eval_in_padded.append(e_in_padded)\n",
    "                if e_out_padded is not None:\n",
    "                    task_eval_out_padded.append(e_out_padded)\n",
    "            task_eval_in_padded = jnp.stack(task_eval_in_padded)\n",
    "            if task_eval_out_padded:\n",
    "                task_eval_out_padded = jnp.stack(task_eval_out_padded)\n",
    "            else:\n",
    "                task_eval_out_padded = None\n",
    "            task_eval_in_padded, task_eval_out_padded = pad_time(k4, task_eval_in_padded, task_eval_out_padded, hp, is_eval=True)\n",
    "\n",
    "            if train_mode:\n",
    "                k4_aug, k5_aug = jax.random.split(k5)\n",
    "                task_train_in_padded, task_train_out_padded = random_augment(k4_aug, task_train_in_padded, task_train_out_padded, hp)\n",
    "                task_eval_in_padded, task_eval_out_padded = random_augment(k5_aug, task_eval_in_padded, task_eval_out_padded, hp)\n",
    "\n",
    "            batch_train_in.append(task_train_in_padded)\n",
    "            batch_train_out.append(task_train_out_padded)\n",
    "            batch_eval_in.append(task_eval_in_padded)\n",
    "            if task_eval_out_padded is not None:\n",
    "                batch_eval_out.append(task_eval_out_padded)\n",
    "            else:\n",
    "                batch_eval_out.append(jnp.zeros_like(task_eval_in_padded))\n",
    "\n",
    "        task_train_in = jnp.stack(batch_train_in)\n",
    "        task_train_out = jnp.stack(batch_train_out)\n",
    "        task_eval_in = jnp.stack(batch_eval_in)\n",
    "        task_eval_out = jnp.stack(batch_eval_out)\n",
    "        yield task_train_in, task_train_out, task_eval_in, task_eval_out\n",
    "\n",
    "# <model>\n",
    "\n",
    "def init_params(key, hp: Hyperparams):\n",
    "    \"\"\"Initialize model parameters.\"\"\"\n",
    "    params = {}\n",
    "    keys = jax.random.split(key, 5)\n",
    "    \n",
    "    # Convolutional layers for processing training examples\n",
    "    params['conv_train'] = {\n",
    "        'w': jax.random.normal(keys[0], (3, 3, 2 * hp.num_channels, 16)) * 0.01,\n",
    "        'b': jnp.zeros((16,))\n",
    "    }\n",
    "    \n",
    "    # Convolutional layers for processing evaluation inputs\n",
    "    params['conv_eval'] = {\n",
    "        'w': jax.random.normal(keys[1], (3, 3, hp.num_channels, 16)) * 0.01,\n",
    "        'b': jnp.zeros((16,))\n",
    "    }\n",
    "    \n",
    "    # Fully connected layer to combine features\n",
    "    params['fc'] = {\n",
    "        'w': jax.random.normal(keys[2], (32, hp.num_channels)) * 0.01,\n",
    "        'b': jnp.zeros((hp.num_channels,))\n",
    "    }\n",
    "    return params\n",
    "\n",
    "def model(params, task_train_in, task_train_out, task_eval_in, hp: Hyperparams):\n",
    "    \"\"\"Model function.\"\"\"\n",
    "    # Rearrange axes to put channels last\n",
    "    task_train_in = task_train_in.transpose(0, 1, 3, 4, 2)\n",
    "    task_train_out = task_train_out.transpose(0, 1, 3, 4, 2)\n",
    "    task_eval_in = task_eval_in.transpose(0, 1, 3, 4, 2)\n",
    "    \n",
    "    # Combine training inputs and outputs\n",
    "    batch_size = task_train_in.shape[0]\n",
    "    pad_dim_time_train = task_train_in.shape[1]\n",
    "    H, W = task_train_in.shape[2], task_train_in.shape[3]\n",
    "    \n",
    "    # Flatten time dimension and concatenate inputs and outputs\n",
    "    train_in = task_train_in.reshape(batch_size, pad_dim_time_train, H, W, hp.num_channels)\n",
    "    train_out = task_train_out.reshape(batch_size, pad_dim_time_train, H, W, hp.num_channels)\n",
    "    train_combined = jnp.concatenate([train_in, train_out], axis=-1)  # Now has 2 * hp.num_channels channels\n",
    "\n",
    "    # Process training examples\n",
    "    def process_train_example(train_example):\n",
    "        x = train_example  # Shape: (T, H, W, 2 * num_channels)\n",
    "        x = x.reshape(-1, H, W, 2 * hp.num_channels)  # Combine time dimension\n",
    "        x = jax.nn.relu(jax.lax.conv_general_dilated(\n",
    "            x,\n",
    "            params['conv_train']['w'],\n",
    "            window_strides=(1, 1),\n",
    "            padding='SAME',\n",
    "            dimension_numbers=('NHWC', 'HWIO', 'NHWC')\n",
    "        ) + params['conv_train']['b'])\n",
    "        x = jnp.mean(x, axis=(1, 2))  # Global average pooling\n",
    "        return x  # Shape: (T * batch_size, features)\n",
    "\n",
    "    train_features = jax.vmap(process_train_example)(train_combined)\n",
    "    train_features = jnp.mean(train_features, axis=1)  # Aggregate over time dimension\n",
    "\n",
    "    # Process evaluation inputs\n",
    "    def process_eval_input(eval_input):\n",
    "        x = eval_input  # Shape: (T_eval, H, W, num_channels)\n",
    "        x = x.reshape(-1, H, W, hp.num_channels)\n",
    "        x = jax.nn.relu(jax.lax.conv_general_dilated(\n",
    "            x,\n",
    "            params['conv_eval']['w'],\n",
    "            window_strides=(1, 1),\n",
    "            padding='SAME',\n",
    "            dimension_numbers=('NHWC', 'HWIO', 'NHWC')\n",
    "        ) + params['conv_eval']['b'])\n",
    "        x = jnp.mean(x, axis=(1, 2))  # Global average pooling\n",
    "        return x  # Shape: (T_eval * batch_size, features)\n",
    "\n",
    "    eval_features = jax.vmap(process_eval_input)(task_eval_in)\n",
    "    eval_features = jnp.mean(eval_features, axis=1)  # Aggregate over time dimension\n",
    "\n",
    "    # Combine features\n",
    "    combined_features = jnp.concatenate([train_features, eval_features], axis=-1)  # Shape: (batch_size, combined_features)\n",
    "\n",
    "    # Fully connected layer to predict output logits\n",
    "    logits = combined_features @ params['fc']['w'] + params['fc']['b']  # Shape: (batch_size, num_channels)\n",
    "\n",
    "    # Expand logits to grid shape\n",
    "    pad_dim_time_eval = task_eval_in.shape[1]\n",
    "    H_eval, W_eval = task_eval_in.shape[2], task_eval_in.shape[3]\n",
    "    logits = logits[:, None, None, None, :]  # Shape: (batch_size, 1, 1, 1, num_channels)\n",
    "    logits = jnp.tile(logits, (1, pad_dim_time_eval, H_eval, W_eval, 1))  # Broadcast to grid shape\n",
    "\n",
    "    return logits  # Shape: (batch_size, pad_dim_time_eval, H, W, num_channels)\n",
    "\n",
    "# </model>\n",
    "\n",
    "def loss_fn(task_eval_out_pred, task_eval_out_targ, hp: Hyperparams):\n",
    "    \"\"\"Compute the loss between predicted and target outputs.\"\"\"\n",
    "    # task_eval_out_pred: (batch_size, pad_dim_time_eval, H, W, num_channels)\n",
    "    # task_eval_out_targ: (batch_size, pad_dim_time_eval, H, W, num_channels)\n",
    "    logits = task_eval_out_pred.reshape(-1, hp.num_channels)\n",
    "    labels = task_eval_out_targ.reshape(-1, hp.num_channels)\n",
    "    labels = jnp.argmax(labels, axis=-1)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    loss = jnp.mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_step(params, opt_state, task_train_in, task_train_out, task_eval_in, task_eval_out, hp: Hyperparams):\n",
    "    def loss_and_grad(params):\n",
    "        task_eval_out_pred = model(params, task_train_in, task_train_out, task_eval_in, hp)\n",
    "        loss = loss_fn(task_eval_out_pred, task_eval_out, hp)\n",
    "        return loss\n",
    "    loss, grads = jax.value_and_grad(loss_and_grad)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "def valid_step(params, valid_gen, hp: Hyperparams):\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    num_batches = 0\n",
    "    for task_train_in, task_train_out, task_eval_in, task_eval_out in valid_gen:\n",
    "        task_eval_out_pred = model(params, task_train_in, task_train_out, task_eval_in, hp)\n",
    "        loss = loss_fn(task_eval_out_pred, task_eval_out, hp)\n",
    "        total_loss += loss\n",
    "        pred_classes = jnp.argmax(task_eval_out_pred, axis=-1)\n",
    "        true_classes = jnp.argmax(task_eval_out, axis=-1)\n",
    "        acc = jnp.mean(pred_classes == true_classes)\n",
    "        total_acc += acc\n",
    "        num_batches += 1\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_acc = total_acc / num_batches\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "print(f\"hyperparameters: {hp}\")\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    entity=hp.wandb_entity,\n",
    "    project=hp.wandb_project,\n",
    "    name=f\"{hp.morph}.{hp.compute_backend}.{str(uuid.uuid4())[:6]}\",\n",
    "    config=hp.__dict__,\n",
    ")\n",
    "key = jax.random.PRNGKey(hp.seed)\n",
    "train_tasks = load_tasks(\n",
    "    '/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json',\n",
    "    '/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json', hp)\n",
    "valid_tasks = load_tasks(\n",
    "    '/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json',\n",
    "    '/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json', hp)\n",
    "params = init_params(key, hp)\n",
    "optimizer = optax.adam(hp.learning_rate)\n",
    "opt_state = optimizer.init(params)\n",
    "for epoch in range(hp.num_epochs):\n",
    "    print(f\"epoch {epoch + 1}/{hp.num_epochs}\")\n",
    "    steps_per_epoch = int(jnp.ceil(len(train_tasks) / hp.batch_size))\n",
    "    train_key, key = jax.random.split(key)\n",
    "    train_gen = data_loader(train_key, train_tasks, hp, train_mode=True)\n",
    "    for step in range(steps_per_epoch):\n",
    "        try:\n",
    "            task_train_in, task_train_out, task_eval_in, task_eval_out = next(train_gen)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        params, opt_state, train_loss = train_step(params, opt_state, task_train_in, task_train_out, task_eval_in, task_eval_out, hp)\n",
    "        if step % hp.print_every == 0:\n",
    "            print(f\"step {step + 1}/{steps_per_epoch}: loss = {train_loss.item():.4f}\")\n",
    "            wandb.log({\"train_loss\": train_loss.item()}, step=step + 1)\n",
    "    valid_key, key = jax.random.split(key)\n",
    "    valid_gen = data_loader(valid_key, valid_tasks, hp, train_mode=False)\n",
    "    valid_loss, valid_acc = valid_step(params, valid_gen, hp)\n",
    "    print(f'valid_loss: {valid_loss.item():.4f}, valid_acc: {valid_acc.item():.4f}')\n",
    "    wandb.log({\"valid_loss\": valid_loss.item(), \"valid_acc\": valid_acc.item()}, step=(epoch + 1) * steps_per_epoch)\n",
    "wandb.finish()\n",
    "\n",
    "# Submission code with corrections\n",
    "submission_challenges_path = '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json'\n",
    "predictions = {}\n",
    "with open(submission_challenges_path, 'r') as f:\n",
    "    challenges_dict = json.load(f)\n",
    "print(f\"loading challenges from {submission_challenges_path}, found {len(challenges_dict)} challenges\")\n",
    "for task_id in challenges_dict.keys():\n",
    "    task = challenges_dict[task_id]\n",
    "    task_train_in = []\n",
    "    task_train_out = []\n",
    "    task_eval_in = []\n",
    "    for pair in task['train']:\n",
    "        task_train_in.append(pair['input'])\n",
    "        task_train_out.append(pair['output'])\n",
    "    for grid in task['test']:\n",
    "        task_eval_in.append(grid['input'])\n",
    "    # Process training data\n",
    "    task_train_in_processed = []\n",
    "    task_train_out_processed = []\n",
    "    for t_in, t_out in zip(task_train_in, task_train_out):\n",
    "        t_in = jnp.array(t_in, dtype=jnp.int32)\n",
    "        t_out = jnp.array(t_out, dtype=jnp.int32)\n",
    "        t_in = jax.nn.one_hot(t_in, hp.num_channels)\n",
    "        t_out = jax.nn.one_hot(t_out, hp.num_channels)\n",
    "        task_train_in_processed.append(t_in)\n",
    "        task_train_out_processed.append(t_out)\n",
    "    num_test_inputs = len(task_eval_in)\n",
    "    outputs_list = [{} for _ in range(num_test_inputs)]\n",
    "    for attempt_id in range(2):\n",
    "        key = jax.random.PRNGKey(attempt_id)\n",
    "        key, subkey1, subkey2 = jax.random.split(key, num=3)\n",
    "        # Pad training data\n",
    "        task_train_in_padded = []\n",
    "        task_train_out_padded = []\n",
    "        k1 = subkey1\n",
    "        for t_in, t_out in zip(task_train_in_processed, task_train_out_processed):\n",
    "            k1, subk = jax.random.split(k1)\n",
    "            t_in_padded, t_out_padded, _ = pad_space(subk, t_in, t_out, hp)\n",
    "            task_train_in_padded.append(t_in_padded)\n",
    "            task_train_out_padded.append(t_out_padded)\n",
    "        task_train_in_padded = jnp.stack(task_train_in_padded)\n",
    "        task_train_out_padded = jnp.stack(task_train_out_padded)\n",
    "        task_train_in_padded, task_train_out_padded = pad_time(subkey2, task_train_in_padded, task_train_out_padded, hp)\n",
    "        # Process each test input\n",
    "        for eval_example_id in range(num_test_inputs):\n",
    "            e_in = task_eval_in[eval_example_id]\n",
    "            e_in = jnp.array(e_in, dtype=jnp.int32)\n",
    "            e_in_one_hot = jax.nn.one_hot(e_in, hp.num_channels)\n",
    "            # Pad eval data\n",
    "            key, subkey3, subkey4 = jax.random.split(key, num=3)\n",
    "            e_in_padded, _, start_pos = pad_space(subkey3, e_in_one_hot, None, hp)\n",
    "            e_in_padded = e_in_padded[None]  # Add time dimension\n",
    "            e_in_padded, _ = pad_time(subkey4, e_in_padded, None, hp, is_eval=True)\n",
    "            # Predict\n",
    "            task_eval_out_pred = model(params, task_train_in_padded[None], task_train_out_padded[None], e_in_padded[None], hp)\n",
    "            task_eval_out_pred = task_eval_out_pred[0, 0]  # Remove batch and time dimension\n",
    "            # Un-padding and converting outputs\n",
    "            grid_out_pred = jnp.argmax(task_eval_out_pred, axis=-1)\n",
    "            sh, sw = start_pos\n",
    "            h = e_in.shape[0]\n",
    "            w = e_in.shape[1]\n",
    "            grid_out_pred = grid_out_pred[sh:sh + h, sw:sw + w]\n",
    "            grid_out_pred = grid_out_pred.tolist()\n",
    "            outputs_list[eval_example_id][f\"attempt_{attempt_id+1}\"] = grid_out_pred\n",
    "    predictions[task_id] = outputs_list\n",
    "\n",
    "with open('submission.json', 'w') as f:\n",
    "    json.dump(predictions, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
