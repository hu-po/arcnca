{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'pytest>=8.3.2' 'numpy>=1.26.4' 'pillow>=10.4.0' 'msgpack>=1.1.0' 'requests>=2.32.3' 'mediapy>=1.2.2' tqdm\n",
    "!pip install --no-deps 'optax==0.2.3' 'chex==0.1.86' 'flax>=0.9.0' orbax-checkpoint tensorstore 'typing-extensions>=4.2' 'absl-py>=2.1.0' 'toolz>=1.0.0' 'etils[epy]>=1.9.4'\n",
    "!git clone https://github.com/hu-po/cax.git /cax\n",
    "!pip install --upgrade /cax --no-deps\n",
    "!pytest --color=no /cax/tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "import os\n",
    "import psutil\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jaxlib\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import flax\n",
    "from flax import nnx\n",
    "import optax\n",
    "import cax\n",
    "\n",
    "for pkg in [jax, jaxlib, cax, flax, optax]:\n",
    "    print(pkg.__name__, pkg.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_memory():\n",
    "    used_mem_mb = None\n",
    "    total_mem_mb = None\n",
    "\n",
    "    # Try to get memory info using nvidia-smi\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=memory.total,memory.used', '--format=csv,noheader,nounits'],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        total_mem = used_mem = 0\n",
    "        for line in result.stdout.strip().split('\\n'):\n",
    "            total, used = map(float, line.strip().split(', '))\n",
    "            total_mem += total\n",
    "            used_mem += used\n",
    "        return used_mem, total_mem\n",
    "    except Exception:\n",
    "        pass  # nvidia-smi not available\n",
    "\n",
    "    # Try to get memory info from JAX devices\n",
    "    try:\n",
    "        devices = jax.devices()\n",
    "        if devices:\n",
    "            total_mem = sum(device.memory_size() for device in devices)\n",
    "            total_mem_mb = total_mem / (1024 * 1024)  # Bytes to MB\n",
    "            # Used memory is not readily available via JAX\n",
    "            print(f\"Total device memory: {total_mem_mb:.2f} MB\")\n",
    "            return None, total_mem_mb\n",
    "    except Exception:\n",
    "        pass  # JAX not available or no devices found\n",
    "\n",
    "    # Try to get memory info using tegrastats (for AGX Orin devices)\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['tegrastats', '--interval', '1', '--count', '1'],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        output = result.stdout.strip()\n",
    "        # Parse tegrastats output if necessary\n",
    "        print(\"Tegrastats output:\", output)\n",
    "        return None, None\n",
    "    except Exception:\n",
    "        pass  # tegrastats not available\n",
    "\n",
    "    # Fallback to system memory info using psutil\n",
    "    try:\n",
    "        mem = psutil.virtual_memory()\n",
    "        total_mem_mb = mem.total / (1024 * 1024)\n",
    "        used_mem_mb = mem.used / (1024 * 1024)\n",
    "        return used_mem_mb, total_mem_mb\n",
    "    except Exception:\n",
    "        pass  # psutil not available\n",
    "\n",
    "    print(\"Could not retrieve device memory usage.\")\n",
    "    return None, None\n",
    "\n",
    "# Load data\n",
    "def load_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {len(data)} tasks from {path}\")\n",
    "    return data\n",
    "\n",
    "train_challenges = load_data('/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json')\n",
    "train_solutions = load_data('/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json')\n",
    "eval_challenges = load_data('/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json')\n",
    "eval_solutions = load_data('/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json')\n",
    "\n",
    "# Process tasks\n",
    "def process_tasks(challenges, solutions):\n",
    "    data = []\n",
    "    task_id_to_index = {}\n",
    "    for index, task_id in enumerate(challenges.keys()):\n",
    "        task_id_to_index[task_id] = index\n",
    "        task = challenges[task_id]\n",
    "        solution = solutions[task_id]\n",
    "        for pair in task['train']:\n",
    "            input_grid = np.array(pair['input'], dtype=np.int32)\n",
    "            output_grid = np.array(pair['output'], dtype=np.int32)\n",
    "            data.append((input_grid, output_grid, index))\n",
    "        for i, test_input in enumerate(task['test']):\n",
    "            input_grid = np.array(test_input['input'], dtype=np.int32)\n",
    "            output_grid = np.array(solution[i], dtype=np.int32)\n",
    "            data.append((input_grid, output_grid, index))\n",
    "    return data, task_id_to_index\n",
    "\n",
    "# Pad grid function\n",
    "def pad_grid(grid, max_size=30, pad_value=0, center_offset=(0, 0)):\n",
    "    padded = np.full((max_size, max_size), pad_value, dtype=np.int32)\n",
    "    rows, cols = grid.shape\n",
    "    offset_row = (max_size - rows) // 2 + center_offset[0]\n",
    "    offset_col = (max_size - cols) // 2 + center_offset[1]\n",
    "    # Ensure offsets are within bounds\n",
    "    offset_row = np.clip(offset_row, 0, max_size - rows)\n",
    "    offset_col = np.clip(offset_col, 0, max_size - cols)\n",
    "    padded[offset_row:offset_row+rows, offset_col:offset_col+cols] = grid\n",
    "    return padded\n",
    "\n",
    "# Augmentation functions\n",
    "@jax.jit\n",
    "def flip_horizontal(grid):\n",
    "    return jnp.fliplr(grid)\n",
    "\n",
    "@jax.jit\n",
    "def flip_vertical(grid):\n",
    "    return jnp.flipud(grid)\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1,))\n",
    "def rotate(grid, k):\n",
    "    return jnp.rot90(grid, k=k)\n",
    "\n",
    "# Mapping from augmentation names to functions\n",
    "AUGMENTATIONS = {\n",
    "    'flip_horizontal': flip_horizontal,\n",
    "    'flip_vertical': flip_vertical,\n",
    "    'rotate_90': partial(rotate, k=1),\n",
    "    'rotate_180': partial(rotate, k=2),\n",
    "    'rotate_270': partial(rotate, k=3),\n",
    "}\n",
    "\n",
    "# Apply random augmentations to a batch\n",
    "def apply_random_augmentations_batch(input_grids, output_grids, rng_keys, augmentations):\n",
    "    num_augs = len(augmentations)\n",
    "\n",
    "    def augment_sample(input_grid, output_grid, rng_key):\n",
    "        apply_aug = jax.random.bernoulli(rng_key, p=0.5, shape=(num_augs,))\n",
    "        for i, aug_name in enumerate(augmentations):\n",
    "            aug_func = AUGMENTATIONS[aug_name]\n",
    "            input_grid, output_grid = jax.lax.cond(\n",
    "                apply_aug[i],\n",
    "                lambda grids: (aug_func(grids[0]), aug_func(grids[1])),\n",
    "                lambda grids: grids,\n",
    "                (input_grid, output_grid)\n",
    "            )\n",
    "        return input_grid, output_grid\n",
    "\n",
    "    # Vectorize over the batch dimension\n",
    "    v_augment_sample = jax.vmap(augment_sample, in_axes=(0, 0, 0), out_axes=(0, 0))\n",
    "    augmented_inputs, augmented_outputs = v_augment_sample(input_grids, output_grids, rng_keys)\n",
    "    return augmented_inputs, augmented_outputs\n",
    "\n",
    "# Data generator\n",
    "def data_generator(data, batch_size, augmentations=None, max_size=30, pad_value=0,\n",
    "                   center_offset=(0, 0), shuffle=True):\n",
    "    num_samples = len(data)\n",
    "    indices = np.arange(num_samples)\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            rng = np.random.default_rng(seed=epoch)\n",
    "            rng.shuffle(indices)\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            excerpt = indices[start_idx:start_idx + batch_size]\n",
    "            batch_inputs = []\n",
    "            batch_outputs = []\n",
    "            batch_task_indices = []\n",
    "            # Collect and pad batch data\n",
    "            for idx in excerpt:\n",
    "                input_grid, output_grid, task_index = data[idx]\n",
    "                # Pad grids\n",
    "                input_padded = pad_grid(input_grid, max_size, pad_value, center_offset)\n",
    "                output_padded = pad_grid(output_grid, max_size, pad_value, center_offset)\n",
    "                batch_inputs.append(input_padded)\n",
    "                batch_outputs.append(output_padded)\n",
    "                batch_task_indices.append(task_index)\n",
    "            # Convert to JAX arrays\n",
    "            batch_inputs = jnp.stack(batch_inputs)\n",
    "            batch_outputs = jnp.stack(batch_outputs)\n",
    "            batch_task_indices = jnp.array(batch_task_indices, dtype=jnp.int32)\n",
    "            # Apply random augmentations\n",
    "            if augmentations:\n",
    "                rng_key = jax.random.PRNGKey(epoch)\n",
    "                rng_key = jax.random.fold_in(rng_key, start_idx)\n",
    "                rng_keys = jax.random.split(rng_key, len(batch_inputs))\n",
    "                batch_inputs, batch_outputs = apply_random_augmentations_batch(\n",
    "                    batch_inputs, batch_outputs, rng_keys, augmentations)\n",
    "            yield batch_inputs, batch_outputs, batch_task_indices\n",
    "        epoch += 1  # Increment epoch counter for new seed\n",
    "\n",
    "# Prepare data\n",
    "train_data, task_id_to_index = process_tasks(train_challenges, train_solutions)\n",
    "eval_data, _ = process_tasks(eval_challenges, eval_solutions)\n",
    "\n",
    "# Define augmentations\n",
    "augmentations = ['flip_horizontal', 'flip_vertical', 'rotate_90', 'rotate_180', 'rotate_270']\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32  # Adjust as needed\n",
    "\n",
    "# Create data generators\n",
    "train_generator = data_generator(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    augmentations=augmentations,\n",
    "    center_offset=(0, 0),\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "eval_generator = data_generator(\n",
    "    eval_data,\n",
    "    batch_size=batch_size,\n",
    "    augmentations=None,  # Typically, we don't apply augmentations during evaluation\n",
    "    center_offset=(0, 0),\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Example usage: Get one batch\n",
    "\n",
    "# Warm-up run to exclude compilation time\n",
    "_ = next(train_generator)\n",
    "\n",
    "# Record initial time and memory usage\n",
    "start_time = time.time()\n",
    "process = psutil.Process()\n",
    "initial_memory = process.memory_info().rss  # in bytes\n",
    "total_ram = psutil.virtual_memory().total  # in bytes\n",
    "initial_gpu_memory_used, total_gpu_memory = get_device_memory()\n",
    "\n",
    "train_batch_inputs, train_batch_outputs, train_batch_task_indices = next(train_generator)\n",
    "\n",
    "# Force synchronization to get accurate timing (for JAX on GPU)\n",
    "jax.block_until_ready(train_batch_inputs)\n",
    "\n",
    "# Record final time and memory usage\n",
    "end_time = time.time()\n",
    "final_memory = process.memory_info().rss  # in bytes\n",
    "final_gpu_memory_used, _ = get_device_memory()\n",
    "\n",
    "# Compute time and memory differences\n",
    "time_taken = end_time - start_time\n",
    "memory_used = final_memory - initial_memory  # in bytes\n",
    "\n",
    "print(\"Train batch inputs shape:\", train_batch_inputs.shape)\n",
    "print(\"Train batch outputs shape:\", train_batch_outputs.shape)\n",
    "print(\"Train batch task indices shape:\", train_batch_task_indices.shape)\n",
    "print(f\"Time taken to generate one batch: {time_taken:.4f} seconds\")\n",
    "\n",
    "# Calculate RAM utilization percentage\n",
    "ram_utilization = (final_memory / total_ram) * 100\n",
    "print(f\"RAM utilization after batch generation: {ram_utilization:.2f}%\")\n",
    "\n",
    "# Calculate RAM used as a percentage of total RAM\n",
    "ram_used_percent = (memory_used / total_ram) * 100\n",
    "print(f\"RAM used to generate one batch: {memory_used / (1024 * 1024):.2f} MB ({ram_used_percent:.6f}% of total RAM)\")\n",
    "\n",
    "if initial_gpu_memory_used is not None and total_gpu_memory is not None and final_gpu_memory_used is not None:\n",
    "    gpu_memory_used = final_gpu_memory_used - initial_gpu_memory_used\n",
    "    gpu_utilization = (final_gpu_memory_used / total_gpu_memory) * 100\n",
    "    gpu_used_percent = (gpu_memory_used / total_gpu_memory) * 100\n",
    "    print(f\"GPU memory used to generate one batch: {gpu_memory_used:.2f} MB ({gpu_used_percent:.6f}% of total GPU memory)\")\n",
    "    print(f\"GPU memory utilization after batch generation: {gpu_utilization:.2f}%\")\n",
    "else:\n",
    "    print(\"Device memory usage information is not available.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
