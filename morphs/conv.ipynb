{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import jax\n",
    "import jaxlib\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "\n",
    "for pkg in [jax, jaxlib, flax, optax]:\n",
    "    print(pkg.__name__, pkg.__version__)\n",
    "\n",
    "# Environment variables\n",
    "wandb_entity = os.environ[\"WANDB_ENTITY\"]\n",
    "wandb_project = os.environ[\"WANDB_PROJECT\"]\n",
    "morph = os.environ[\"MORPH\"]\n",
    "morph_output_dir = os.environ[\"MORPH_OUTPUT_DIR\"]\n",
    "\n",
    "# Initialize WandB\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_name = f\"{morph}_{current_datetime}\"\n",
    "wandb.init(\n",
    "    entity=wandb_entity,\n",
    "    project=wandb_project,\n",
    "    name=run_name,\n",
    "    config={\n",
    "        \"morph\": morph,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 32,\n",
    "        \"num_epochs\": 2,\n",
    "        \"augmentations\": ['flip_horizontal', 'flip_vertical', 'rotate_90', 'rotate_180', 'rotate_270']\n",
    "    }\n",
    ")\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "train_challenges = load_data('/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json')\n",
    "train_solutions = load_data('/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json')\n",
    "eval_challenges = load_data('/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json')\n",
    "eval_solutions = load_data('/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json')\n",
    "\n",
    "# Process tasks\n",
    "def process_tasks(challenges, solutions):\n",
    "    data = []\n",
    "    task_id_to_index = {}\n",
    "    for index, task_id in enumerate(challenges.keys()):\n",
    "        task_id_to_index[task_id] = index\n",
    "        task = challenges[task_id]\n",
    "        solution = solutions[task_id]\n",
    "        for pair in task['train']:\n",
    "            input_grid = np.array(pair['input'], dtype=np.int32)\n",
    "            output_grid = np.array(pair['output'], dtype=np.int32)\n",
    "            data.append((input_grid, output_grid, index))\n",
    "        for i, test_input in enumerate(task['test']):\n",
    "            input_grid = np.array(test_input['input'], dtype=np.int32)\n",
    "            output_grid = np.array(solution[i], dtype=np.int32)\n",
    "            data.append((input_grid, output_grid, index))\n",
    "    return data, task_id_to_index\n",
    "\n",
    "# Pad grid function\n",
    "def pad_grid(grid, max_size=30, pad_value=0, center_offset=(0, 0)):\n",
    "    padded = np.full((max_size, max_size), pad_value, dtype=np.int32)\n",
    "    rows, cols = grid.shape\n",
    "    offset_row = (max_size - rows) // 2 + center_offset[0]\n",
    "    offset_col = (max_size - cols) // 2 + center_offset[1]\n",
    "    offset_row = np.clip(offset_row, 0, max_size - rows)\n",
    "    offset_col = np.clip(offset_col, 0, max_size - cols)\n",
    "    padded[offset_row:offset_row+rows, offset_col:offset_col+cols] = grid\n",
    "    return padded\n",
    "\n",
    "# Augmentation functions using JAX\n",
    "@jax.jit\n",
    "def flip_horizontal(grid):\n",
    "    return jnp.fliplr(grid)\n",
    "\n",
    "@jax.jit\n",
    "def flip_vertical(grid):\n",
    "    return jnp.flipud(grid)\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1,))\n",
    "def rotate(grid, k):\n",
    "    return jnp.rot90(grid, k=k)\n",
    "\n",
    "AUGMENTATIONS = {\n",
    "    'flip_horizontal': flip_horizontal,\n",
    "    'flip_vertical': flip_vertical,\n",
    "    'rotate_90': partial(rotate, k=1),\n",
    "    'rotate_180': partial(rotate, k=2),\n",
    "    'rotate_270': partial(rotate, k=3),\n",
    "}\n",
    "\n",
    "# Apply random augmentations to a batch\n",
    "def apply_random_augmentations_batch(input_grids, output_grids, rng_keys, augmentations):\n",
    "    num_augs = len(augmentations)\n",
    "\n",
    "    def augment_sample(input_grid, output_grid, rng_key):\n",
    "        apply_aug = jax.random.bernoulli(rng_key, p=0.5, shape=(num_augs,))\n",
    "        for i, aug_name in enumerate(augmentations):\n",
    "            aug_func = AUGMENTATIONS[aug_name]\n",
    "            input_grid, output_grid = jax.lax.cond(\n",
    "                apply_aug[i],\n",
    "                lambda grids: (aug_func(grids[0]), aug_func(grids[1])),\n",
    "                lambda grids: grids,\n",
    "                (input_grid, output_grid)\n",
    "            )\n",
    "        return input_grid, output_grid\n",
    "\n",
    "    # Vectorize over the batch dimension\n",
    "    v_augment_sample = jax.vmap(augment_sample, in_axes=(0, 0, 0), out_axes=(0, 0))\n",
    "    augmented_inputs, augmented_outputs = v_augment_sample(input_grids, output_grids, rng_keys)\n",
    "    return augmented_inputs, augmented_outputs\n",
    "\n",
    "# Data generator\n",
    "def data_generator(data, batch_size, augmentations=None, max_size=30, pad_value=0, center_offset=(0, 0), shuffle=True):\n",
    "    num_samples = len(data)\n",
    "    indices = np.arange(num_samples)\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            rng = np.random.default_rng(seed=epoch)\n",
    "            rng.shuffle(indices)\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            excerpt = indices[start_idx:start_idx + batch_size]\n",
    "            batch_inputs = []\n",
    "            batch_outputs = []\n",
    "            batch_task_indices = []\n",
    "            for idx in excerpt:\n",
    "                input_grid, output_grid, task_index = data[idx]\n",
    "                input_padded = pad_grid(input_grid, max_size, pad_value, center_offset)\n",
    "                output_padded = pad_grid(output_grid, max_size, pad_value, center_offset)\n",
    "                batch_inputs.append(input_padded)\n",
    "                batch_outputs.append(output_padded)\n",
    "                batch_task_indices.append(task_index)\n",
    "            batch_inputs = jnp.stack(batch_inputs)\n",
    "            batch_outputs = jnp.stack(batch_outputs)\n",
    "            batch_task_indices = jnp.array(batch_task_indices, dtype=jnp.int32)\n",
    "            if augmentations:\n",
    "                rng_key = jax.random.PRNGKey(epoch)\n",
    "                rng_key = jax.random.fold_in(rng_key, start_idx)\n",
    "                rng_keys = jax.random.split(rng_key, len(batch_inputs))\n",
    "                batch_inputs, batch_outputs = apply_random_augmentations_batch(batch_inputs, batch_outputs, rng_keys, augmentations)\n",
    "            yield batch_inputs, batch_outputs, batch_task_indices\n",
    "        epoch += 1\n",
    "\n",
    "train_data, task_id_to_index = process_tasks(train_challenges, train_solutions)\n",
    "eval_data, _ = process_tasks(eval_challenges, eval_solutions)\n",
    "\n",
    "augmentations = ['flip_horizontal', 'flip_vertical', 'rotate_90', 'rotate_180', 'rotate_270']\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = data_generator(train_data, batch_size, augmentations=augmentations)\n",
    "# We'll create a function to get evaluation batches\n",
    "def get_eval_batches(eval_data, batch_size, max_size=30, pad_value=0, center_offset=(0, 0)):\n",
    "    num_samples = len(eval_data)\n",
    "    indices = np.arange(num_samples)\n",
    "    for start_idx in range(0, num_samples, batch_size):\n",
    "        excerpt = indices[start_idx:start_idx + batch_size]\n",
    "        batch_inputs = []\n",
    "        batch_outputs = []\n",
    "        batch_task_indices = []\n",
    "        for idx in excerpt:\n",
    "            input_grid, output_grid, task_index = eval_data[idx]\n",
    "            input_padded = pad_grid(input_grid, max_size, pad_value, center_offset)\n",
    "            output_padded = pad_grid(output_grid, max_size, pad_value, center_offset)\n",
    "            batch_inputs.append(input_padded)\n",
    "            batch_outputs.append(output_padded)\n",
    "            batch_task_indices.append(task_index)\n",
    "        batch_inputs = jnp.stack(batch_inputs)\n",
    "        batch_outputs = jnp.stack(batch_outputs)\n",
    "        batch_task_indices = jnp.array(batch_task_indices, dtype=jnp.int32)\n",
    "        yield batch_inputs, batch_outputs, batch_task_indices\n",
    "\n",
    "# Define the model\n",
    "class GridModel(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # x is of shape (batch_size, 30, 30)\n",
    "        x = x[..., None]  # Add channel dimension, shape (batch_size, 30, 30, 1)\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3), padding='SAME')(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3), padding='SAME')(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3), padding='SAME')(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=10, kernel_size=(1, 1))(x)  # Output logits for each class\n",
    "        # x has shape (batch_size, 30, 30, 10)\n",
    "        return x  # Return logits\n",
    "\n",
    "model = GridModel()\n",
    "\n",
    "# Define the loss function\n",
    "def compute_loss(logits, labels):\n",
    "    # logits: shape (batch_size, 30, 30, 10)\n",
    "    # labels: shape (batch_size, 30, 30)\n",
    "    logits = logits.reshape(-1, 10)\n",
    "    labels = labels.reshape(-1)\n",
    "    one_hot_labels = jax.nn.one_hot(labels, num_classes=10)\n",
    "    loss = optax.softmax_cross_entropy(logits, one_hot_labels).mean()\n",
    "    return loss\n",
    "\n",
    "# Initialize model parameters and optimizer state\n",
    "learning_rate = 1e-3\n",
    "params = model.init(jax.random.PRNGKey(0), jnp.ones((batch_size, 30, 30)))\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Define a training step function\n",
    "@jax.jit\n",
    "def train_step(params, batch_inputs, batch_outputs, opt_state):\n",
    "    def loss_fn(params):\n",
    "        logits = model.apply(params, batch_inputs)\n",
    "        loss = compute_loss(logits, batch_outputs)\n",
    "        return loss\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "# Define an evaluation step function\n",
    "@jax.jit\n",
    "def eval_step(params, batch_inputs, batch_outputs):\n",
    "    logits = model.apply(params, batch_inputs)\n",
    "    loss = compute_loss(logits, batch_outputs)\n",
    "    preds = jnp.argmax(logits, axis=-1)\n",
    "    accuracy = jnp.mean(preds == batch_outputs)\n",
    "    return loss, accuracy\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(params, eval_data, batch_size):\n",
    "    eval_batches = get_eval_batches(eval_data, batch_size)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for batch_inputs, batch_outputs, batch_task_indices in eval_batches:\n",
    "        loss, accuracy = eval_step(params, batch_inputs, batch_outputs)\n",
    "        losses.append(loss)\n",
    "        accuracies.append(accuracy)\n",
    "    avg_loss = np.mean(losses)\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "steps_per_epoch = len(train_data) // batch_size\n",
    "print_every = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_losses = []\n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_inputs, batch_outputs, batch_task_indices = next(train_generator)\n",
    "        params, opt_state, loss = train_step(params, batch_inputs, batch_outputs, opt_state)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        # Log training loss every `print_every` steps\n",
    "        if (step + 1) % print_every == 0:\n",
    "            current_step = epoch * steps_per_epoch + step + 1\n",
    "            print(f\"Step {step + 1}/{steps_per_epoch}, Loss: {loss:.4f}\")\n",
    "            wandb.log({\"train_loss\": loss.item()}, step=current_step)\n",
    "    \n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    print(f\"Epoch {epoch + 1} completed. Average training loss: {avg_train_loss:.4f}\")\n",
    "    wandb.log({\"avg_train_loss\": avg_train_loss}, step=(epoch + 1) * steps_per_epoch)\n",
    "    \n",
    "    # Evaluation\n",
    "    avg_eval_loss, avg_eval_accuracy = evaluate(params, eval_data, batch_size)\n",
    "    print(f\"Validation loss: {avg_eval_loss:.4f}, Validation accuracy: {avg_eval_accuracy:.4f}\")\n",
    "    wandb.log({\n",
    "        \"val_loss\": avg_eval_loss,\n",
    "        \"val_accuracy\": avg_eval_accuracy\n",
    "    }, step=(epoch + 1) * steps_per_epoch)\n",
    "\n",
    "# Load test challenges\n",
    "test_challenges = load_data('/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json')\n",
    "\n",
    "# Process test tasks\n",
    "def process_test_tasks(challenges):\n",
    "    data = []\n",
    "    for task_id in challenges.keys():\n",
    "        task = challenges[task_id]\n",
    "        for i, test_input in enumerate(task['test']):\n",
    "            input_grid = np.array(test_input['input'], dtype=np.int32)\n",
    "            data.append((task_id, i, input_grid))\n",
    "    return data\n",
    "\n",
    "test_data = process_test_tasks(test_challenges)\n",
    "\n",
    "# Generate predictions\n",
    "def generate_predictions(params, test_data, batch_size):\n",
    "    predictions = {}\n",
    "    for i in range(0, len(test_data), batch_size):\n",
    "        batch = test_data[i:i+batch_size]\n",
    "        batch_inputs = []\n",
    "        task_ids = []\n",
    "        input_indices = []\n",
    "        for task_id, input_index, input_grid in batch:\n",
    "            input_padded = pad_grid(input_grid, max_size=30, pad_value=0)\n",
    "            batch_inputs.append(input_padded)\n",
    "            task_ids.append(task_id)\n",
    "            input_indices.append(input_index)\n",
    "        batch_inputs = jnp.stack(batch_inputs)\n",
    "        logits = model.apply(params, batch_inputs)\n",
    "        preds = jnp.argmax(logits, axis=-1)\n",
    "        preds = np.array(preds, dtype=int)\n",
    "        # For each prediction, store it in the predictions dict\n",
    "        for idx in range(len(batch)):\n",
    "            task_id = task_ids[idx]\n",
    "            input_index = input_indices[idx]\n",
    "            pred_grid = preds[idx]\n",
    "            pred_grid_list = pred_grid.tolist()\n",
    "            # For simplicity, use the same prediction for both attempts\n",
    "            attempt = {\"attempt_1\": pred_grid_list, \"attempt_2\": pred_grid_list}\n",
    "            if task_id not in predictions:\n",
    "                predictions[task_id] = []\n",
    "            # Since tasks may have multiple test inputs, we need to maintain order\n",
    "            while len(predictions[task_id]) <= input_index:\n",
    "                predictions[task_id].append({})\n",
    "            predictions[task_id][input_index] = attempt\n",
    "    return predictions\n",
    "\n",
    "predictions = generate_predictions(params, test_data, batch_size)\n",
    "\n",
    "# Save predictions to submission.json\n",
    "with open('submission.json', 'w') as f:\n",
    "    json.dump(predictions, f)\n",
    "\n",
    "# Finish WandB run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
