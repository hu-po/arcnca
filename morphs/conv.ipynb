{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import io\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import wandb\n",
    "from PIL import Image\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "\n",
    "from arckit import draw_task\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "num_epochs = 8\n",
    "print_every = 100\n",
    "augmentations = ['flip_horizontal', 'flip_vertical', 'rotate_90', 'rotate_180', 'rotate_270']\n",
    "max_size = 30\n",
    "pad_value = 0\n",
    "num_visualization_samples = 3  # Number of samples to visualize during training and submission\n",
    "\n",
    "compute_backend = os.environ.get(\"COMPUTE_BACKEND\", \"unk\")\n",
    "morph = os.environ[\"MORPH\"]\n",
    "wandb_entity = os.environ[\"WANDB_ENTITY\"]\n",
    "wandb_project = os.environ[\"WANDB_PROJECT\"]\n",
    "\n",
    "# Initialize WandB\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "run_name = f\"{compute_backend}.{morph}.{current_datetime}\"\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    entity=wandb_entity,\n",
    "    project=wandb_project,\n",
    "    name=run_name,\n",
    "    config={\n",
    "        \"morph\": morph,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"augmentations\": augmentations,\n",
    "        \"max_size\": max_size,\n",
    "        \"pad_value\": pad_value,\n",
    "    }\n",
    ")\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Load datasets\n",
    "train_challenges = load_data('/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json')\n",
    "train_solutions = load_data('/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json')\n",
    "eval_challenges = load_data('/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json')\n",
    "eval_solutions = load_data('/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json')\n",
    "\n",
    "def process_tasks(challenges, solutions):\n",
    "    data = []\n",
    "    task_id_to_index = {}\n",
    "    for index, task_id in enumerate(challenges.keys()):\n",
    "        task_id_to_index[task_id] = index\n",
    "        task = challenges[task_id]\n",
    "        solution = solutions[task_id]\n",
    "        for pair in task['train']:\n",
    "            input_grid = np.array(pair['input'], dtype=np.int32)\n",
    "            output_grid = np.array(pair['output'], dtype=np.int32)\n",
    "            data.append((input_grid, output_grid, index))\n",
    "        for i, test_input in enumerate(task['test']):\n",
    "            input_grid = np.array(test_input['input'], dtype=np.int32)\n",
    "            output_grid = np.array(solution[i], dtype=np.int32)\n",
    "            data.append((input_grid, output_grid, index))\n",
    "    return data, task_id_to_index\n",
    "\n",
    "train_data, task_id_to_index = process_tasks(train_challenges, train_solutions)\n",
    "eval_data, _ = process_tasks(eval_challenges, eval_solutions)\n",
    "\n",
    "def pad_grid(grid, max_size=max_size, pad_value=pad_value):\n",
    "    padded = np.full((max_size, max_size), pad_value, dtype=np.int32)\n",
    "    rows, cols = grid.shape\n",
    "    start_row = (max_size - rows) // 2\n",
    "    start_col = (max_size - cols) // 2\n",
    "    padded[start_row:start_row+rows, start_col:start_col+cols] = grid\n",
    "    return padded\n",
    "\n",
    "def unpad_grid(grid, original_shape, max_size=max_size):\n",
    "    rows, cols = original_shape\n",
    "    start_row = (max_size - rows) // 2\n",
    "    start_col = (max_size - cols) // 2\n",
    "    return grid[start_row:start_row + rows, start_col:start_col + cols]\n",
    "\n",
    "@jax.jit\n",
    "def flip_horizontal(grid):\n",
    "    return jnp.fliplr(grid)\n",
    "\n",
    "@jax.jit\n",
    "def flip_vertical(grid):\n",
    "    return jnp.flipud(grid)\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1,))\n",
    "def rotate(grid, k):\n",
    "    return jnp.rot90(grid, k=k)\n",
    "\n",
    "AUGMENTATIONS = {\n",
    "    'flip_horizontal': flip_horizontal,\n",
    "    'flip_vertical': flip_vertical,\n",
    "    'rotate_90': partial(rotate, k=1),\n",
    "    'rotate_180': partial(rotate, k=2),\n",
    "    'rotate_270': partial(rotate, k=3),\n",
    "}\n",
    "\n",
    "def apply_random_augmentations_batch(input_grids, output_grids, rng_keys, augmentations):\n",
    "    num_augs = len(augmentations)\n",
    "\n",
    "    def augment_sample(input_grid, output_grid, rng_key):\n",
    "        apply_aug = jax.random.bernoulli(rng_key, p=0.5, shape=(num_augs,))\n",
    "        for i, aug_name in enumerate(augmentations):\n",
    "            aug_func = AUGMENTATIONS[aug_name]\n",
    "            input_grid, output_grid = jax.lax.cond(\n",
    "                apply_aug[i],\n",
    "                lambda grids: (aug_func(grids[0]), aug_func(grids[1])),\n",
    "                lambda grids: grids,\n",
    "                (input_grid, output_grid)\n",
    "            )\n",
    "        return input_grid, output_grid\n",
    "\n",
    "    v_augment_sample = jax.vmap(augment_sample, in_axes=(0, 0, 0), out_axes=(0, 0))\n",
    "    augmented_inputs, augmented_outputs = v_augment_sample(input_grids, output_grids, rng_keys)\n",
    "    return augmented_inputs, augmented_outputs\n",
    "\n",
    "def data_generator(data, batch_size, augmentations=None, max_size=max_size, pad_value=pad_value, shuffle=True):\n",
    "    num_samples = len(data)\n",
    "    indices = np.arange(num_samples)\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            rng = np.random.default_rng(seed=epoch)\n",
    "            rng.shuffle(indices)\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            excerpt = indices[start_idx:start_idx + batch_size]\n",
    "            batch_inputs = []\n",
    "            batch_outputs = []\n",
    "            batch_task_indices = []\n",
    "            for idx in excerpt:\n",
    "                input_grid, output_grid, task_index = data[idx]\n",
    "                input_padded = pad_grid(input_grid, max_size, pad_value)\n",
    "                output_padded = pad_grid(output_grid, max_size, pad_value)\n",
    "                batch_inputs.append(input_padded)\n",
    "                batch_outputs.append(output_padded)\n",
    "                batch_task_indices.append(task_index)\n",
    "            batch_inputs = jnp.stack(batch_inputs)\n",
    "            batch_outputs = jnp.stack(batch_outputs)\n",
    "            batch_task_indices = jnp.array(batch_task_indices, dtype=jnp.int32)\n",
    "            if augmentations:\n",
    "                rng_key = jax.random.PRNGKey(epoch)\n",
    "                rng_key = jax.random.fold_in(rng_key, start_idx)\n",
    "                rng_keys = jax.random.split(rng_key, len(batch_inputs))\n",
    "                batch_inputs, batch_outputs = apply_random_augmentations_batch(batch_inputs, batch_outputs, rng_keys, augmentations)\n",
    "            yield batch_inputs, batch_outputs, batch_task_indices\n",
    "        epoch += 1\n",
    "\n",
    "def get_eval_batches(eval_data, batch_size, max_size=max_size, pad_value=pad_value):\n",
    "    num_samples = len(eval_data)\n",
    "    indices = np.arange(num_samples)\n",
    "    for start_idx in range(0, num_samples, batch_size):\n",
    "        excerpt = indices[start_idx:start_idx + batch_size]\n",
    "        batch_inputs = []\n",
    "        batch_outputs = []\n",
    "        batch_task_indices = []\n",
    "        for idx in excerpt:\n",
    "            input_grid, output_grid, task_index = eval_data[idx]\n",
    "            input_padded = pad_grid(input_grid, max_size, pad_value)\n",
    "            output_padded = pad_grid(output_grid, max_size, pad_value)\n",
    "            batch_inputs.append(input_padded)\n",
    "            batch_outputs.append(output_padded)\n",
    "            batch_task_indices.append(task_index)\n",
    "        batch_inputs = jnp.stack(batch_inputs)\n",
    "        batch_outputs = jnp.stack(batch_outputs)\n",
    "        batch_task_indices = jnp.array(batch_task_indices, dtype=jnp.int32)\n",
    "        yield batch_inputs, batch_outputs, batch_task_indices\n",
    "\n",
    "class Model(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x[..., None]  # Add channel dimension, shape (batch_size, 30, 30, 1)\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3), padding='SAME')(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=64, kernel_size=(3, 3), padding='SAME')(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=32, kernel_size=(3, 3), padding='SAME')(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=10, kernel_size=(1, 1))(x)  # Output logits for each class\n",
    "        return x  # Shape: (batch_size, 30, 30, 10)\n",
    "\n",
    "model = Model()\n",
    "\n",
    "def compute_loss(logits, labels):\n",
    "    logits = logits.reshape(-1, 10)\n",
    "    labels = labels.reshape(-1)\n",
    "    one_hot_labels = jax.nn.one_hot(labels, num_classes=10)\n",
    "    loss = optax.softmax_cross_entropy(logits, one_hot_labels).mean()\n",
    "    return loss\n",
    "\n",
    "# Initialize model parameters and optimizer state\n",
    "params = model.init(jax.random.PRNGKey(0), jnp.ones((batch_size, max_size, max_size)))\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, batch_inputs, batch_outputs, opt_state):\n",
    "    def loss_fn(params):\n",
    "        logits = model.apply(params, batch_inputs)\n",
    "        loss = compute_loss(logits, batch_outputs)\n",
    "        return loss\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(params, batch_inputs, batch_outputs):\n",
    "    logits = model.apply(params, batch_inputs)\n",
    "    loss = compute_loss(logits, batch_outputs)\n",
    "    preds = jnp.argmax(logits, axis=-1)\n",
    "    # accuracy = jnp.mean(preds == batch_outputs)\n",
    "    # accuracy without counting zeroes\n",
    "    accuracy = jnp.mean(jnp.where(batch_outputs == 0, 0, preds == batch_outputs))\n",
    "    return loss, accuracy\n",
    "\n",
    "def evaluate(params, eval_data, batch_size):\n",
    "    eval_batches = get_eval_batches(eval_data, batch_size)\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for batch_inputs, batch_outputs, batch_task_indices in eval_batches:\n",
    "        loss, accuracy = eval_step(params, batch_inputs, batch_outputs)\n",
    "        losses.append(loss)\n",
    "        accuracies.append(accuracy)\n",
    "    avg_loss = np.mean(losses)\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "def wrap_draw_task(task_dict, include_test=False):\n",
    "    class TaskVisualization:\n",
    "        def __init__(self, task_id, train, test):\n",
    "            # Ensure that input and output grids are NumPy arrays\n",
    "            self.id = task_id\n",
    "            self.train = [(np.array(input_grid), np.array(output_grid)) for input_grid, output_grid in train]\n",
    "            self.test = [(np.array(input_grid), np.array(output_grid)) for input_grid, output_grid in test] if test else []\n",
    "\n",
    "    task_obj = TaskVisualization(\n",
    "        task_id=task_dict['id'],\n",
    "        train=task_dict['train'],\n",
    "        test=task_dict.get('test', [])\n",
    "    )\n",
    "    return draw_task(task_obj, include_test=include_test)\n",
    "\n",
    "# Create data generators\n",
    "train_generator = data_generator(train_data, batch_size, augmentations=augmentations)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_losses = []\n",
    "    steps_per_epoch = len(train_data) // batch_size\n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_inputs, batch_outputs, batch_task_indices = next(train_generator)\n",
    "        params, opt_state, loss = train_step(params, batch_inputs, batch_outputs, opt_state)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        if (step + 1) % print_every == 0:\n",
    "            current_step = epoch * steps_per_epoch + step + 1\n",
    "            print(f\"Step {step + 1}/{steps_per_epoch}, Loss: {loss:.4f}\")\n",
    "            wandb.log({\"train_loss\": loss.item()}, step=current_step)\n",
    "    \n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    print(f\"Epoch {epoch + 1} completed. Average training loss: {avg_train_loss:.4f}\")\n",
    "    wandb.log({\"avg_train_loss\": avg_train_loss}, step=(epoch + 1) * steps_per_epoch)\n",
    "    \n",
    "    # Evaluation\n",
    "    avg_eval_loss, avg_eval_accuracy = evaluate(params, eval_data, batch_size)\n",
    "    print(f\"Validation loss: {avg_eval_loss:.4f}, Validation accuracy: {avg_eval_accuracy:.4f}\")\n",
    "    wandb.log({\n",
    "        \"val_loss\": avg_eval_loss,\n",
    "        \"val_accuracy\": avg_eval_accuracy\n",
    "    }, step=(epoch + 1) * steps_per_epoch)\n",
    "    \n",
    "    # Visualization: Log a sample of evaluation predictions\n",
    "    sample_generator = data_generator(eval_data, batch_size, shuffle=False)\n",
    "    sample_batch_inputs, sample_batch_outputs, _ = next(sample_generator)\n",
    "    logits = model.apply(params, sample_batch_inputs)\n",
    "    preds = jnp.argmax(logits, axis=-1)\n",
    "    \n",
    "    for i in range(min(num_visualization_samples, batch_size)):\n",
    "        input_grid = np.array(sample_batch_inputs[i])\n",
    "        true_output = np.array(sample_batch_outputs[i])\n",
    "        predicted_output = np.array(preds[i])\n",
    "        \n",
    "        task_visual = {\n",
    "            'id': f'Epoch{epoch+1}_Sample{i+1}',\n",
    "            'train': [(input_grid.tolist(), true_output.tolist())],\n",
    "            'test': []\n",
    "        }\n",
    "        \n",
    "        # Use the wrap_draw_task function instead of calling draw_task directly\n",
    "        drawing = wrap_draw_task(task_visual, include_test=False)\n",
    "        img_buffer = io.BytesIO()\n",
    "        drawing.save_png(img_buffer)\n",
    "        img_buffer.seek(0)\n",
    "        img_buffer.seek(0)  # Ensure we're at the start of the buffer\n",
    "        image = Image.open(img_buffer)  # Use PIL to open the image from the BytesIO buffer\n",
    "        wandb.log({f\"Evaluation_Sample_{i+1}\": wandb.Image(image)}, step=(epoch + 1) * steps_per_epoch)\n",
    "\n",
    "\n",
    "def process_test_tasks(challenges):\n",
    "    data = []\n",
    "    for task_id in challenges.keys():\n",
    "        task = challenges[task_id]\n",
    "        for i, test_input in enumerate(task['test']):\n",
    "            input_grid = np.array(test_input['input'], dtype=np.int32)\n",
    "            data.append((task_id, i, input_grid))\n",
    "    return data\n",
    "\n",
    "# Load and process test challenges\n",
    "test_challenges = load_data('/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json')\n",
    "test_data = process_test_tasks(test_challenges)\n",
    "\n",
    "def generate_predictions(params, test_data, batch_size):\n",
    "    predictions = {}\n",
    "    visualization_samples = []  # To store samples for visualization\n",
    "    for i in range(0, len(test_data), batch_size):\n",
    "        batch = test_data[i:i+batch_size]\n",
    "        batch_inputs = []\n",
    "        task_ids = []\n",
    "        input_indices = []\n",
    "        original_shapes = []\n",
    "        for task_id, input_index, input_grid in batch:\n",
    "            original_shape = input_grid.shape  # Store original shape before padding\n",
    "            input_padded = pad_grid(input_grid, max_size, pad_value)\n",
    "            batch_inputs.append(input_padded)\n",
    "            task_ids.append(task_id)\n",
    "            input_indices.append(input_index)\n",
    "            original_shapes.append(original_shape)\n",
    "        batch_inputs = jnp.stack(batch_inputs)\n",
    "        logits = model.apply(params, batch_inputs)\n",
    "        preds = jnp.argmax(logits, axis=-1)\n",
    "        preds = np.array(preds, dtype=int)\n",
    "        \n",
    "        for idx in range(len(batch)):\n",
    "            task_id = task_ids[idx]\n",
    "            input_index = input_indices[idx]\n",
    "            pred_grid = preds[idx]\n",
    "            unpadded_pred = unpad_grid(pred_grid, original_shapes[idx])  # Unpad the prediction\n",
    "            pred_grid_list = unpadded_pred.tolist()\n",
    "            attempt = {\"attempt_1\": pred_grid_list, \"attempt_2\": pred_grid_list}\n",
    "            if task_id not in predictions:\n",
    "                predictions[task_id] = []\n",
    "            while len(predictions[task_id]) <= input_index:\n",
    "                predictions[task_id].append({})\n",
    "            predictions[task_id][input_index] = attempt\n",
    "            \n",
    "            if len(visualization_samples) < num_visualization_samples:\n",
    "                original_input = np.array(batch[idx][2])\n",
    "                visualization_samples.append((original_input, unpadded_pred))\n",
    "                \n",
    "    return predictions, visualization_samples\n",
    "\n",
    "predictions, visualization_samples = generate_predictions(params, test_data, batch_size)\n",
    "\n",
    "# Log submission visualizations\n",
    "for i, (input_grid, pred_grid) in enumerate(visualization_samples):\n",
    "    task_visual = {\n",
    "        'id': f'Submission_Sample_{i+1}',\n",
    "        'train': [(input_grid.tolist(), pred_grid.tolist())],\n",
    "        'test': []\n",
    "    }\n",
    "    # Use the wrap_draw_task function instead of calling draw_task directly\n",
    "    drawing = wrap_draw_task(task_visual, include_test=False)\n",
    "    img_buffer = io.BytesIO()\n",
    "    drawing.save_png(img_buffer)\n",
    "    img_buffer.seek(0)  # Ensure we're at the start of the buffer\n",
    "    image = Image.open(img_buffer)  # Use PIL to open the image from the BytesIO buffer\n",
    "    wandb.log({f\"Submission_Sample_{i+1}\": wandb.Image(image)})\n",
    "\n",
    "# Save predictions to submission.json\n",
    "# TODO: Uncomment when cleaning for submission during export\n",
    "# with open('submission.json', 'w') as f:\n",
    "#     json.dump(predictions, f)\n",
    "\n",
    "# Finish WandB run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
