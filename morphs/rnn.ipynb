{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import io\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import wandb\n",
    "from PIL import Image\n",
    "import drawsvg\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "\n",
    "import arckit.vis as vis\n",
    "from arckit import draw_task\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 8  # Reduced batch size due to complexity of handling multiple pairs\n",
    "num_epochs = 8\n",
    "print_every = 100\n",
    "augmentations = ['flip_horizontal', 'flip_vertical', 'rotate_90', 'rotate_180', 'rotate_270']\n",
    "max_size = 30\n",
    "pad_value = 0\n",
    "num_visualization_samples = 3\n",
    "aug_prob = 0.1\n",
    "conv_features = 32\n",
    "rnn_hidden_size = 64\n",
    "\n",
    "# Initialize WandB\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "run_name = f\"contextual_rnn_convnet.{current_datetime}\"\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"arc-puzzle-solver\",\n",
    "    name=run_name,\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"augmentations\": augmentations,\n",
    "        \"max_size\": max_size,\n",
    "        \"pad_value\": pad_value,\n",
    "        \"conv_features\": conv_features,\n",
    "        \"rnn_hidden_size\": rnn_hidden_size,\n",
    "    }\n",
    ")\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Load datasets\n",
    "train_challenges = load_data('/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json')\n",
    "train_solutions = load_data('/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json')\n",
    "eval_challenges = load_data('/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json')\n",
    "eval_solutions = load_data('/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json')\n",
    "\n",
    "def process_tasks(challenges, solutions):\n",
    "    data = []\n",
    "    for task_id in challenges.keys():\n",
    "        task = challenges[task_id]\n",
    "        solution = solutions[task_id]\n",
    "        train_inputs = [np.array(pair[0], dtype=np.uint8) for pair in task['train']]\n",
    "        train_outputs = [np.array(pair[1], dtype=np.uint8) for pair in task['train']]\n",
    "        test_inputs = [np.array(test['input'], dtype=np.uint8) for test in task['test']]\n",
    "        test_outputs = [np.array(output, dtype=np.uint8) for output in solution]\n",
    "        data.append((train_inputs, train_outputs, test_inputs, test_outputs))\n",
    "    return data\n",
    "\n",
    "train_data = process_tasks(train_challenges, train_solutions)\n",
    "eval_data = process_tasks(eval_challenges, eval_solutions)\n",
    "\n",
    "def pad_grid(grid, max_size=max_size, pad_value=pad_value):\n",
    "    padded = np.full((max_size, max_size), pad_value, dtype=np.int32)\n",
    "    rows, cols = grid.shape\n",
    "    start_row = (max_size - rows) // 2\n",
    "    start_col = (max_size - cols) // 2\n",
    "    padded[start_row:start_row+rows, start_col:start_col+cols] = grid\n",
    "    return padded\n",
    "\n",
    "def unpad_grid(grid, original_shape, max_size=max_size):\n",
    "    rows, cols = original_shape\n",
    "    start_row = (max_size - rows) // 2\n",
    "    start_col = (max_size - cols) // 2\n",
    "    return grid[start_row:start_row + rows, start_col:start_col + cols]\n",
    "\n",
    "@jax.jit\n",
    "def flip_horizontal(grid):\n",
    "    return jnp.fliplr(grid)\n",
    "\n",
    "@jax.jit\n",
    "def flip_vertical(grid):\n",
    "    return jnp.flipud(grid)\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1,))\n",
    "def rotate(grid, k):\n",
    "    return jnp.rot90(grid, k=k)\n",
    "\n",
    "AUGMENTATIONS = {\n",
    "    'flip_horizontal': flip_horizontal,\n",
    "    'flip_vertical': flip_vertical,\n",
    "    'rotate_90': partial(rotate, k=1),\n",
    "    'rotate_180': partial(rotate, k=2),\n",
    "    'rotate_270': partial(rotate, k=3),\n",
    "}\n",
    "\n",
    "def augment_sample(train_inputs, train_outputs, test_inputs, test_outputs, rng_key):\n",
    "    apply_aug = jax.random.bernoulli(rng_key, p=aug_prob, shape=(len(augmentations),))\n",
    "    for i, aug_name in enumerate(augmentations):\n",
    "        aug_func = AUGMENTATIONS[aug_name]\n",
    "        train_inputs, train_outputs, test_inputs, test_outputs = jax.lax.cond(\n",
    "            apply_aug[i],\n",
    "            lambda x: (aug_func(x[0]), aug_func(x[1]), aug_func(x[2]), aug_func(x[3])),\n",
    "            (train_inputs, train_outputs, test_inputs, test_outputs)\n",
    "        )\n",
    "    return train_inputs, train_outputs, test_inputs, test_outputs\n",
    "\n",
    "def train_generator(data, batch_size, augmentations=None, max_size=max_size, pad_value=pad_value, shuffle=True):\n",
    "    num_samples = len(data)\n",
    "    indices = np.arange(num_samples)\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            rng = np.random.default_rng(seed=epoch)\n",
    "            rng.shuffle(indices)\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            excerpt = indices[start_idx:start_idx + batch_size]\n",
    "            batch_train_inputs, batch_train_outputs, batch_test_inputs, batch_test_outputs = [], [], [], []\n",
    "            for idx in excerpt:\n",
    "                train_inputs, train_outputs, test_inputs, test_outputs = data[idx]\n",
    "                train_inputs_pad = [pad_grid(inp, max_size=max_size, pad_value=pad_value) for inp in train_inputs]\n",
    "                train_outputs_pad = [pad_grid(out, max_size=max_size, pad_value=pad_value) for out in train_outputs]                \n",
    "                test_inputs_pad = [pad_grid(inp, max_size=max_size, pad_value=pad_value) for inp in test_inputs]\n",
    "                padded_test_outputs = [pad_grid(out, max_size=max_size, pad_value=pad_value) for out in test_outputs]\n",
    "                if augmentations:\n",
    "                    rng_key = jax.random.PRNGKey(epoch)\n",
    "                    rng_key = jax.random.fold_in(rng_key, start_idx)\n",
    "                    rng_keys = jax.random.split(rng_key, len(augmentations))\n",
    "                    v_aug = jax.vmap(augment_sample, in_axes=(0, 0, 0, 0, 0))\n",
    "                    train_inputs_pad, train_outputs_pad, test_inputs_pad, padded_test_outputs = v_aug(train_inputs_pad, train_outputs_pad, test_inputs_pad, padded_test_outputs, rng_keys)\n",
    "                batch_train_inputs.append(train_inputs_pad)\n",
    "                batch_train_outputs.append(train_outputs_pad)\n",
    "                batch_test_inputs.append(test_inputs_pad)\n",
    "                batch_test_outputs.append(padded_test_outputs)\n",
    "            yield batch_train_inputs, batch_train_outputs, batch_test_inputs, batch_test_outputs\n",
    "        epoch += 1\n",
    "\n",
    "def eval_generator(data, batch_size, max_size=max_size, pad_value=pad_value):\n",
    "    num_samples = len(data)\n",
    "    indices = np.arange(num_samples)\n",
    "    while True:\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            excerpt = indices[start_idx:start_idx + batch_size]\n",
    "            batch_train_inputs, batch_train_outputs, batch_test_inputs, batch_test_outputs = [], [], [], []\n",
    "            for idx in excerpt:\n",
    "                train_inputs, train_outputs, test_inputs, test_outputs = data[idx]\n",
    "                train_inputs_pad = [pad_grid(inp, max_size=max_size, pad_value=pad_value) for inp in train_inputs]\n",
    "                train_outputs_pad = [pad_grid(out, max_size=max_size, pad_value=pad_value) for out in train_outputs]\n",
    "                test_inputs_pad = [pad_grid(inp, max_size=max_size, pad_value=pad_value) for inp in test_inputs]\n",
    "                padded_test_outputs = [pad_grid(out, max_size=max_size, pad_value=pad_value) for out in test_outputs]\n",
    "                batch_train_inputs.append(train_inputs_pad)\n",
    "                batch_train_outputs.append(train_outputs_pad)\n",
    "                batch_test_inputs.append(test_inputs_pad)\n",
    "                batch_test_outputs.append(padded_test_outputs)\n",
    "            yield batch_train_inputs, batch_train_outputs, batch_test_inputs, batch_test_outputs\n",
    "\n",
    "# Define the ConvNet encoder\n",
    "class ConvEncoder(nn.Module):\n",
    "    # TODO\n",
    "\n",
    "# Define the RNN sequence model\n",
    "class RNNModel(nn.Module):\n",
    "    # TODO\n",
    "    # process alternating training inputs and outputs, then test inputs, and predict test outputs\n",
    "\n",
    "# Initialize models\n",
    "conv_encoder = ConvEncoder()\n",
    "rnn_model = RNNModel()\n",
    "\n",
    "# Initialize optimizer\n",
    "params = conv_encoder.init(#TODO\n",
    "params = {**params, **rnn_model.init( #TODO\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "def train_step():\n",
    "# TODO\n",
    "\n",
    "def eval_step():\n",
    "# TODO\n",
    "\n",
    "# Training loop\n",
    "train_generator = train_generator(train_data, batch_size)\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_losses = []\n",
    "    steps_per_epoch = len(train_data) // batch_size\n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_train_pairs, batch_train_outputs, batch_test_inputs, batch_test_outputs = next(train_generator)\n",
    "        params, opt_state, loss = train_step(params, opt_state, batch_train_pairs, batch_train_outputs, batch_test_inputs, batch_test_outputs)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        if (step + 1) % print_every == 0:\n",
    "            current_step = epoch * steps_per_epoch + step + 1\n",
    "            print(f\"Step {step + 1}/{steps_per_epoch}, Loss: {loss:.4f}\")\n",
    "            wandb.log({\"train_loss\": loss.item()}, step=current_step)\n",
    "\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    print(f\"Epoch {epoch + 1} completed. Average training loss: {avg_train_loss:.4f}\")\n",
    "    wandb.log({\"avg_train_loss\": avg_train_loss}, step=(epoch + 1) * steps_per_epoch)\n",
    "\n",
    "    # Evaluation\n",
    "    for eval_data in eval_generator(eval_data, batch_size):\n",
    "        eval_losses = []\n",
    "        eval_accuracies = []\n",
    "        for batch_train_pairs, batch_train_outputs, batch_test_inputs, batch_test_outputs in eval_data:\n",
    "            eval_loss, eval_accuracy = eval_step(params, batch_train_pairs, batch_train_outputs, batch_test_inputs, batch_test_outputs)\n",
    "            eval_losses.append(eval_loss)\n",
    "            eval_accuracies.append(eval_accuracy)\n",
    "        avg_eval_loss = np.mean(eval_losses)\n",
    "        avg_eval_accuracy = np.mean(eval_accuracies)\n",
    "    print(f\"Validation loss: {avg_eval_loss:.4f}, Validation accuracy: {avg_eval_accuracy:.4f}\")\n",
    "    wandb.log({\n",
    "        \"val_loss\": avg_eval_loss,\n",
    "        \"val_accuracy\": avg_eval_accuracy\n",
    "    }, step=(epoch + 1) * steps_per_epoch)\n",
    "\n",
    "# Finish WandB run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
