{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import io\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import wandb\n",
    "from PIL import Image\n",
    "import drawsvg\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "\n",
    "import arckit.vis as vis\n",
    "from arckit import draw_task\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 8  # Reduced batch size due to complexity of handling multiple pairs\n",
    "num_epochs = 8\n",
    "print_every = 100\n",
    "augmentations = ['flip_horizontal', 'flip_vertical', 'rotate_90', 'rotate_180', 'rotate_270']\n",
    "max_size = 30\n",
    "pad_value = 0\n",
    "num_visualization_samples = 3\n",
    "\n",
    "# Hyperparameters for the models\n",
    "conv_features = 32\n",
    "rnn_hidden_size = 64\n",
    "\n",
    "# Initialize WandB\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "run_name = f\"contextual_rnn_convnet.{current_datetime}\"\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"arc-puzzle-solver\",\n",
    "    name=run_name,\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"augmentations\": augmentations,\n",
    "        \"max_size\": max_size,\n",
    "        \"pad_value\": pad_value,\n",
    "        \"conv_features\": conv_features,\n",
    "        \"rnn_hidden_size\": rnn_hidden_size,\n",
    "    }\n",
    ")\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Load datasets\n",
    "train_challenges = load_data('/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json')\n",
    "train_solutions = load_data('/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json')\n",
    "eval_challenges = load_data('/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json')\n",
    "eval_solutions = load_data('/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json')\n",
    "\n",
    "def process_tasks(challenges, solutions):\n",
    "    data = []\n",
    "    for task_id in challenges.keys():\n",
    "        task = challenges[task_id]\n",
    "        solution = solutions[task_id]\n",
    "        train_pairs = []\n",
    "        for pair in task['train']:\n",
    "            input_grid = np.array(pair['input'], dtype=np.int32)\n",
    "            output_grid = np.array(pair['output'], dtype=np.int32)\n",
    "            train_pairs.append((input_grid, output_grid))\n",
    "        test_inputs = [np.array(test['input'], dtype=np.int32) for test in task['test']]\n",
    "        test_outputs = [np.array(output, dtype=np.int32) for output in solution]\n",
    "        data.append((train_pairs, test_inputs, test_outputs))\n",
    "    return data\n",
    "\n",
    "train_data = process_tasks(train_challenges, train_solutions)\n",
    "eval_data = process_tasks(eval_challenges, eval_solutions)\n",
    "\n",
    "def pad_grid(grid, max_size=max_size, pad_value=pad_value):\n",
    "    padded = np.full((max_size, max_size), pad_value, dtype=np.int32)\n",
    "    rows, cols = grid.shape\n",
    "    start_row = (max_size - rows) // 2\n",
    "    start_col = (max_size - cols) // 2\n",
    "    padded[start_row:start_row+rows, start_col:start_col+cols] = grid\n",
    "    return padded\n",
    "\n",
    "def data_generator(data, batch_size, max_size=max_size, pad_value=pad_value, shuffle=True):\n",
    "    num_samples = len(data)\n",
    "    indices = np.arange(num_samples)\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            rng = np.random.default_rng(seed=epoch)\n",
    "            rng.shuffle(indices)\n",
    "        for start_idx in range(0, num_samples, batch_size):\n",
    "            excerpt = indices[start_idx:start_idx + batch_size]\n",
    "            batch_train_pairs = []\n",
    "            batch_test_inputs = []\n",
    "            batch_test_outputs = []\n",
    "            for idx in excerpt:\n",
    "                train_pairs, test_inputs, test_outputs = data[idx]\n",
    "                padded_train_pairs = [(pad_grid(inp), pad_grid(out)) for inp, out in train_pairs]\n",
    "                padded_test_inputs = [pad_grid(inp) for inp in test_inputs]\n",
    "                padded_test_outputs = [pad_grid(out) for out in test_outputs]\n",
    "                batch_train_pairs.append(padded_train_pairs)\n",
    "                batch_test_inputs.append(padded_test_inputs)\n",
    "                batch_test_outputs.append(padded_test_outputs)\n",
    "            yield batch_train_pairs, batch_test_inputs, batch_test_outputs\n",
    "        epoch += 1\n",
    "\n",
    "# Define the ConvNet encoder\n",
    "class ConvEncoder(nn.Module):\n",
    "    features: int = conv_features\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x[..., None]  # Add channel dimension, shape (batch_size, 30, 30, 1)\n",
    "        x = nn.Conv(features=self.features, kernel_size=(3, 3), padding='SAME')(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(features=self.features, kernel_size=(3, 3), padding='SAME')(x)\n",
    "        x = nn.relu(x)\n",
    "        return x  # Shape: (batch_size, 30, 30, features)\n",
    "\n",
    "# Define the RNN sequence model\n",
    "class RNNModel(nn.Module):\n",
    "    hidden_size: int = rnn_hidden_size\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, train_pairs, test_inputs):\n",
    "        rnn = nn.recurrent.GRUCell()\n",
    "        hidden_state = jnp.zeros((train_pairs[0][0].shape[0], self.hidden_size))\n",
    "\n",
    "        # Process each training pair\n",
    "        for input_grid, output_grid in train_pairs:\n",
    "            combined = jnp.concatenate([input_grid, output_grid], axis=-1)  # Concatenate along feature axis\n",
    "            hidden_state, _ = rnn(hidden_state, combined)\n",
    "\n",
    "        # Predict for each test input\n",
    "        predictions = []\n",
    "        for test_input in test_inputs:\n",
    "            hidden_state, _ = rnn(hidden_state, test_input)\n",
    "            pred = nn.Dense(features=10)(hidden_state)  # Output logits for each class\n",
    "            predictions.append(pred)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# Initialize models\n",
    "conv_encoder = ConvEncoder()\n",
    "rnn_model = RNNModel()\n",
    "\n",
    "# Initialize optimizer\n",
    "params = conv_encoder.init(jax.random.PRNGKey(0), jnp.ones((batch_size, max_size, max_size)))\n",
    "params = {**params, **rnn_model.init(jax.random.PRNGKey(0), [(jnp.ones((batch_size, max_size, max_size, conv_features)), jnp.ones((batch_size, max_size, max_size, conv_features)))], [jnp.ones((batch_size, max_size, max_size, conv_features))])}\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, batch_train_pairs, batch_test_inputs, batch_test_outputs, opt_state):\n",
    "    def loss_fn(params):\n",
    "        train_pairs_encoded = [(conv_encoder.apply(params, inp), conv_encoder.apply(params, out)) for inp, out in batch_train_pairs]\n",
    "        test_inputs_encoded = [conv_encoder.apply(params, inp) for inp in batch_test_inputs]\n",
    "        preds = rnn_model.apply(params, train_pairs_encoded, test_inputs_encoded)\n",
    "        loss = 0\n",
    "        for pred, true_output in zip(preds, batch_test_outputs):\n",
    "            logits = pred.reshape(-1, 10)\n",
    "            labels = true_output.reshape(-1)\n",
    "            one_hot_labels = jax.nn.one_hot(labels, num_classes=10)\n",
    "            loss += optax.softmax_cross_entropy(logits, one_hot_labels).mean()\n",
    "        return loss\n",
    "\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "# Training loop\n",
    "train_generator = data_generator(train_data, batch_size)\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train_losses = []\n",
    "    steps_per_epoch = len(train_data) // batch_size\n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_train_pairs, batch_test_inputs, batch_test_outputs = next(train_generator)\n",
    "        params, opt_state, loss = train_step(params, batch_train_pairs, batch_test_inputs, batch_test_outputs, opt_state)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        if (step + 1) % print_every == 0:\n",
    "            current_step = epoch * steps_per_epoch + step + 1\n",
    "            print(f\"Step {step + 1}/{steps_per_epoch}, Loss: {loss:.4f}\")\n",
    "            wandb.log({\"train_loss\": loss.item()}, step=current_step)\n",
    "\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    print(f\"Epoch {epoch + 1} completed. Average training loss: {avg_train_loss:.4f}\")\n",
    "    wandb.log({\"avg_train_loss\": avg_train_loss}, step=(epoch + 1) * steps_per_epoch)\n",
    "\n",
    "    # Visualization: Log a sample of training predictions\n",
    "    sample_train_pairs, sample_test_inputs, _ = next(train_generator)\n",
    "    train_pairs_encoded = [(conv_encoder.apply(params, inp), conv_encoder.apply(params, out)) for inp, out in sample_train_pairs]\n",
    "    test_inputs_encoded = [conv_encoder.apply(params, inp) for inp in sample_test_inputs]\n",
    "    preds = rnn_model.apply(params, train_pairs_encoded, test_inputs_encoded)\n",
    "    \n",
    "    for i in range(min(num_visualization_samples, len(sample_test_inputs))):\n",
    "        input_grid = sample_test_inputs[i]\n",
    "        predicted_output = jnp.argmax(preds[i], axis=-1)\n",
    "        \n",
    "        task_visual = {\n",
    "            'id': f'Epoch{epoch+1}_Sample{i+1}',\n",
    "            'train': sample_train_pairs[i],\n",
    "            'test': [(input_grid.tolist(), predicted_output.tolist())]\n",
    "        }\n",
    "        drawing = vis.draw_task(task_visual, width=10, height=6)\n",
    "        img_buffer = io.BytesIO()\n",
    "        vis.output_drawing(drawing, img_buffer)\n",
    "        img_buffer.seek(0)\n",
    "        image = Image.open(img_buffer)\n",
    "        wandb.log({f\"Training_Sample_{i+1}\": wandb.Image(image)}, step=(epoch + 1) * steps_per_epoch)\n",
    "\n",
    "# Finish WandB run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
