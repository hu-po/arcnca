{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'pytest>=8.3.2' 'numpy>=1.26.4' 'pillow>=10.4.0' 'msgpack>=1.1.0' 'requests>=2.32.3' 'mediapy>=1.2.2' tqdm\n",
    "!pip install --no-deps 'optax==0.2.3' 'chex==0.1.86' 'flax>=0.9.0' orbax-checkpoint tensorstore 'typing-extensions>=4.2' 'absl-py>=2.1.0' 'toolz>=1.0.0' 'etils[epy]>=1.9.4'\n",
    "!pip install wandb\n",
    "!wandb login\n",
    "!git clone https://github.com/hu-po/cax.git /cax\n",
    "!pip install --upgrade /cax --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import jax\n",
    "import jaxlib\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "import flax\n",
    "from flax import nnx\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import cax\n",
    "\n",
    "for pkg in [jax, jaxlib, cax, flax, optax]:\n",
    "    print(pkg.__name__, pkg.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph =  os.environ[\"MORPH\"]\n",
    "morph_nb_filepath =  os.environ[\"MORPH_NB_FILEPATH\"]\n",
    "morph_output_dir =  os.environ[\"MORPH_OUTPUT_DIR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "def load_data(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {len(data)} tasks from {path}\")\n",
    "    return data\n",
    "\n",
    "train_challenges = load_data('/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json')\n",
    "train_solutions = load_data('/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json')\n",
    "eval_challenges = load_data('/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json')\n",
    "eval_solutions = load_data('/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json')\n",
    "\n",
    "def process_tasks(challenges, solutions):\n",
    "    inputs, outputs, task_indices = [], [], []\n",
    "    task_id_to_index = {}\n",
    "    for index, task_id in enumerate(challenges.keys()):\n",
    "        task_id_to_index[task_id] = index\n",
    "        task = challenges[task_id]\n",
    "        solution = solutions[task_id]\n",
    "        for pair in task['train']:\n",
    "            inputs.append(np.array(pair['input'], dtype=np.int32))\n",
    "            outputs.append(np.array(pair['output'], dtype=np.int32))\n",
    "            task_indices.append(index)\n",
    "        for i, test_input in enumerate(task['test']):\n",
    "            inputs.append(np.array(test_input['input'], dtype=np.int32))\n",
    "            outputs.append(np.array(solution[i], dtype=np.int32))\n",
    "            task_indices.append(index)\n",
    "    return inputs, outputs, task_indices, task_id_to_index\n",
    "\n",
    "def pad_grids(grids, max_size=30, pad_value=0):\n",
    "    padded_grids = []\n",
    "    for grid in grids:\n",
    "        rows, cols = grid.shape\n",
    "        padded = np.pad(grid, ((0, max_size - rows), (0, max_size - cols)), \n",
    "                        mode='constant', constant_values=pad_value)\n",
    "        padded_grids.append(padded)\n",
    "    return np.stack(padded_grids)\n",
    "\n",
    "def prepare_data(challenges, solutions):\n",
    "    inputs, outputs, task_indices, task_id_to_index = process_tasks(challenges, solutions)\n",
    "    print(f\"\\t number of samples: {len(inputs)}\")\n",
    "    inputs_array = pad_grids(inputs)\n",
    "    outputs_array = pad_grids(outputs)\n",
    "    task_indices_array = np.array(task_indices, dtype=np.int32)\n",
    "    inputs_array = jnp.array(inputs_array)\n",
    "    outputs_array = jnp.array(outputs_array)\n",
    "    task_indices_array = jnp.array(task_indices_array)\n",
    "    return inputs_array, outputs_array, task_indices_array, task_id_to_index\n",
    "\n",
    "print(\"Processing train data...\")\n",
    "train_inputs, train_outputs, train_task_indices, task_id_to_index = prepare_data(train_challenges, train_solutions)\n",
    "print(f\"\\t inputs shape: {train_inputs.shape}\")\n",
    "print(f\"\\t outputs shape: {train_outputs.shape}\")\n",
    "print(f\"\\t task indices shape: {train_task_indices.shape}\")\n",
    "\n",
    "print(\"Processing eval data...\")\n",
    "eval_inputs, eval_outputs, eval_task_indices, _ = prepare_data(eval_challenges, eval_solutions)\n",
    "print(f\"\\t inputs shape: {eval_inputs.shape}\")\n",
    "print(f\"\\t outputs shape: {eval_outputs.shape}\")\n",
    "print(f\"\\t task indices shape: {eval_task_indices.shape}\")\n",
    "\n",
    "# Data Augmentation\n",
    "def augment_data(inputs, outputs):\n",
    "    augmented_inputs = []\n",
    "    augmented_outputs = []\n",
    "    \n",
    "    for input_grid, output_grid in zip(inputs, outputs):\n",
    "        # Original data\n",
    "        augmented_inputs.append(input_grid)\n",
    "        augmented_outputs.append(output_grid)\n",
    "        \n",
    "        # Horizontal flip\n",
    "        augmented_inputs.append(np.fliplr(input_grid))\n",
    "        augmented_outputs.append(np.fliplr(output_grid))\n",
    "        \n",
    "        # Vertical flip\n",
    "        augmented_inputs.append(np.flipud(input_grid))\n",
    "        augmented_outputs.append(np.flipud(output_grid))\n",
    "        \n",
    "        # 90-degree rotation\n",
    "        augmented_inputs.append(np.rot90(input_grid))\n",
    "        augmented_outputs.append(np.rot90(output_grid))\n",
    "        \n",
    "        # 180-degree rotation\n",
    "        augmented_inputs.append(np.rot90(input_grid, 2))\n",
    "        augmented_outputs.append(np.rot90(output_grid, 2))\n",
    "        \n",
    "        # Add noise (small random perturbations)\n",
    "        noisy_input = input_grid + np.random.randint(-1, 2, input_grid.shape)\n",
    "        noisy_input = np.clip(noisy_input, 0, 9)\n",
    "        augmented_inputs.append(noisy_input)\n",
    "        augmented_outputs.append(output_grid)\n",
    "    \n",
    "    return np.array(augmented_inputs), np.array(augmented_outputs)\n",
    "\n",
    "# Apply augmentation to training data\n",
    "augmented_train_inputs, augmented_train_outputs = augment_data(train_inputs, train_outputs)\n",
    "\n",
    "# Model Architecture\n",
    "class ImprovedCAX(nnx.Module):\n",
    "    features: int = 64\n",
    "    num_layers: int = 2\n",
    "\n",
    "    @nnx.compact\n",
    "    def __call__(self, x, task_embedding):\n",
    "        # Expand task embedding to match grid dimensions\n",
    "        task_embedding = jnp.tile(task_embedding[:, None, None, :], (1, x.shape[1], x.shape[2], 1))\n",
    "        \n",
    "        # Concatenate input grid with task embedding\n",
    "        x = jnp.concatenate([x[..., None], task_embedding], axis=-1)\n",
    "        \n",
    "        # Perceive module\n",
    "        for _ in range(self.num_layers):\n",
    "            x = nnx.Conv(features=self.features, kernel_size=(3, 3), padding='SAME')(x)\n",
    "            x = jax.nn.relu(x)\n",
    "        \n",
    "        # Update module\n",
    "        x = nnx.Conv(features=self.features, kernel_size=(1, 1))(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = nnx.Conv(features=1, kernel_size=(1, 1))(x)\n",
    "        \n",
    "        return x.squeeze(-1)\n",
    "\n",
    "# Training Process\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        inputs, outputs, task_indices = batch\n",
    "        task_embeddings = state.params['task_embeddings'][task_indices]\n",
    "        predictions = state.apply_fn({'params': params}, inputs, task_embeddings)\n",
    "        loss = jnp.mean((predictions - outputs) ** 2)\n",
    "        return loss\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "# Evaluation and Submission\n",
    "def evaluate(state, inputs, outputs, task_indices):\n",
    "    task_embeddings = state.params['task_embeddings'][task_indices]\n",
    "    predictions = state.apply_fn({'params': state.params}, inputs, task_embeddings)\n",
    "    accuracy = jnp.mean(jnp.all(predictions.round() == outputs, axis=(1, 2)))\n",
    "    return accuracy\n",
    "\n",
    "def prepare_submission(state, test_challenges_path):\n",
    "    with open(test_challenges_path, 'r') as f:\n",
    "        test_challenges = json.load(f)\n",
    "    \n",
    "    submission = {}\n",
    "    for task_id, task in test_challenges.items():\n",
    "        task_index = task_id_to_index[task_id]\n",
    "        task_embedding = state.params['task_embeddings'][task_index]\n",
    "        \n",
    "        task_submission = []\n",
    "        for test_input in task['test']:\n",
    "            input_grid = jnp.array(pad_grids([np.array(test_input['input'], dtype=np.int32)]))\n",
    "            predictions = state.apply_fn({'params': state.params}, input_grid, task_embedding[None, ...])\n",
    "            rounded_predictions = predictions.round().astype(int).tolist()\n",
    "            \n",
    "            # Trim padding\n",
    "            height, width = np.array(test_input['input']).shape\n",
    "            trimmed_prediction = [row[:width] for row in rounded_predictions[0][:height]]\n",
    "            \n",
    "            task_submission.append({\n",
    "                \"attempt_1\": trimmed_prediction,\n",
    "                \"attempt_2\": trimmed_prediction  # You might want to generate a different second attempt\n",
    "            })\n",
    "        \n",
    "        submission[task_id] = task_submission\n",
    "    \n",
    "    with open('submission.json', 'w') as f:\n",
    "        json.dump(submission, f)\n",
    "\n",
    "# Additional Improvements\n",
    "\n",
    "# Implement curriculum learning\n",
    "def curriculum_learning_schedule(epoch):\n",
    "    if epoch < 10:\n",
    "        return 0.5  # Start with easier tasks\n",
    "    elif epoch < 20:\n",
    "        return 0.75  # Gradually increase difficulty\n",
    "    else:\n",
    "        return 1.0  # Full dataset\n",
    "\n",
    "# Implement task embedding initialization\n",
    "def initialize_task_embeddings(num_tasks, embedding_dim=64):\n",
    "    return jax.random.normal(jax.random.PRNGKey(0), (num_tasks, embedding_dim))\n",
    "\n",
    "# Main training loop\n",
    "def train_model():\n",
    "    model = ImprovedCAX()\n",
    "    task_embeddings = initialize_task_embeddings(len(task_id_to_index))\n",
    "    \n",
    "    learning_rate = optax.exponential_decay(\n",
    "        init_value=1e-3, \n",
    "        transition_steps=1000,\n",
    "        decay_rate=0.9\n",
    "    )\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    \n",
    "    state = train_state.TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=model.init(jax.random.PRNGKey(0), jnp.zeros((1, 30, 30)), jnp.zeros((1, 64))),\n",
    "        tx=optimizer\n",
    "    )\n",
    "    state = state.replace(params={**state.params, 'task_embeddings': task_embeddings})\n",
    "    \n",
    "    num_epochs = 100\n",
    "    batch_size = 32\n",
    "    \n",
    "    best_eval_accuracy = 0\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Apply curriculum learning\n",
    "        difficulty = curriculum_learning_schedule(epoch)\n",
    "        num_samples = int(len(augmented_train_inputs) * difficulty)\n",
    "        \n",
    "        # Shuffle and batch data\n",
    "        permutation = jax.random.permutation(jax.random.PRNGKey(epoch), num_samples)\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_indices = permutation[i:i+batch_size]\n",
    "            batch = (\n",
    "                augmented_train_inputs[batch_indices],\n",
    "                augmented_train_outputs[batch_indices],\n",
    "                train_task_indices[batch_indices]\n",
    "            )\n",
    "            state, loss = train_step(state, batch)\n",
    "            epoch_loss += loss\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        eval_accuracy = evaluate(state, eval_inputs, eval_outputs, eval_task_indices)\n",
    "        print(f\"Epoch {epoch}, Loss: {epoch_loss / (num_samples // batch_size)}, Eval Accuracy: {eval_accuracy}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if eval_accuracy > best_eval_accuracy:\n",
    "            best_eval_accuracy = eval_accuracy\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    return state\n",
    "\n",
    "# Run training\n",
    "final_state = train_model()\n",
    "\n",
    "# Prepare submission\n",
    "test_challenges_path = '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json'\n",
    "prepare_submission(final_state, test_challenges_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
