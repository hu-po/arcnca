{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import wandb\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Hyperparams:\n",
    "    seed: int = os.environ.get(\"SEED\", 42)\n",
    "    morph: str = os.environ.get(\"MORPH\", \"test\")\n",
    "    compute_backend: str = os.environ.get(\"COMPUTE_BACKEND\", \"oop\")\n",
    "    wandb_entity: str = os.environ.get(\"WANDB_ENTITY\", \"hug\")\n",
    "    wandb_project: str = os.environ.get(\"WANDB_PROJECT\", \"arc-test\")\n",
    "    run_name: str = \"test\"\n",
    "    created_on: str = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    # ---\n",
    "    learning_rate: float = 1e-3\n",
    "    batch_size: int = 64\n",
    "    num_epochs: int = 8\n",
    "    print_every: int = 100\n",
    "    pad_dim_space: int = 30\n",
    "    pad_dim_time: int = 5\n",
    "    pad_value: int = 0\n",
    "    num_channels: int = 9\n",
    "    augment_prob: float = 0.5\n",
    "\n",
    "hp = Hyperparams()\n",
    "hp.run_name = f\"{hp.morph}.{hp.compute_backend}.{str(uuid.uuid4())[:6]}\"\n",
    "\n",
    "def load_tasks(challenges_path, solutions_path):\n",
    "    \"\"\" loads in raw dataset from json files, stored in RAM \"\"\"\n",
    "    tasks = []\n",
    "    with open(challenges_path, 'r') as f:\n",
    "        challenges_dict = json.load(f)\n",
    "    print(f\"loading challenges from {challenges_path}, found {len(challenges_dict)} challenges\")\n",
    "    with open(solutions_path, 'r') as f:\n",
    "        solutions_dict = json.load(f)\n",
    "    print(f\"loading solutions from {solutions_path}, found {len(solutions_dict)} solutions\")\n",
    "    for task_id in challenges_dict.keys():\n",
    "        task_train_in = []\n",
    "        task_train_out = []\n",
    "        task_eval_in = []\n",
    "        task_eval_out = []\n",
    "        # there may be multiple training pairs for each task\n",
    "        for pair in challenges_dict[task_id]['train']:\n",
    "            task_train_in.append(pair['input'])\n",
    "            task_train_out.append(pair['output'])\n",
    "        for grid in challenges_dict[task_id]['test']:\n",
    "            task_eval_in.append(grid)\n",
    "        for grid in solutions_dict[task_id]:\n",
    "            task_eval_out.append(grid)\n",
    "        tasks.append((task_train_in, task_train_out, task_eval_in, task_eval_out))\n",
    "    return tasks\n",
    "\n",
    "@jax.jit\n",
    "def pad_space(key, grid_in, grid_out, hp: Hyperparams):\n",
    "    # grid is [T, H, W, C], pick random start position in space based on grid shape\n",
    "    start_h = jax.random.randint(key, minval=0, maxval=hp.pad_dim_space - grid_in.shape[1])\n",
    "    start_w = jax.random.randint(key, minval=0, maxval=hp.pad_dim_space - grid_in.shape[2])\n",
    "    pad_width = ((0, 0),\n",
    "        (start_h, hp.pad_dim_space - grid_in.shape[1] - start_h),\n",
    "        (start_w, hp.pad_dim_space - grid_in.shape[2] - start_w),\n",
    "        (0, 0))\n",
    "    grid_in = jnp.pad(grid_in, pad_width, mode='constant', constant_values=hp.pad_value)\n",
    "    grid_out = jnp.pad(grid_out, pad_width, mode='constant', constant_values=hp.pad_value)\n",
    "    return grid_in, grid_out\n",
    "\n",
    "@jax.jit\n",
    "def pad_time(key, grid_in, grid_out, hp: Hyperparams):\n",
    "    # grid is [T, H, W, C], pick random start position in space based on grid shape\n",
    "    # grids are [T, H, W, C], pick random T indices to pad in time\n",
    "    for _ in range(hp.pad_dim_time - grid_in.shape[0]):\n",
    "        repeat_example_idx = jax.random.randint(key, minval=0, maxval=grid_in.shape[0])\n",
    "        grid_in = jnp.concatenate([grid_in, grid_in[repeat_example_idx][None]], axis=0)\n",
    "        grid_out = jnp.concatenate([grid_out, grid_out[repeat_example_idx][None]], axis=0)\n",
    "    # shuffle the T dimmension\n",
    "    perm = jax.random.permutation(key, jnp.arange(hp.pad_dim_time))\n",
    "    grid_in = grid_in[perm]\n",
    "    grid_out = grid_out[perm]\n",
    "    return grid_in, grid_out\n",
    "\n",
    "augmentations = [jnp.fliplr, jnp.flipud, jnp.rot90]\n",
    "\n",
    "@jax.jit\n",
    "def random_augment(key, grid_in, grid_out, hp: Hyperparams):\n",
    "    # rotate and flip in the H and W dimmensions\n",
    "    apply = jax.random.bernoulli(key, hp.augment_prob, shape=(len(augmentations),))\n",
    "    for augmentation in augmentations:\n",
    "        # apply augmentations on H and W dimmensions only\n",
    "        # use jax.lax.cond\n",
    "        grid_in = jax.lax.cond(apply, augmentation, lambda x: x, grid_in)\n",
    "        grid_out = jax.lax.cond(apply, augmentation, lambda x: x, grid_out)\n",
    "    # shuffle the C dimmension\n",
    "    perm = jax.random.permutation(key, jnp.arange(hp.num_channels))\n",
    "    grid_in = grid_in[..., perm]\n",
    "    grid_out = grid_out[..., perm]\n",
    "    return grid_in, grid_out\n",
    "\n",
    "@jax.jit\n",
    "def data_loader(key, tasks, hp: Hyperparams, train_mode=True):\n",
    "    num_tasks = tasks.shape[0]\n",
    "    if train_mode:\n",
    "        indices = jax.random.permutation(key, num_tasks)\n",
    "        tasks = tasks[indices]\n",
    "    num_batches = num_tasks // hp.batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch_train_in = []\n",
    "        batch_train_out = []\n",
    "        batch_eval_in = []\n",
    "        batch_eval_out = []\n",
    "        for j in range(i*hp.batch_size,(i+1)*hp.batch_size):\n",
    "            # load into gpu memory, expand channel dimmension based on integer value\n",
    "            task_train_in = jnp.array(tasks[j][0], dtype=jnp.int32)\n",
    "            task_train_out = jnp.array(tasks[j][1], dtype=jnp.int32)\n",
    "            task_eval_in = jnp.array(tasks[j][2], dtype=jnp.int32)\n",
    "            task_eval_out = jnp.array(tasks[j][3], dtype=jnp.int32)\n",
    "            # pad space and time for task_train grids\n",
    "            task_train_in, task_train_out = jax.vmap(pad_space)(key, task_train_in, task_train_out, hp)\n",
    "            task_train_in, task_train_out = jax.vmap(pad_time)(key, task_train_in, task_train_out, hp)\n",
    "            # pad space for task_eval grids\n",
    "            task_eval_in, task_eval_out = jax.vmap(pad_space)(key, task_eval_in, task_eval_out, hp)\n",
    "            if train_mode:\n",
    "                task_train_in, task_train_out = jax.vmap(random_augment)(key, task_train_in, task_train_out, hp)\n",
    "                task_eval_in, task_eval_out = jax.vmap(random_augment)(key, task_eval_in, task_eval_out, hp)\n",
    "            batch_train_in.append(task_train_in)\n",
    "            batch_train_out.append(task_train_out)\n",
    "            batch_eval_in.append(task_eval_in)\n",
    "            batch_eval_out.append(task_eval_out)\n",
    "        # stack into batches\n",
    "        task_train_in = jnp.stack(batch_train_in) # [batch_size, pad_dim_time, pad_dim_space, pad_dim_space, num_channels]\n",
    "        task_train_out = jnp.stack(batch_train_out) # [batch_size, pad_dim_time, pad_dim_space, pad_dim_space, num_channels]\n",
    "        task_eval_in = jnp.stack(batch_eval_in) # [batch_size, 1, pad_dim_space, pad_dim_space, num_channels]\n",
    "        task_eval_out = jnp.stack(batch_eval_out) # [batch_size, 1, pad_dim_space, pad_dim_space, num_channels]\n",
    "        yield task_train_in, task_train_out, task_eval_in, task_eval_out\n",
    "\n",
    "def init_params(key, hp: Hyperparams):\n",
    "    # TODO: implement\n",
    "    pass\n",
    "\n",
    "def model(params, task_train_in, task_train_out, task_eval_in, hp: Hyperparams):\n",
    "    # TODO: implement\n",
    "    pass\n",
    "\n",
    "def loss_fn(params, task_eval_out_pred, task_eval_out_targ, hp: Hyperparams):\n",
    "    # TODO: implement\n",
    "    pass\n",
    "\n",
    "def train_step(params, opt_state, task_train_in, task_train_out, task_eval_in, task_eval_out, hp: Hyperparams):\n",
    "    # TODO: implement\n",
    "    pass\n",
    "\n",
    "def valid_step(params, task_eval_in, task_eval_out, hp: Hyperparams):\n",
    "    # TODO: implement\n",
    "    pass\n",
    "\n",
    "print(f\"training run {hp.run_name} with hyperparameters: {hp}\")\n",
    "wandb.login()\n",
    "wandb.init(name=hp.wandb_entity, project=hp.wandb_project, config=hp.__dict__)\n",
    "key = jax.random.PRNGKey(hp.seed)\n",
    "train_dataset = load_tasks(\n",
    "    '/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json',\n",
    "    '/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json',\n",
    ")\n",
    "valid_dataset = load_tasks(\n",
    "    '/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json',\n",
    "    '/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json',\n",
    ")\n",
    "train_gen = data_loader(key, train_dataset, hp, train_mode=True)\n",
    "valid_gen = data_loader(key, valid_dataset, hp, train_mode=False)\n",
    "params = init_params(key, hp)\n",
    "optimizer = optax.adam(hp.learning_rate)\n",
    "opt_state = optimizer.init(params)\n",
    "for epoch in range(hp.num_epochs):\n",
    "    print(f\"epoch {epoch + 1}/{hp.num_epochs}\")\n",
    "    steps_per_epoch = len(train_dataset) // hp.batch_size\n",
    "    step = 0\n",
    "    for step in range(steps_per_epoch):\n",
    "        step += epoch * steps_per_epoch + step + 1\n",
    "        task_train_in, task_train_out, task_eval_in, task_eval_out = next(train_gen)\n",
    "        params, opt_state, train_loss = train_step(params, opt_state, task_train_in, task_train_out, task_eval_in, task_eval_out, hp)\n",
    "        if step % hp.print_every == 0:\n",
    "            print(f\"step {step}/{steps_per_epoch}: loss = {train_loss.item():.4f}\")\n",
    "            wandb.log({\"train_loss\": train_loss.item()}, step=step)\n",
    "    valid_loss, valid_acc = valid_step(params, valid_gen, hp)\n",
    "    print(f'valid_loss: {valid_loss.item():.4f}, valid_acc: {valid_acc.item():.4f}')\n",
    "    wandb.log({\"valid_loss\": valid_loss.item(), \"valid_acc\": valid_acc.item()}, step=step)\n",
    "wandb.finish()\n",
    "\n",
    "\n",
    "\n",
    "submission_challenges_path = '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json'\n",
    "tasks = []\n",
    "with open(submission_challenges_path, 'r') as f:\n",
    "    challenges_dict = json.load(f)\n",
    "print(f\"loading challenges from {submission_challenges_path}, found {len(challenges_dict)} challenges\")\n",
    "for task_id in challenges_dict.keys():\n",
    "    # TODO: implement\n",
    "    pass\n",
    "\n",
    "with open('submission.json', 'w') as f:\n",
    "    json.dump(predictions, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
