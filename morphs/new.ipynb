{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import wandb\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Hyperparams:\n",
    "    seed: int = int(os.environ.get(\"SEED\", 42))\n",
    "    morph: str = os.environ.get(\"MORPH\", \"test\")\n",
    "    compute_backend: str = os.environ.get(\"COMPUTE_BACKEND\", \"oop\")\n",
    "    wandb_entity: str = os.environ.get(\"WANDB_ENTITY\", \"hug\")\n",
    "    wandb_project: str = os.environ.get(\"WANDB_PROJECT\", \"arc-test\")\n",
    "    created_on: str = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    # ---\n",
    "    learning_rate: float = 1e-3\n",
    "    batch_size: int = 8\n",
    "    num_epochs: int = 1\n",
    "    print_every: int = 1\n",
    "    pad_dim_space: int = 30\n",
    "    pad_dim_time_train: int = 10  # max number of training pairs\n",
    "    pad_dim_time_eval: int = 3    # max number of eval examples\n",
    "    pad_value: int = 0\n",
    "    num_channels: int = 10\n",
    "    augment_prob: float = 0.5\n",
    "\n",
    "hp = Hyperparams()\n",
    "\n",
    "def load_tasks(challenges_path, solutions_path, hp: Hyperparams):\n",
    "    \"\"\"Loads raw dataset from json files, stored in RAM.\"\"\"\n",
    "    tasks = []\n",
    "    with open(challenges_path, 'r') as f:\n",
    "        challenges_dict = json.load(f)\n",
    "    print(f\"loading challenges from {challenges_path}, found {len(challenges_dict)} challenges\")\n",
    "    with open(solutions_path, 'r') as f:\n",
    "        solutions_dict = json.load(f)\n",
    "    print(f\"loading solutions from {solutions_path}, found {len(solutions_dict)} solutions\")\n",
    "    for task_id in challenges_dict.keys():\n",
    "        print(f\"\\t task {task_id}\")\n",
    "        task_train_in = []\n",
    "        task_train_out = []\n",
    "        task_eval_in = []\n",
    "        task_eval_out = []\n",
    "        # there may be multiple training pairs for each task\n",
    "        for pair in challenges_dict[task_id]['train']:\n",
    "            task_train_in.append(pair['input'])\n",
    "            task_train_out.append(pair['output'])\n",
    "            print(f\"shape of task_train_in {jnp.array(pair['input']).shape}\")\n",
    "            print(f\"shape of task_train_out {jnp.array(pair['output']).shape}\")\n",
    "        for grid in challenges_dict[task_id]['test']:\n",
    "            task_eval_in.append(grid['input'])\n",
    "            print(f\"shape of task_eval_in {jnp.array(grid['input']).shape}\")\n",
    "        for grid in solutions_dict[task_id]:\n",
    "            task_eval_out.append(grid)\n",
    "            print(f\"shape of task_eval_out {jnp.array(grid).shape}\")\n",
    "        assert len(task_eval_in) == len(task_eval_out)\n",
    "        assert len(task_train_in) == len(task_train_out)\n",
    "        assert len(task_eval_in) <= hp.pad_dim_time_eval\n",
    "        assert len(task_train_in) <= hp.pad_dim_time_train\n",
    "        tasks.append((task_train_in, task_train_out, task_eval_in, task_eval_out))\n",
    "    return tasks\n",
    "\n",
    "def pad_space(key, grid_in, grid_out, hp: Hyperparams):\n",
    "    # grids are [H, W, C], pick random start position in space based on grid shape\n",
    "    assert grid_in.shape[0] <= hp.pad_dim_space\n",
    "    assert grid_in.shape[1] <= hp.pad_dim_space\n",
    "    assert grid_in.shape[0] == grid_in.shape[1]\n",
    "    assert grid_out is None or grid_out.shape == grid_in.shape\n",
    "    start_h = jax.random.randint(key, minval=0, maxval=hp.pad_dim_space - grid_in.shape[0] + 1)\n",
    "    start_w = jax.random.randint(key, minval=0, maxval=hp.pad_dim_space - grid_in.shape[1] + 1)\n",
    "    pad_width = ((start_h, hp.pad_dim_space - grid_in.shape[0] - start_h),\n",
    "                 (start_w, hp.pad_dim_space - grid_in.shape[1] - start_w),\n",
    "                 (0, 0))\n",
    "    grid_in = jnp.pad(grid_in, pad_width, mode='constant', constant_values=hp.pad_value)\n",
    "    if grid_out is not None:\n",
    "        grid_out = jnp.pad(grid_out, pad_width, mode='constant', constant_values=hp.pad_value)\n",
    "    return grid_in, grid_out, (start_h, start_w)\n",
    "\n",
    "def pad_time(key, grid_in, grid_out, hp: Hyperparams, is_eval=False):\n",
    "    # different padding in time for eval and train examples\n",
    "    if is_eval:\n",
    "        pad_dim_time = hp.pad_dim_time_eval\n",
    "    else:\n",
    "        pad_dim_time = hp.pad_dim_time_train\n",
    "    # grids are [T, H, W, C], repeat random example in time until T == pad_dim_time\n",
    "    assert grid_in.shape[0] <= pad_dim_time\n",
    "    assert grid_in.shape[1] <= hp.pad_dim_space\n",
    "    assert grid_in.shape[2] <= hp.pad_dim_space\n",
    "    assert grid_in.shape[1] == grid_in.shape[2]\n",
    "    assert grid_out is None or grid_out.shape == grid_in.shape\n",
    "    for _ in range(pad_dim_time - grid_in.shape[0]):\n",
    "        repeat_example_idx = jax.random.randint(key, minval=0, maxval=grid_in.shape[0])\n",
    "        grid_in = jnp.concatenate([grid_in, grid_in[repeat_example_idx][None]], axis=0)\n",
    "        if grid_out is not None:\n",
    "            grid_out = jnp.concatenate([grid_out, grid_out[repeat_example_idx][None]], axis=0)\n",
    "    # shuffle the T dimension\n",
    "    perm = jax.random.permutation(key, pad_dim_time)\n",
    "    grid_in = grid_in[perm]\n",
    "    if grid_out is not None:\n",
    "        grid_out = grid_out[perm]\n",
    "    return grid_in, grid_out\n",
    "\n",
    "augmentations = [jnp.fliplr, jnp.flipud, jnp.rot90]\n",
    "\n",
    "def random_augment(key, grid_in, grid_out, hp: Hyperparams):\n",
    "    assert grid_in.shape == grid_out.shape\n",
    "    assert grid_in.shape == (hp.pad_dim_time, hp.pad_dim_space, hp.pad_dim_space, hp.num_channels)\n",
    "    apply = jax.random.bernoulli(key, hp.augment_prob, shape=(len(augmentations),))\n",
    "    for i, augmentation in enumerate(augmentations):\n",
    "        grid_in = jax.lax.cond(apply[i], augmentation, lambda x: x, grid_in)\n",
    "        if grid_out is not None:\n",
    "            grid_out = jax.lax.cond(apply[i], augmentation, lambda x: x, grid_out)\n",
    "    # shuffle the C dimension\n",
    "    perm = jax.random.permutation(key, hp.num_channels)\n",
    "    grid_in = grid_in[..., perm]\n",
    "    if grid_out is not None:\n",
    "        grid_out = grid_out[..., perm]\n",
    "    return grid_in, grid_out\n",
    "\n",
    "def data_loader(key, tasks, hp: Hyperparams, train_mode=True):\n",
    "    num_tasks = len(tasks)\n",
    "    if train_mode:\n",
    "        indices = jax.random.permutation(key, num_tasks)\n",
    "        tasks = [tasks[idx] for idx in indices]\n",
    "    num_batches = int(jnp.ceil(num_tasks / hp.batch_size))\n",
    "    for i in range(num_batches):\n",
    "        batch_key, key = jax.random.split(key)\n",
    "        start_idx = i * hp.batch_size\n",
    "        end_idx = min((i + 1) * hp.batch_size, num_tasks)\n",
    "        current_batch_size = end_idx - start_idx\n",
    "        subkeys = jax.random.split(batch_key, current_batch_size * 5)\n",
    "        batch_train_in = []\n",
    "        batch_train_out = []\n",
    "        batch_eval_in = []\n",
    "        batch_eval_out = []\n",
    "        for j, subkey in zip(range(start_idx, end_idx), subkeys.reshape(current_batch_size, 5, -1)):\n",
    "            k1, k2, k3, k4, k5 = subkey\n",
    "            task_train_in = jnp.array(tasks[j][0], dtype=jnp.int32)\n",
    "            task_train_out = jnp.array(tasks[j][1], dtype=jnp.int32)\n",
    "            task_eval_in = jnp.array(tasks[j][2], dtype=jnp.int32)\n",
    "            task_eval_out = jnp.array(tasks[j][3], dtype=jnp.int32)\n",
    "            # convert grids to one-hot (channel dimension)\n",
    "            task_train_in = jax.vmap(lambda x: jax.nn.one_hot(x, hp.num_channels))(task_train_in)\n",
    "            task_train_out = jax.vmap(lambda x: jax.nn.one_hot(x, hp.num_channels))(task_train_out)\n",
    "            task_eval_in = jax.vmap(lambda x: jax.nn.one_hot(x, hp.num_channels))(task_eval_in)\n",
    "            task_eval_out = jax.vmap(lambda x: jax.nn.one_hot(x, hp.num_channels))(task_eval_out)\n",
    "            # pad in space\n",
    "            task_train_in_padded = []\n",
    "            task_train_out_padded = []\n",
    "            for t_in, t_out in zip(task_train_in, task_train_out):\n",
    "                t_in_padded, t_out_padded, _ = pad_space(k1, t_in, t_out, hp)\n",
    "                task_train_in_padded.append(t_in_padded)\n",
    "                task_train_out_padded.append(t_out_padded)\n",
    "            task_train_in_padded = jnp.stack(task_train_in_padded)\n",
    "            task_train_out_padded = jnp.stack(task_train_out_padded)\n",
    "            # pad in time\n",
    "            task_train_in_padded, task_train_out_padded = pad_time(k2, task_train_in_padded, task_train_out_padded, hp)\n",
    "            # same for eval data\n",
    "            task_eval_in_padded = []\n",
    "            task_eval_out_padded = []\n",
    "            for e_in, e_out in zip(task_eval_in, task_eval_out):\n",
    "                e_in_padded, e_out_padded, _ = pad_space(k3, e_in, e_out, hp)\n",
    "                task_eval_in_padded.append(e_in_padded)\n",
    "                task_eval_out_padded.append(e_out_padded)\n",
    "            task_eval_in_padded = jnp.stack(task_eval_in_padded)\n",
    "            task_eval_out_padded = jnp.stack(task_eval_out_padded)\n",
    "            task_eval_in_padded, task_eval_out_padded = pad_time(k4, task_eval_in_padded, task_eval_out_padded, hp, is_eval=True)\n",
    "            if train_mode:\n",
    "                task_train_in_padded, task_train_out_padded = random_augment(k4, task_train_in_padded, task_train_out_padded, hp)\n",
    "                task_eval_in_padded, task_eval_out_padded = random_augment(k5, task_eval_in_padded, task_eval_out_padded, hp)\n",
    "            batch_train_in.append(task_train_in_padded)\n",
    "            batch_train_out.append(task_train_out_padded)\n",
    "            batch_eval_in.append(task_eval_in_padded)\n",
    "            batch_eval_out.append(task_eval_out_padded)\n",
    "        task_train_in = jnp.stack(batch_train_in)\n",
    "        assert task_train_in.shape == (current_batch_size, hp.pad_dim_time_train, hp.pad_dim_space, hp.pad_dim_space, hp.num_channels)\n",
    "        task_train_out = jnp.stack(batch_train_out)\n",
    "        assert task_train_out.shape == (current_batch_size, hp.pad_dim_time_train, hp.pad_dim_space, hp.pad_dim_space, hp.num_channels)\n",
    "        task_eval_in = jnp.stack(batch_eval_in)\n",
    "        assert task_eval_in.shape == (current_batch_size, hp.pad_dim_time_eval, hp.pad_dim_space, hp.pad_dim_space, hp.num_channels)\n",
    "        task_eval_out = jnp.stack(batch_eval_out)\n",
    "        assert task_eval_out.shape == (current_batch_size, hp.pad_dim_time_eval, hp.pad_dim_space, hp.pad_dim_space, hp.num_channels)\n",
    "        yield task_train_in, task_train_out, task_eval_in, task_eval_out\n",
    "\n",
    "# <model>\n",
    "\n",
    "def init_params(key, hp: Hyperparams):\n",
    "    \"\"\"Initialize model parameters.\"\"\"\n",
    "    params = {}\n",
    "    keys = jax.random.split(key, num=8)\n",
    "    # Encoder for training pairs (input and output grids concatenated along channel dimension)\n",
    "    params['encoder'] = {\n",
    "        'conv1': conv_layer_params(keys[0], in_channels=hp.num_channels * 2, out_channels=32, kernel_size=3),\n",
    "        'conv2': conv_layer_params(keys[1], in_channels=32, out_channels=64, kernel_size=3),\n",
    "        'conv3': conv_layer_params(keys[2], in_channels=64, out_channels=128, kernel_size=3),\n",
    "    }\n",
    "    # Encoder for test input grid\n",
    "    params['test_encoder'] = {\n",
    "        'conv1': conv_layer_params(keys[3], in_channels=hp.num_channels, out_channels=32, kernel_size=3),\n",
    "        'conv2': conv_layer_params(keys[4], in_channels=32, out_channels=64, kernel_size=3),\n",
    "        'conv3': conv_layer_params(keys[5], in_channels=64, out_channels=128, kernel_size=3),\n",
    "    }\n",
    "    # Decoder to produce the output grid\n",
    "    params['decoder'] = mlp_layer_params(keys[6], in_dim=256, out_dim=hp.pad_dim_space * hp.pad_dim_space * hp.num_channels)\n",
    "    return params\n",
    "\n",
    "def conv_layer_params(key, in_channels, out_channels, kernel_size):\n",
    "    \"\"\"Initialize convolutional layer parameters.\"\"\"\n",
    "    w_key, b_key = jax.random.split(key)\n",
    "    w_shape = (kernel_size, kernel_size, in_channels, out_channels)\n",
    "    w = jax.random.normal(w_key, w_shape) * jnp.sqrt(2.0 / (kernel_size * kernel_size * in_channels))\n",
    "    b = jnp.zeros(out_channels)\n",
    "    return {'w': w, 'b': b}\n",
    "\n",
    "def mlp_layer_params(key, in_dim, out_dim):\n",
    "    \"\"\"Initialize MLP layer parameters.\"\"\"\n",
    "    w_key, b_key = jax.random.split(key)\n",
    "    w = jax.random.normal(w_key, (in_dim, out_dim)) * jnp.sqrt(2.0 / in_dim)\n",
    "    b = jnp.zeros(out_dim)\n",
    "    return {'w': w, 'b': b}\n",
    "\n",
    "def model(params, task_train_in, task_train_out, task_eval_in, hp: Hyperparams):\n",
    "    \"\"\"Model that predicts the task_eval_out given training pairs and task_eval_in.\"\"\"\n",
    "\n",
    "    def process_training_examples(task_train_in, task_train_out):\n",
    "        # Concatenate input and output grids along the channel dimension\n",
    "        concatenated = jnp.concatenate([task_train_in, task_train_out], axis=-1)  # (pad_dim_time, H, W, num_channels * 2)\n",
    "\n",
    "        # Encode each training example\n",
    "        def encode_grid(grid):\n",
    "            h = grid\n",
    "            h = jax.lax.conv_general_dilated(\n",
    "                h[None], params['encoder']['conv1']['w'], window_strides=(1,1), padding='SAME'\n",
    "            )\n",
    "            h = h + params['encoder']['conv1']['b']\n",
    "            h = jax.nn.relu(h)\n",
    "            h = jax.lax.conv_general_dilated(\n",
    "                h, params['encoder']['conv2']['w'], window_strides=(1,1), padding='SAME'\n",
    "            )\n",
    "            h = h + params['encoder']['conv2']['b']\n",
    "            h = jax.nn.relu(h)\n",
    "            h = jax.lax.conv_general_dilated(\n",
    "                h, params['encoder']['conv3']['w'], window_strides=(1,1), padding='SAME'\n",
    "            )\n",
    "            h = h + params['encoder']['conv3']['b']\n",
    "            h = jax.nn.relu(h)\n",
    "            h = jnp.mean(h, axis=(1,2))  # Global average pooling\n",
    "            return h[0]  # (128,)\n",
    "\n",
    "        # Apply to each training example and aggregate\n",
    "        representations = jax.vmap(encode_grid)(concatenated)\n",
    "        aggregated = jnp.mean(representations, axis=0)  # (128,)\n",
    "        return aggregated\n",
    "\n",
    "    def process_test_input(task_eval_in):\n",
    "        # Encode the test input grid\n",
    "        grid = task_eval_in[0]\n",
    "        h = grid\n",
    "        h = jax.lax.conv_general_dilated(\n",
    "            h[None], params['test_encoder']['conv1']['w'], window_strides=(1,1), padding='SAME'\n",
    "        )\n",
    "        h = h + params['test_encoder']['conv1']['b']\n",
    "        h = jax.nn.relu(h)\n",
    "        h = jax.lax.conv_general_dilated(\n",
    "            h, params['test_encoder']['conv2']['w'], window_strides=(1,1), padding='SAME'\n",
    "        )\n",
    "        h = h + params['test_encoder']['conv2']['b']\n",
    "        h = jax.nn.relu(h)\n",
    "        h = jax.lax.conv_general_dilated(\n",
    "            h, params['test_encoder']['conv3']['w'], window_strides=(1,1), padding='SAME'\n",
    "        )\n",
    "        h = h + params['test_encoder']['conv3']['b']\n",
    "        h = jax.nn.relu(h)\n",
    "        h = jnp.mean(h, axis=(1,2))  # Global average pooling\n",
    "        return h[0]  # (128,)\n",
    "\n",
    "    def process_task(task_train_in, task_train_out, task_eval_in):\n",
    "        train_repr = process_training_examples(task_train_in, task_train_out)  # (128,)\n",
    "        test_repr = process_test_input(task_eval_in)  # (128,)\n",
    "        combined = jnp.concatenate([train_repr, test_repr], axis=0)  # (256,)\n",
    "        # Decoder\n",
    "        logits = jnp.dot(combined, params['decoder']['w']) + params['decoder']['b']  # (H*W*C,)\n",
    "        output_grid = logits.reshape(hp.pad_dim_space, hp.pad_dim_space, hp.num_channels)  # (H, W, C)\n",
    "        return output_grid\n",
    "\n",
    "    task_eval_out_pred = jax.vmap(process_task, in_axes=(0, 0, 0))(task_train_in, task_train_out, task_eval_in)\n",
    "    return task_eval_out_pred  # (batch_size, H, W, num_channels)\n",
    "\n",
    "def loss_fn(task_eval_out_pred, task_eval_out_targ, hp: Hyperparams):\n",
    "    \"\"\"Compute the loss between predicted and target outputs.\"\"\"\n",
    "    logits = task_eval_out_pred.reshape(-1, hp.num_channels)\n",
    "    labels = task_eval_out_targ.reshape(-1, hp.num_channels)\n",
    "    labels = jnp.argmax(labels, axis=-1)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    loss = jnp.mean(loss)\n",
    "    return loss\n",
    "\n",
    "# </model>\n",
    "\n",
    "def train_step(params, opt_state, task_train_in, task_train_out, task_eval_in, task_eval_out, hp: Hyperparams):\n",
    "    def loss_and_grad(params):\n",
    "        task_eval_out_pred = model(params, task_train_in, task_train_out, task_eval_in, hp)\n",
    "        loss = loss_fn(task_eval_out_pred, task_eval_out.squeeze(1), hp)\n",
    "        return loss\n",
    "    loss, grads = jax.value_and_grad(loss_and_grad)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "def valid_step(params, valid_gen, hp: Hyperparams):\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    num_batches = 0\n",
    "    for task_train_in, task_train_out, task_eval_in, task_eval_out in valid_gen:\n",
    "        task_eval_out_pred = model(params, task_train_in, task_train_out, task_eval_in, hp)\n",
    "        loss = loss_fn(task_eval_out_pred, task_eval_out.squeeze(1), hp)\n",
    "        total_loss += loss\n",
    "        pred_classes = jnp.argmax(task_eval_out_pred, axis=-1)\n",
    "        true_classes = jnp.argmax(task_eval_out.squeeze(1), axis=-1)\n",
    "        acc = jnp.mean(pred_classes == true_classes)\n",
    "        total_acc += acc\n",
    "        num_batches += 1\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_acc = total_acc / num_batches\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "print(f\"hyperparameters: {hp}\")\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    entity=hp.wandb_entity,\n",
    "    project=hp.wandb_project,\n",
    "    name=f\"{hp.morph}.{hp.compute_backend}.{str(uuid.uuid4())[:6]}\",\n",
    "    config=hp.__dict__,\n",
    ")\n",
    "key = jax.random.PRNGKey(hp.seed)\n",
    "train_tasks = load_tasks(\n",
    "    '/kaggle/input/arc-prize-2024/arc-agi_training_challenges.json',\n",
    "    '/kaggle/input/arc-prize-2024/arc-agi_training_solutions.json', hp)\n",
    "valid_tasks = load_tasks(\n",
    "    '/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json',\n",
    "    '/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json', hp)\n",
    "params = init_params(key, hp)\n",
    "optimizer = optax.adam(hp.learning_rate)\n",
    "opt_state = optimizer.init(params)\n",
    "for epoch in range(hp.num_epochs):\n",
    "    print(f\"epoch {epoch + 1}/{hp.num_epochs}\")\n",
    "    steps_per_epoch = int(jnp.ceil(len(train_tasks) / hp.batch_size))\n",
    "    train_key, key = jax.random.split(key)\n",
    "    train_gen = data_loader(train_key, train_tasks, hp, train_mode=True)\n",
    "    for step in range(steps_per_epoch):\n",
    "        task_train_in, task_train_out, task_eval_in, task_eval_out = next(train_gen)\n",
    "        params, opt_state, train_loss = train_step(params, opt_state, task_train_in, task_train_out, task_eval_in, task_eval_out, hp)\n",
    "        if step % hp.print_every == 0:\n",
    "            print(f\"step {step + 1}/{steps_per_epoch}: loss = {train_loss.item():.4f}\")\n",
    "            wandb.log({\"train_loss\": train_loss.item()}, step=step + 1)\n",
    "    valid_key, key = jax.random.split(key)\n",
    "    valid_gen = data_loader(valid_key, valid_tasks, hp, train_mode=False)\n",
    "    valid_loss, valid_acc = valid_step(params, valid_gen, hp)\n",
    "    print(f'valid_loss: {valid_loss.item():.4f}, valid_acc: {valid_acc.item():.4f}')\n",
    "    wandb.log({\"valid_loss\": valid_loss.item(), \"valid_acc\": valid_acc.item()}, step=(epoch + 1) * steps_per_epoch)\n",
    "wandb.finish()\n",
    "\n",
    "submission_challenges_path = '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json'\n",
    "predictions = {}\n",
    "with open(submission_challenges_path, 'r') as f:\n",
    "    challenges_dict = json.load(f)\n",
    "print(f\"loading challenges from {submission_challenges_path}, found {len(challenges_dict)} challenges\")\n",
    "for task_id in challenges_dict.keys():\n",
    "    task = challenges_dict[task_id]\n",
    "    task_train_in = []\n",
    "    task_train_out = []\n",
    "    task_eval_in = []\n",
    "    for pair in task['train']:\n",
    "        task_train_in.append(pair['input'])\n",
    "        task_train_out.append(pair['output'])\n",
    "    for grid in task['test']:\n",
    "        task_eval_in.append(grid['input'])\n",
    "    task_train_in = jnp.array(task_train_in, dtype=jnp.int32)\n",
    "    task_train_out = jnp.array(task_train_out, dtype=jnp.int32)\n",
    "    task_eval_in = jnp.array(task_eval_in, dtype=jnp.int32)\n",
    "    task_train_in = jax.vmap(lambda x: jax.nn.one_hot(x, hp.num_channels))(task_train_in)\n",
    "    task_train_out = jax.vmap(lambda x: jax.nn.one_hot(x, hp.num_channels))(task_train_out)\n",
    "    task_eval_in = jax.vmap(lambda x: jax.nn.one_hot(x, hp.num_channels))(task_eval_in)\n",
    "    task_attempt = {}\n",
    "    for attempt_id in range(2):\n",
    "        key = jax.random.PRNGKey(attempt_id)\n",
    "        # Pad training data\n",
    "        task_train_in_padded = []\n",
    "        task_train_out_padded = []\n",
    "        for t_in, t_out in zip(task_train_in, task_train_out):\n",
    "            t_in_padded, t_out_padded, _ = pad_space(key, t_in, t_out, hp)\n",
    "            task_train_in_padded.append(t_in_padded)\n",
    "            task_train_out_padded.append(t_out_padded)\n",
    "        task_train_in_padded = jnp.stack(task_train_in_padded)\n",
    "        task_train_out_padded = jnp.stack(task_train_out_padded)\n",
    "        task_train_in_padded, task_train_out_padded = pad_time(key, task_train_in_padded, task_train_out_padded, hp)\n",
    "        # Pad eval data\n",
    "        task_eval_in_padded = []\n",
    "        start_positions = []\n",
    "        for e_in in task_eval_in:\n",
    "            e_in_padded, _, start_pos = pad_space(key, e_in, None, hp)\n",
    "            task_eval_in_padded.append(e_in_padded)\n",
    "            start_positions.append(start_pos)\n",
    "        task_eval_in_padded = jnp.stack(task_eval_in_padded)\n",
    "        task_eval_in_padded, _ = pad_time(key, task_eval_in_padded, None, hp, is_eval=True)\n",
    "        # Predict\n",
    "        task_eval_out_pred = model(params, task_train_in_padded[None], task_train_out_padded[None], task_eval_in_padded[None], hp)\n",
    "        task_eval_out_pred = task_eval_out_pred[0]  # Remove batch dimension\n",
    "        # Un-padding and converting outputs\n",
    "        outputs = []\n",
    "        for eval_example_id in range(task_eval_in_padded.shape[0]):\n",
    "            grid_out_pred = task_eval_out_pred[eval_example_id]\n",
    "            grid_out_pred = jnp.argmax(grid_out_pred, axis=-1)\n",
    "            sh, sw = start_positions[eval_example_id]\n",
    "            h = task_eval_in[eval_example_id].shape[0]\n",
    "            w = task_eval_in[eval_example_id].shape[1]\n",
    "            grid_out_pred = grid_out_pred[sh:sh + h, sw:sw + w]\n",
    "            grid_out_pred = grid_out_pred.tolist()\n",
    "            outputs.append(grid_out_pred)\n",
    "        task_attempt[f\"attempt_{attempt_id+1}\"] = outputs\n",
    "    predictions[task_id] = task_attempt\n",
    "\n",
    "with open('submission.json', 'w') as f:\n",
    "    json.dump(predictions, f)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
